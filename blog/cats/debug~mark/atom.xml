<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: debug~mark | kk Blog —— 通用基础]]></title>
  <link href="http://abcdxyzk.github.io/blog/cats/debug~mark/atom.xml" rel="self"/>
  <link href="http://abcdxyzk.github.io/"/>
  <updated>2015-05-08T16:54:59+08:00</updated>
  <id>http://abcdxyzk.github.io/</id>
  <author>
    <name><![CDATA[kk]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[gro收包]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/05/08/debug-mark-gro-attention/"/>
    <updated>2015-05-08T16:32:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/05/08/debug-mark-gro-attention</id>
    <content type="html"><![CDATA[<p><a href="/blog/2015/04/18/kernel-net-gro/">linux kernel 网络协议栈之GRO(Generic receive offload)</a></p>

<p>gro会合并多个gso_size不同的包, 会将gso_size设置成第一个包的gso_size.</p>

<p>如果此时把这个包发出去，那么就会导致不满足： skb->gso_size * (skb->segs-1) &lt; skb->len &lt;= skb->gso_size * skb->segs</p>

<p>那么后面的三个函数就有可能出错</p>

<h4>一、tcp_shift_skb_data</h4>

<pre><code>    mss = skb-&gt;gso_size
    len = len/mss * mss

    |---|-------|-------|
     mss    |
            V
    |---|---|
</code></pre>

<h4>二、tcp_mark_head_lost</h4>

<pre><code>    len = (packets - cnt) * mss

    |--------|--|--|
       mss   |
             V
    |--------|--------|
</code></pre>

<h4>三、tcp_match_skb_to_sack</h4>

<pre><code>    new_len = (pkt_len/mm)*mss
    in_sack = 1
    pkt_len = new_len

    |---|-------|-------|
     mss    |
            V
    |---|---|
</code></pre>

<h4>修改</h4>

<p>加入发包队列前
<code>
    skb_shinfo(skb)-&gt;gso_size = 0;
    skb_shinfo(skb)-&gt;gso_segs = 0;
    skb_shinfo(skb)-&gt;gso_type = 0;
</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[tcp_trim_head BUG]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/05/08/debug-mark-tcp_trim_head_bug/"/>
    <updated>2015-05-08T16:24:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/05/08/debug-mark-tcp_trim_head_bug</id>
    <content type="html"><![CDATA[<p><a href="http://kernel.opensuse.org/cgit/kernel/commit/?id=5b35e1e6e9ca651e6b291c96d1106043c9af314a">http://kernel.opensuse.org/cgit/kernel/commit/?id=5b35e1e6e9ca651e6b291c96d1106043c9af314a</a></p>

<p>author  Neal Cardwell <a href="&#109;&#x61;&#x69;&#108;&#116;&#x6f;&#58;&#110;&#99;&#x61;&#x72;&#100;&#119;&#101;&#x6c;&#x6c;&#64;&#103;&#x6f;&#x6f;&#103;&#108;&#x65;&#46;&#99;&#111;&#x6d;">&#110;&#x63;&#97;&#x72;&#x64;&#x77;&#101;&#108;&#108;&#64;&#103;&#111;&#111;&#x67;&#x6c;&#101;&#46;&#x63;&#x6f;&#x6d;</a>    2012-01-28 17:29:46 (GMT)<br/>
committer   David S. Miller <a href="&#109;&#97;&#x69;&#x6c;&#116;&#x6f;&#58;&#100;&#x61;&#118;&#x65;&#x6d;&#x40;&#x64;&#97;&#x76;&#x65;&#x6d;&#x6c;&#x6f;&#x66;&#x74;&#x2e;&#x6e;&#x65;&#x74;">&#100;&#x61;&#118;&#x65;&#109;&#64;&#x64;&#x61;&#118;&#x65;&#109;&#108;&#x6f;&#x66;&#x74;&#x2e;&#110;&#101;&#116;</a>   2012-01-30 17:42:58 (GMT)<br/>
commit  5b35e1e6e9ca651e6b291c96d1106043c9af314a (patch)<br/>
tree    d18caadee5e93dc45d0c5fa2c530537cfa14586c<br/>
parent  4acb41903b2f99f3dffd4c3df9acc84ca5942cb2 (diff)</p>

<h4>tcp: fix tcp_trim_head() to adjust segment count with skb MSS</h4>

<p>This commit fixes tcp_trim_head() to recalculate the number of segments in the skb with the skb&rsquo;s existing MSS, so trimming the head causes the skb segment count to be monotonically non-increasing - it should stay the same or go down, but not increase.</p>

<p>Previously tcp_trim_head() used the current MSS of the connection. But if there was a decrease in MSS between original transmission and ACK (e.g. due to PMTUD), this could cause tcp_trim_head() to counter-intuitively increase the segment count when trimming bytes off the head of an skb. This violated assumptions in tcp_tso_acked() that tcp_trim_head() only decreases the packet count, so that packets_acked in tcp_tso_acked() could underflow, leading tcp_clean_rtx_queue() to pass u32 pkts_acked values as large as 0xffffffff to ca_ops->pkts_acked().</p>

<p>As an aside, if tcp_trim_head() had really wanted the skb to reflect the current MSS, it should have called tcp_set_skb_tso_segs() unconditionally, since a decrease in MSS would mean that a single-packet skb should now be sliced into multiple segments.</p>

<p>Signed-off-by: Neal Cardwell <a href="&#x6d;&#x61;&#x69;&#108;&#116;&#111;&#x3a;&#110;&#99;&#97;&#x72;&#x64;&#119;&#101;&#x6c;&#108;&#64;&#103;&#x6f;&#111;&#103;&#x6c;&#101;&#46;&#99;&#111;&#x6d;">&#x6e;&#99;&#97;&#114;&#x64;&#x77;&#x65;&#x6c;&#108;&#64;&#103;&#111;&#111;&#x67;&#x6c;&#x65;&#x2e;&#x63;&#111;&#x6d;</a> <br/>
Acked-by: Nandita Dukkipati <a href="&#x6d;&#97;&#x69;&#x6c;&#116;&#111;&#58;&#x6e;&#97;&#110;&#100;&#105;&#116;&#97;&#100;&#64;&#103;&#x6f;&#x6f;&#103;&#x6c;&#101;&#46;&#x63;&#x6f;&#x6d;">&#110;&#97;&#110;&#x64;&#105;&#x74;&#x61;&#x64;&#x40;&#103;&#x6f;&#x6f;&#x67;&#x6c;&#x65;&#x2e;&#x63;&#x6f;&#109;</a> <br/>
Acked-by: Ilpo Järvinen <a href="&#x6d;&#x61;&#105;&#x6c;&#x74;&#111;&#58;&#x69;&#108;&#112;&#111;&#x2e;&#106;&#97;&#x72;&#x76;&#105;&#110;&#x65;&#x6e;&#x40;&#x68;&#101;&#108;&#115;&#105;&#x6e;&#x6b;&#x69;&#x2e;&#x66;&#x69;">&#105;&#x6c;&#112;&#x6f;&#x2e;&#106;&#97;&#x72;&#118;&#105;&#110;&#x65;&#x6e;&#x40;&#104;&#x65;&#108;&#x73;&#x69;&#x6e;&#107;&#x69;&#46;&#102;&#x69;</a> <br/>
Signed-off-by: David S. Miller <a href="&#x6d;&#97;&#x69;&#108;&#x74;&#111;&#x3a;&#x64;&#97;&#118;&#101;&#109;&#x40;&#100;&#x61;&#x76;&#101;&#109;&#108;&#111;&#102;&#116;&#46;&#110;&#x65;&#x74;">&#100;&#97;&#x76;&#x65;&#109;&#x40;&#x64;&#x61;&#x76;&#101;&#x6d;&#108;&#x6f;&#x66;&#x74;&#46;&#x6e;&#101;&#116;</a></p>

<p>1 files changed, 2 insertions, 4 deletions
<code>
    diff --git a/net/ipv4/tcp_output.c b/net/ipv4/tcp_output.c
    index 8c8de27..4ff3b6d 100644
    --- a/net/ipv4/tcp_output.c
    +++ b/net/ipv4/tcp_output.c
    @@ -1141,11 +1141,9 @@ int tcp_trim_head(struct sock *sk, struct sk_buff *skb, u32 len)
        sk_mem_uncharge(sk, len);
        sock_set_flag(sk, SOCK_QUEUE_SHRUNK);
    -   /* Any change of skb-&gt;len requires recalculation of tso
    -    * factor and mss.
    -    */
    +   /* Any change of skb-&gt;len requires recalculation of tso factor. */
        if (tcp_skb_pcount(skb) &gt; 1)
    -       tcp_set_skb_tso_segs(sk, skb, tcp_current_mss(sk));
    +       tcp_set_skb_tso_segs(sk, skb, tcp_skb_mss(skb));
        return 0;
    }
</code></p>

<hr />

<p>会出现tp->packets_out不正确, 导致sk_write_queue为空时却掉tcp_rearm_rto()，判断tp->packets_out不为0，启动重传定时器，然后重传时取出的是list_head的地址，不是skb的地址，导致后面异常。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[如果sk_write_queue异常]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/05/08/debug-mark-write_queue/"/>
    <updated>2015-05-08T14:14:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/05/08/debug-mark-write_queue</id>
    <content type="html"><![CDATA[<ul>
<li>注意，以下情况内核都不可能产生，纯属假设</li>
</ul>


<h4>一、连续的SYN/FIN</h4>

<pre><code>    |---FIN---|---SYN/FIN---|
        skb       next_skb
</code></pre>

<ul>
<li>内核不可能出现是因为：发送FIN包后就不再发包。所以FIN包只可能在sk_write_queue的最后一个包</li>
</ul>


<p>假设skb和next_skb发出去后都丢了，那tcp_retransmit_skb会重传skb，
重传的时候会调用tcp_retrans_try_collapse尝试去和下一个包合并。</p>

<p>skb和next_skb合并过程：<br/>
先检查一些条件，然后
<code>
    ...
    skb_copy_from_linear_data(next_skb, skb_put(skb, next_skb_size), next_skb_size);
    ...
    TCP_SKB_CB(skb)-&gt;end_seq = TCP_SKB_CB(next_skb)-&gt;end_seq;
</code>
也就是skb->len += next_skb->len; skb->end_seq = next_skb->end_seq;</p>

<p>假设:
<code>
    skb-&gt;len = 0;      skb-&gt;seq = 10;      skb-&gt;end_seq = 10 + FIN = 11;
    next_skb-&gt;len = 0; next_skb-&gt;seq = 11; next_skb-&gt;end_seq = 11 + SYN/FIN = 12;
</code>
那么合并后：
<code>
    skb-&gt;len = 0;      skb-&gt;seq = 10;      skb-&gt;end_seq = 12;
</code>
很明显不正常了，正常情况下：skb->len &lt;= skb->end_seq - skb->seq &lt;= skb->len+1</p>

<p>这时如果来了ack 11，那么会再重传合并后的skb，然后会调用tcp_trim_head(struct ws_st_sock <em>sk, struct sk_buff </em>skb, u32 len)，参数len = tp->snd_una - TCP_SKB_CB(skb)->seq = 1，但skb->len = 0;</p>

<p>tcp_trim_head函数中会：
<code>
    skb-&gt;len -= len;
</code>
这时skb->len = (U32)-1 = 0xFFFFFFFF，skb->len错误后，再调用skb_copy之类的就会访问越界，报BUG。
<code>
     821 struct sk_buff *skb_copy(const struct sk_buff *skb, gfp_t gfp_mask)
     822 {
            ......
     835         if (skb_copy_bits(skb, -headerlen, n-&gt;head, headerlen + skb-&gt;len))
     836                 BUG();
</code></p>

<h4>二、write_queue的skb->end_seq > next_skb->seq可能的问题</h4>

<ul>
<li>内核用tp->write_seq控制，保证了write_queue的skb->end_seq == next_skb->seq</li>
</ul>


<pre><code>    skb:       |------------------|
    next_skb:  |---------------------|

    假设skb已经发送出去，并被ack了，这时tp-&gt;snd_una = skb-&gt;end_seq
    此时再发送next_skb，并且mss变小了，需要对next_skb分包，分包后如下：

    skb:       |------------------|
    next_skb:  |-------|-------:-----|
                  skb1       skb2

    next_skb 被分成了两个包，skb1-&gt;len = mss, skb1-&gt;gso_segs = 1; skb2-&gt;len &gt; mss, skb2-&gt;gso_segs = 2;
    skb1, skb2发送出去，丢了，然后重传skb1，
    此时 skb1-&gt;end_seq &lt; tp-&gt;snd_una

    2092 int tcp_retransmit_skb(struct sock *sk, struct sk_buff *skb)
    2093 {
            ......
    2111         if (before(TCP_SKB_CB(skb)-&gt;seq, tp-&gt;snd_una)) {
    2112                 if (before(TCP_SKB_CB(skb)-&gt;end_seq, tp-&gt;snd_una))
    2113                         BUG();
</code></pre>

<h4>三、write_queue的skb->end_seq > next_skb->seq可能的问题</h4>

<pre><code>    skb:       |------------------|
    next_skb:  |---------------------|

    skb, next_skb 发送出去丢了，重传，调用tcp_retrans_try_collapse合并。
    合并后：skb-&gt;len += next_skb-&gt;len; skb-&gt;end_seq = next_skb-&gt;end_seq;

    假设   skb-&gt;len = 100;      skb-&gt;seq = 0;      skb-&gt;end_seq = 100;
          next_skb-&gt;len = 120  next_skb-&gt;seq = 0; next_skb-&gt;end_seq = 120;
    合并后 skb-&gt;len = 200;      skb-&gt;seq = 0;      skb-&gt;end_seq = 120;

    发送合并后的skb，再丢包，再重传，mss = 150，skb-&gt;len &gt; mss, 会分包
          skb-&gt;len = 150;      skb-&gt;seq = 0;      skb-&gt;end_seq = 150;
          next_skb-&gt;len = 50;  next_skb-&gt;seq = 150; next_skb-&gt;end_seq = 120;
    也就是出现了next_skb-&gt;seq &gt; next_skb-&gt;end_seq
    (此时如果ack skb也会把next_skb一起清了，因为next_skb-&gt;end_seq &lt; skb-&gt;end_seq)

    这时如果skb再重传分包，分成skb3，skb4
        skb3-&gt;len = 130;   skb3-&gt;seq = 0;   skb3-&gt;end_seq = 130;
        skb4-&gt;len = 20;    skb4-&gt;seq = 130; skb4-&gt;end_seq = 150;

    这时ack了skb3，tp-&gt;snd_una = 130 (虽然next_skb-&gt;end_seq &lt; skb3-&gt;end_seq, 但skb4-&gt;end_seq &gt; skb3-&gt;end_seq, 所以不会把next_skb清掉)
    重传skb4，skb5，此时skb5-&gt;end_seq &lt; tp-&gt;snd_una

    2092 int tcp_retransmit_skb(struct sock *sk, struct sk_buff *skb)
    2093 {
            ......
    2111         if (before(TCP_SKB_CB(skb)-&gt;seq, tp-&gt;snd_una)) {
    2112                 if (before(TCP_SKB_CB(skb)-&gt;end_seq, tp-&gt;snd_una))
    2113                         BUG();
</code></pre>

<h4>四、write_queue的skb->end_seq > next_skb->seq可能的问题</h4>

<pre><code>    skb:       |------------------|
    next_skb:  |---------------------|

    发送 skb，next_skb
    接收到 sack:|---------------------|

    调用tcp_sacktag_walk() ---&gt; tcp_shift_skb_data() 将多个被sack的包合并成一个。
    合并过程：
        skb-&gt;len += next_skb-&gt;len; skb-&gt;end_seq += next_skb-&gt;len;
    那么就会合并出一超出原来end_seq的包：
               |----------------------------------------|
    然后再ack:  |----------------------|
    这时把合并出的包trim掉一部分，剩skb7:  |-----------------|

    再发包skb_new:                     |-------|
    这时tp-&gt;snd_nxt = skb_new-&gt;end_seq
    再重传skb7, 并分包:                 |----------|------|
    分包时skb7-&gt;end_seq &gt; tp-&gt;snd_nxt, 所以不会调整tp-&gt;packets_out，
    但ack到来时(tcp_clean_rtx_queue)tp-&gt;packets_out却会减去分包后的gso_segs。
    导致tp-&gt;packets_out &lt; 0, 但sk_write_queue却是空的。
    tcp_rearm_rto()判断tp-&gt;packets_out不为0，启动重传定时器，然后重传时取出的是list_head的地址，不是skb的地址，导致后面异常。
    代码：
     974 int tcp_fragment(struct sock *sk, struct sk_buff *skb, u32 len,
     975                  unsigned int mss_now)
     976 {
        ......
    1047         if (!before(tp-&gt;snd_nxt, TCP_SKB_CB(buff)-&gt;end_seq)) {
    1048                 int diff = old_factor - tcp_skb_pcount(skb) -
    1049                         tcp_skb_pcount(buff);
    1050 
    1051                 if (diff)
    1052                         tcp_adjust_pcount(sk, skb, diff);
    1053         }
</code></pre>

<h4>五（发现好像没错）、write_queue的skb->end_seq > next_skb->seq可能的问题</h4>

<ul>
<li>内核用tp->write_seq控制，保证了write_queue的skb->end_seq == next_skb->seq</li>
</ul>


<pre><code>    skb:       |------------------|
    next_skb:  |---------------------|

    假设skb已经发送出去，这时tp-&gt;snd_nxt = skb-&gt;end_seq
    发送next_skb时mss变小了，需要对next_skb分包，分包后如下：

    skb:       |------------------|
    next_skb:  |-------|-------:-----|
                  skb1       skb2
    next_skb 被分成了两个包，skb1-&gt;len = mss, skb1-&gt;gso_segs = 1; skb2-&gt;len &gt; mss, skb2-&gt;gso_segs = 2;

    然后将skb1, skb2发送出去, tp-&gt;packets_out += 3; 这时假设ack了skb，清掉skb1和skb2的一个mss，。。。没错。。。
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[拥塞控制模块注意]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/04/15/debug-mark-cong/"/>
    <updated>2015-04-15T14:24:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/04/15/debug-mark-cong</id>
    <content type="html"><![CDATA[<h4>应用改变sock的拥塞控制算法</h4>

<pre><code>    #define SOL_TCP 6
    #define TCP_CONGESTION  13

    strcpy(name, "cubic");
    setsockopt (connfd, SOL_TCP, TCP_CONGESTION, name, strlen(name));
</code></pre>

<h5>net/socket.c</h5>

<pre><code>    SYSCALL_DEFINE5(setsockopt, int, fd, int, level, int, optname,
            char __user *, optval, int, optlen)
    {
        ...
                err =
                    sock-&gt;ops-&gt;setsockopt(sock, level, optname, optval,
                              optlen);
        ...
    }
</code></pre>

<p>对于ipv4的tcp，sock->ops指向 net/ipv4/af_inet.c 中的 inet_stream_ops，所以setsockopt等于sock_common_setsockopt。</p>

<h5>net/core/sock.c</h5>

<pre><code>    int sock_common_setsockopt(struct socket *sock, int level, int optname,
                   char __user *optval, unsigned int optlen)
    {
        struct sock *sk = sock-&gt;sk;

        return sk-&gt;sk_prot-&gt;setsockopt(sk, level, optname, optval, optlen);
    }
</code></pre>

<p>sk_prot 指向 net/ipv4/tcp_ipv4.c 中的 tcp_prot，所以setsockopt等于tcp_setsockopt</p>

<h5>net/ipv4/tcp.c</h5>

<pre><code>    int tcp_setsockopt(struct sock *sk, int level, int optname, char __user *optval,
               unsigned int optlen)
    {
        struct inet_connection_sock *icsk = inet_csk(sk);

        if (level != SOL_TCP)
            return icsk-&gt;icsk_af_ops-&gt;setsockopt(sk, level, optname,
                                 optval, optlen);
        return do_tcp_setsockopt(sk, level, optname, optval, optlen);
    }
</code></pre>

<p>因为level = SOL_TCP, optname = TCP_CONGESTION, 所以直接到do_tcp_setsockopt的第一个if里。</p>

<pre><code>    static int do_tcp_setsockopt(struct sock *sk, int level,
            int optname, char __user *optval, unsigned int optlen)
    {
        struct tcp_sock *tp = tcp_sk(sk);
        struct inet_connection_sock *icsk = inet_csk(sk); 
        int val;      
        int err = 0;

        /* This is a string value all the others are int's */
        if (optname == TCP_CONGESTION) {      
            char name[TCP_CA_NAME_MAX]; 

            if (optlen &lt; 1)
                return -EINVAL;

            val = strncpy_from_user(name, optval,
                        min_t(long, TCP_CA_NAME_MAX-1, optlen));
            if (val &lt; 0)
                return -EFAULT;
            name[val] = 0;

            lock_sock(sk);
            err = tcp_set_congestion_control(sk, name);
            release_sock(sk);
            return err;
        }

    ...
</code></pre>

<h4>net/ipv4/tcp_cong.c</h4>

<pre><code>    /* Change congestion control for socket */
    int tcp_set_congestion_control(struct sock *sk, const char *name)
    {
        struct inet_connection_sock *icsk = inet_csk(sk);
        struct tcp_congestion_ops *ca;
        int err = 0;

        rcu_read_lock();
        ca = tcp_ca_find(name);

        /* no change asking for existing value */
        if (ca == icsk-&gt;icsk_ca_ops)
            goto out;

    #ifdef CONFIG_MODULES
        /* not found attempt to autoload module */
        if (!ca &amp;&amp; capable(CAP_NET_ADMIN)) {
            rcu_read_unlock();
            request_module("tcp_%s", name);
            rcu_read_lock();
            ca = tcp_ca_find(name);
        }
    #endif
        if (!ca)
            err = -ENOENT;

        else if (!((ca-&gt;flags &amp; TCP_CONG_NON_RESTRICTED) || capable(CAP_NET_ADMIN)))
            err = -EPERM;

        else if (!try_module_get(ca-&gt;owner))
            err = -EBUSY;

        else {
            tcp_cleanup_congestion_control(sk);
            icsk-&gt;icsk_ca_ops = ca;

            if (sk-&gt;sk_state != TCP_CLOSE &amp;&amp; icsk-&gt;icsk_ca_ops-&gt;init) // 如果sk-&gt;sk_state = TCP_CLOSE, 那么不会调用拥塞控制模块的初始化
                icsk-&gt;icsk_ca_ops-&gt;init(sk);
        }
     out:
        rcu_read_unlock();
        return err;
    }
</code></pre>

<p>可以看到，如果sk->sk_state = TCP_CLOSE, 那么不会调用拥塞控制模块的初始化。</p>

<hr />

<h4>那么什么时候sk->sk_state == TCP_CLOSE，并且还能调用setsockopt呢？</h4>

<h5>举一种情况：当收到RST包的时候，tcp_rcv_established()->tcp_validate_incoming()->tcp_reset()->tcp_done()将sk置为TCP_CLOSE。</h5>

<h5>如果拥塞控制模块中init有申请内存，release中释放内存。那么在上述情况下将会出现没有申请而直接释放的情况，导致panic。</h5>

<pre><code>    BUG: unable to handle kernel paging request at ffffeba4000002a0

    [&lt;ffffffff8115b17e&gt;] kfree+0x6e/0x240
    [&lt;ffffffffa0068055&gt;] cong_release+0x35/0x50 [cong]
    [&lt;ffffffff81467953&gt;] tcp_cleanup_congestion_control+0x23/0x40
    [&lt;ffffffff81465bb9&gt;] tcp_v4_destroy_sock+0x29/0x2d0
    [&lt;ffffffff8144e9e3&gt;] inet_csk_destroy_sock+0x53/0x140
    [&lt;ffffffff814504c0&gt;] tcp_close+0x340/0x4a0
    [&lt;ffffffff814748de&gt;] inet_release+0x5e/0x90
    [&lt;ffffffff813f4359&gt;] sock_release+0x29/0x90
    [&lt;ffffffff813f43d7&gt;] sock_close+0x17/0x40
    [&lt;ffffffff81173ed3&gt;] __fput+0xf3/0x220
    [&lt;ffffffff8117401c&gt;] fput+0x1c/0x30
    [&lt;ffffffff8116df2d&gt;] filp_close+0x5d/0x90
    [&lt;ffffffff8117090c&gt;] sys_close+0xac/0x110
    [&lt;ffffffff8100af72&gt;] system_call_fastpath+0x16/0x1b
</code></pre>

<h4>测试代码</h4>

<p><a href="/download/debug/congestion_mod_panic.tar.gz">congestion_mod_panic</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[mod_timer会切换cpu]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/01/14/debug-mod-timer/"/>
    <updated>2015-01-14T23:59:01+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/01/14/debug-mod-timer</id>
    <content type="html"><![CDATA[<p><a href="https://lkml.org/lkml/2009/4/16/45">https://lkml.org/lkml/2009/4/16/45</a></p>

<blockquote><p>Ingo, Thomas, all,</p>

<p>In an SMP system, tasks are scheduled on different CPUs by the
scheduler, interrupts are managed by irqbalancer daemon, but timers
are still stuck to the CPUs that they have been initialised.  Timers
queued by tasks gets re-queued on the CPU where the task gets to run
next, but timers from IRQ context like the ones in device drivers are
still stuck on the CPU they were initialised.  This framework will
help move all &lsquo;movable timers&rsquo; using a sysctl interface.</p></blockquote>

<p>kernel/timer.c 中 __mod_timer函数的部分patch：
<code>
+   cpu = smp_processor_id();
+   if (get_sysctl_timer_migration() &amp;&amp; idle_cpu(cpu) &amp;&amp; !pinned) {
+#if defined(CONFIG_NO_HZ) &amp;&amp; (CONFIG_SMP)
+       preferred_cpu = get_nohz_load_balancer();
+#endif
+       if (preferred_cpu &gt;= 0)
+           cpu = preferred_cpu;
+   }
+
+   new_base = per_cpu(tvec_bases, cpu);
+
</code></p>

<hr />

<p>也就是说：如果当前进程是idle（函数idle_cpu(cpu)判定），那么在mod_timer时会根据cpu的struct rq runqueues;中的 struct sched_domain *sd; 来选一个不是idle的cpu，然后把timer移到他上去。如果都是idle，就还在本cpu。<br/>
禁用该功能可以 echo 0 > /proc/sys/kernel/timer_magration，默认的启用是1。</p>

<p>也就是说：系统默认状态下mod_timer有可能会mod_timer到其他cpu上。</p>

<hr />

<p>但是基本只有softirq时（如 <a href="/blog/2015/01/14/debug-softirq-time-count/">/blog/2015/01/14/debug-softirq-time-count/</a>），这时会的当前进程就是idle，但cpu实际并不空闲。这样的话softirq的timer在mod_timer时，会被加到其他cpu的定时器队列。如果这些timer是不允许切换cpu的（如对per_cpu变量的操作），那么就会产生bug。</p>
]]></content>
  </entry>
  
</feed>
