<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: debug | kk Blog —— 通用基础]]></title>
  <link href="http://abcdxyzk.github.io/blog/cats/debug/atom.xml" rel="self"/>
  <link href="http://abcdxyzk.github.io/"/>
  <updated>2015-04-10T16:02:28+08:00</updated>
  <id>http://abcdxyzk.github.io/</id>
  <author>
    <name><![CDATA[kk]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[mod_timer会切换cpu]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/01/14/debug-mod-timer/"/>
    <updated>2015-01-14T23:59:01+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/01/14/debug-mod-timer</id>
    <content type="html"><![CDATA[<p><a href="https://lkml.org/lkml/2009/4/16/45">https://lkml.org/lkml/2009/4/16/45</a></p>

<blockquote><p>Ingo, Thomas, all,</p>

<p>In an SMP system, tasks are scheduled on different CPUs by the
scheduler, interrupts are managed by irqbalancer daemon, but timers
are still stuck to the CPUs that they have been initialised.  Timers
queued by tasks gets re-queued on the CPU where the task gets to run
next, but timers from IRQ context like the ones in device drivers are
still stuck on the CPU they were initialised.  This framework will
help move all &lsquo;movable timers&rsquo; using a sysctl interface.</p></blockquote>

<p>kernel/timer.c 中 __mod_timer函数的部分patch：
<code>
+   cpu = smp_processor_id();
+   if (get_sysctl_timer_migration() &amp;&amp; idle_cpu(cpu) &amp;&amp; !pinned) {
+#if defined(CONFIG_NO_HZ) &amp;&amp; (CONFIG_SMP)
+       preferred_cpu = get_nohz_load_balancer();
+#endif
+       if (preferred_cpu &gt;= 0)
+           cpu = preferred_cpu;
+   }
+
+   new_base = per_cpu(tvec_bases, cpu);
+
</code></p>

<hr />

<p>也就是说：如果当前进程是idle（函数idle_cpu(cpu)判定），那么在mod_timer时会根据cpu的struct rq runqueues;中的 struct sched_domain *sd; 来选一个不是idle的cpu，然后把timer移到他上去。如果都是idle，就还在本cpu。<br/>
禁用该功能可以 echo 0 > /proc/sys/kernel/timer_magration，默认的启用是1。</p>

<p>也就是说：系统默认状态下mod_timer有可能会mod_timer到其他cpu上。</p>

<hr />

<p>但是基本只有softirq时（如 <a href="/blog/2015/01/14/debug-softirq-time-count/">/blog/2015/01/14/debug-softirq-time-count/</a>），这时会的当前进程就是idle，但cpu实际并不空闲。这样的话softirq的timer在mod_timer时，会被加到其他cpu的定时器队列。如果这些timer是不允许切换cpu的（如对per_cpu变量的操作），那么就会产生bug。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[中断时间统计]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/01/14/debug-softirq-time-count/"/>
    <updated>2015-01-14T23:59:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/01/14/debug-softirq-time-count</id>
    <content type="html"><![CDATA[<p>软中断运行在中断上下文，不会被抢占调度，只会被硬中断打断，但硬中断退出时还是继续执行没结束的软中断。</p>

<p>因为软中断不是运行在进程上下文，不具备被调度的前提，也不具备统计运行时间。系统是将softirq的时间加到当前被他打断的进程上（还是不统计softirq时间？？？有待学习）。</p>

<p>如果当前系统只运行数据包的接收服务，那么系统很可能显示的是100%idle，因为被softirq打断的进程就是idle。</p>

<p>如果softirq足够多，导致启动了ksoftirqd进程来协助处理，那么softirq的时间会被记到ksoftirqd的进程上，显示有“有点正常”了。</p>

<p>这样就会出现：当cpu个数充足时显示100%idle，然后减少到一半cpu就显示X%si。也就是说显示100%idle是不对，应该是近似的(x/2)%si</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[高精度定时器 high-cpu-load]]></title>
    <link href="http://abcdxyzk.github.io/blog/2014/11/06/debug-mark-sleep/"/>
    <updated>2014-11-06T14:30:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2014/11/06/debug-mark-sleep</id>
    <content type="html"><![CDATA[<p><a href="http://stackoverflow.com/questions/1125297/nanosleep-high-cpu-usage">http://stackoverflow.com/questions/1125297/nanosleep-high-cpu-usage</a></p>

<p>I noticed that a little test program which calls nanosleep is showing a huge difference in CPU usage when run on Linux machines with a kernel newer than 2.6.22.
<code>
    #include &lt;time.h&gt;
    int main (void)
    {
        struct timespec sleepTime;
        struct timespec returnTime;
        sleepTime.tv_sec = 0;
        sleepTime.tv_nsec = 1000;
        while (1)
        {
            nanosleep(&amp;sleepTime, &amp;returnTime); // usleep(1); 同样异常
        }
        return 0;
    }
</code>
(Yes, I realise this program does nothing)</p>

<p>  If I compile this and run it on an openSUSE 10.3 machine (2.6.22.19-0.2-default), the program does not even show up on the process list generated by &ldquo;top&rdquo;, indicating to me that it is using very little CPU time.  If I run it on an openSUSE 11.1 machine (2.6.27.23-0.1-default), top shows the program taking 40% of the CPU time.  Running on Fedora 9 (2.6.25-14.fc9.i686) and Fedora 10 also showed the same high CPU usage in &ldquo;top&rdquo;.</p>

<p>Has there been a change in the kernel that affects this?</p>

<hr />

<h4>Answers</h4>

<p>This is due to the introduction of NO_HZ into the mainline scheduler.</p>

<p>Previously, your 1,000 ns sleep was usually sleeping for a whole tick - 1,000,000 ns.  Now, when the machine is otherwise idle, it&rsquo;s actually only sleeping for what you asked for.  So it&rsquo;s running the while() loop and syscall around 1,000 times more frequently - hence a lot more CPU usage.  If you increase tv_nsec you should see a reduction in the CPU usage.</p>

<hr />

<pre><code>    int nanosleep(const struct timespec *req, struct timespec *rem);

    struct timespec
    {
        time_t  tv_sec;         /* seconds */
        long    tv_nsec;        /* nanoseconds */
    };
</code></pre>

<p> 这个函数功能是暂停某个进程直到你规定的时间后恢复，参数req就是你要暂停的时间，其中req->tv_sec是以秒为单位，而tv_nsec以毫 微秒为单位（10的-9次方秒）。由于调用nanosleep是是进程进入TASK_INTERRUPTIBLE,这种状态是会相应信号而进入 TASK_RUNNING状态的，这就意味着有可能会没有等到你规定的时间就因为其它信号而唤醒，此时函数返回-1，切还剩余的时间会被记录在rem中。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[crash vs gdb work]]></title>
    <link href="http://abcdxyzk.github.io/blog/2014/11/06/debug-crash-work/"/>
    <updated>2014-11-06T10:51:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2014/11/06/debug-crash-work</id>
    <content type="html"><![CDATA[<p><a href="https://www.redhat.com/archives/crash-utility/2014-October/msg00002.html">贴自https://www.redhat.com/archives/crash-utility/2014-October/msg00002.html</a><br/>
Yes, sure. GDB works very differently from crash. There main conceptual<br/>
difference is that GDB only handles with VIRTUAL addresses, while the<br/>
crash utility first translates everything to PHYSICAL addresses.<br/>
Consequently, GDB ignores the PhysAddr field in ELF program headers,<br/>
and crash ignores the VirtAddr field.</p>

<p>I have looked at some of my ELF dump files, and it seems to me that<br/>
VirtAddr is not filled correctly, except for kernel text and static<br/>
data (address range 0xffffffff80000000-0xffffffff9fffffff). Your linked<br/>
list is most likely allocated in the direct mapping<br/>
(0xffff880000000000-0xffffc7ffffffffff). However, I found out that the<br/>
virtual addresses for the direct mapping segments are wrong, e.g. my<br/>
dump file specifies it at 0xffff810000000000 (hypervisor area). This is<br/>
most likely a bug in the kernel code that implements /proc/vmcore.</p>

<p>But that&rsquo;s beside the point. Why?  The Linux kernel maps many physical<br/>
pages more than once into the virtual address space. It would be waste<br/>
of space if you saved it multiple times (for each virtual address that<br/>
maps to it). The crash utility can translate each virtual address to<br/>
the physical address and map it onto ELF segments using PhysAddr.<br/>
Incidentally, the PhysAddr fields are correct in my dump files&hellip;</p>

<p>I&rsquo;m glad you&rsquo;re interested in using GDB to read kernel dump files,<br/>
especially if you&rsquo;re willing to make it work for real. I have proposed<br/>
more than once that the crash utility be re-implemented in pure gdb.<br/>
Last time I looked (approx. 1.5 years ago) the main missing pieces were:</p>

<ol>
<li> Use of physical addresses (described above)</li>
<li> Support for multiple virtual address spaces (for different process<br/>
 contexts)</li>
<li> Ability to read compressed kdump files</li>
<li> Ability to use 64-bit files on 32-bit platforms (to handle PAE)</li>
</ol>


<p>HTH,<br/>
Petr Tesarik</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[静态编译crash + xbt + bt -H]]></title>
    <link href="http://abcdxyzk.github.io/blog/2014/11/04/debug-crash-static/"/>
    <updated>2014-11-04T18:23:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2014/11/04/debug-crash-static</id>
    <content type="html"><![CDATA[<h5>要在centos6上编译，为了能在centos5用，用静态编译</h5>

<h5>有两个显示函数参数的patch，但是不一定能起作用</h5>

<h5>patch1:</h5>

<p><a href="https://github.com/jhammond/xbt">https://github.com/jhammond/xbt</a>
<a href="https://www.redhat.com/archives/crash-utility/2013-September/msg00010.html">https://www.redhat.com/archives/crash-utility/2013-September/msg00010.html</a></p>

<h5>patch2:</h5>

<p><a href="https://github.com/hziSot/crash-stack-parser">https://github.com/hziSot/crash-stack-parser</a>
<a href="https://github.com/hziSot/crash-stack-parser/blob/master/crash-parse-stack-7.0.1.patch">https://github.com/hziSot/crash-stack-parser/blob/master/crash-parse-stack-7.0.1.patch</a></p>

<h4>一、依赖包：</h4>

<p>yum install bison zlib zlib-static glibc-static elfutils-devel elfutils-devel-static elfutils-libelf-devel-static ncurses ncurses-static crash-devel</p>

<h4>二、patch1: xbt 显示参数</h4>

<p>patch: <a href="https://github.com/hziSot/crash-stack-parser  ">https://github.com/hziSot/crash-stack-parser  </a>
make CFLAGS+=&ndash;static LDFLAGS+=&ndash;static</p>

<h4>三、patch2: bt -H 显示参数</h4>

<pre><code>    依赖：有些没有静态包，要自己编译安装：
    liblzma.a: http://tukaani.org/xz/xz-5.0.7.tar.bz2
    libbz2.a:  http://www.bzip.org/1.0.6/bzip2-1.0.6.tar.gz
    下载代码：git clone https://github.com/jhammond/xbt.git xbt.git
    把xbt.git/xbt_crash.c中函数xbt_func前的static删了
    把xbt.git/xbt_crash.c中函数xmod_init的register_extension删了
    把 xbt 命令加到global_data.c        函数x86_64_exception_frame已经在其他库中定义了，所以要换个名字
    编译xbt代码：make   ==  rm -rf *.so
    把 xbt.git/xbt_crash.o  xbt.git/xbt_dwarf.o  xbt.git/xbt_dwfl.o  xbt.git/xbt_eval.o  xbt.git/xbt_frame_print.o 加到 Makefile 的 OBJECT_FILES= 中
    make CFLAGS+=--static LDFLAGS+="--static -lc  -lm -ldl -ldw -lebl -lelf -lbz2 -llzma"


    注意:-lelf -lebl要放在-ldw后面。
</code></pre>
]]></content>
  </entry>
  
</feed>
