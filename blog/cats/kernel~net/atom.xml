<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: kernel~net | kk Blog —— 通用基础]]></title>
  <link href="http://abcdxyzk.github.io/blog/cats/kernel~net/atom.xml" rel="self"/>
  <link href="http://abcdxyzk.github.io/"/>
  <updated>2015-06-03T15:51:48+08:00</updated>
  <id>http://abcdxyzk.github.io/</id>
  <author>
    <name><![CDATA[kk]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Receive packet steering patch详解]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/06/03/kernel-net-rps/"/>
    <updated>2015-06-03T15:39:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/06/03/kernel-net-rps</id>
    <content type="html"><![CDATA[<p><a href="http://simohayha.iteye.com/blog/720850">http://simohayha.iteye.com/blog/720850</a></p>

<p>Receive packet steering简称rps，是google贡献给linux kernel的一个patch，主要的功能是解决多核情况下，网络协议栈的软中断的负载均衡。这里的负载均衡也就是指能够将软中断均衡的放在不同的cpu核心上运行。</p>

<p>简介在这里：<br/>
<a href="http://lwn.net/Articles/362339/">http://lwn.net/Articles/362339/</a></p>

<p>linux现在网卡的驱动支持两种模式，一种是NAPI，一种是非NAPI模式，这两种模式的区别，我前面的blog都有介绍，这里就再次简要的介绍下。</p>

<p>在NAPI中，中断收到数据包后调用__napi_schedule调度软中断，然后软中断处理函数中会调用注册的poll回掉函数中调用netif_receive_skb将数据包发送到3层，没有进行任何的软中断负载均衡。</p>

<p>在非NAPI中，中断收到数据包后调用netif_rx，这个函数会将数据包保存到input_pkt_queue，然后调度软中断，这里为了兼容NAPI的驱动，他的poll方法默认是process_backlog，最终这个函数会从input_pkt_queue中取得数据包然后发送到3层。</p>

<p>通过比较我们可以看到，不管是NAPI还是非NAPI的话都无法做到软中断的负载均衡，因为软中断此时都是运行在在硬件中断相应的cpu上。也就是说如果始终是cpu0相应网卡的硬件中断，那么始终都是cpu0在处理软中断，而此时cpu1就被浪费了，因为无法并行的执行多个软中断。</p>

<p>google的这个patch的基本原理是这样的,根据数据包的源地址，目的地址以及目的和源端口(这里它是将两个端口组合成一个4字节的无符数进行计算的，后面会看到)计算出一个hash值，然后根据这个hash值来选择软中断运行的cpu，从上层来看，也就是说将每个连接和cpu绑定，并通过这个hash值，来均衡软中断在多个cpu上。</p>

<p>这个介绍比较简单，我们来看代码是如何实现的。</p>

<p>它这里主要是hook了两个内核的函数，一个是netif_rx主要是针对非NAPI的驱动，一个是netif_receive_skb这个主要是针对NAPI的驱动，这两个函数我前面blog都有介绍过，想了解可以看我前面的blog，现在这里我只介绍打过patch的实现。</p>

<p>在看netif_rx和netif_receive_skb之前，我们先来看这个patch中两个重要的函数get_rps_cpu和enqueue_to_backlog，我们一个个看。</p>

<p>先来看相关的两个数据结构，首先是netdev_rx_queue，它表示对应的接收队列，因为有的网卡可能硬件上就支持多队列的模式，此时对应就会有多个rx队列，这个结构是挂载在net_device中的，也就是每个网络设备最终都会有一个或者多个rx队列。这个结构在sys文件系统中的表示类似这样的/sys/class/net/<device>/queues/rx-<n> 几个队列就是rx-n.
<code>
    struct netdev_rx_queue {
        // 保存了当前队列的rps map
        struct rps_map *rps_map;
        // 对应的kobject
        struct kobject kobj;
        // 指向第一个rx队列
        struct netdev_rx_queue *first;
        // 引用计数
        atomic_t count;
    } ____cacheline_aligned_in_smp;
</code></p>

<p>然后就是rps_map，其实这个也就是保存了能够执行数据包的cpu。
<code>
    struct rps_map {
        // cpu的个数，也就是cpus数组的个数
        unsigned int len;
        // RCU锁
        struct rcu_head rcu;
        // 保存了cpu的id.
        u16 cpus[0];
    };
</code></p>

<p>看完上面的结构，我们来看函数的实现。
get_rps_cpu主要是通过传递进来的skb然后来选择这个skb所应该被处理的cpu。它的逻辑很简单，就是通过skb计算hash，然后通过hash从对应的队列的rps_mapping中取得对应的cpu id。</p>

<p>这里有个要注意的就是这个hash值是可以交给硬件网卡去计算的，作者自己说是最好交由硬件去计算这个hash值，因为如果是软件计算的话会导致CPU 缓存不命中，带来一定的性能开销。</p>

<p>还有就是rps_mapping这个值是可以通过sys 文件系统设置的，位置在这里：
/sys/class/net/<device>/queues/rx-<n>/rps_cpus 。</p>

<pre><code>    static int get_rps_cpu(struct net_device *dev, struct sk_buff *skb)
    {
        struct ipv6hdr *ip6;
        struct iphdr *ip;
        struct netdev_rx_queue *rxqueue;
        struct rps_map *map;
        int cpu = -1;
        u8 ip_proto;
        u32 addr1, addr2, ports, ihl;
        // rcu锁
        rcu_read_lock();
        // 取得设备对应的rx 队列
        if (skb_rx_queue_recorded(skb)) {
        ..........................................
            rxqueue = dev-&gt;_rx + index;
        } else
            rxqueue = dev-&gt;_rx;

        if (!rxqueue-&gt;rps_map)
            goto done;
        // 如果硬件已经计算，则跳过计算过程
        if (skb-&gt;rxhash)
            goto got_hash; /* Skip hash computation on packet header */

        switch (skb-&gt;protocol) {
        case __constant_htons(ETH_P_IP):
            if (!pskb_may_pull(skb, sizeof(*ip)))
                goto done;
            // 得到计算hash的几个值
            ip = (struct iphdr *) skb-&gt;data;
            ip_proto = ip-&gt;protocol;
            // 两个地址
            addr1 = ip-&gt;saddr;
            addr2 = ip-&gt;daddr;
            // 得到ip头
            ihl = ip-&gt;ihl;
            break;
        case __constant_htons(ETH_P_IPV6):
            ..........................................
            break;
        default:
            goto done;
        }
        ports = 0;
        switch (ip_proto) {
        case IPPROTO_TCP:
        case IPPROTO_UDP:
        case IPPROTO_DCCP:
        case IPPROTO_ESP:
        case IPPROTO_AH:
        case IPPROTO_SCTP:
        case IPPROTO_UDPLITE:
            if (pskb_may_pull(skb, (ihl * 4) + 4))
            // 我们知道tcp头的前4个字节就是源和目的端口，因此这里跳过ip头得到tcp头的前4个字节
                ports = *((u32 *) (skb-&gt;data + (ihl * 4)));
            break;

        default:
            break;
        }
        // 计算hash
        skb-&gt;rxhash = jhash_3words(addr1, addr2, ports, hashrnd);
        if (!skb-&gt;rxhash)
            skb-&gt;rxhash = 1;

    got_hash:
        // 通过rcu得到对应rps map
        map = rcu_dereference(rxqueue-&gt;rps_map);
        if (map) {
            // 取得对应的cpu
            u16 tcpu = map-&gt;cpus[((u64) skb-&gt;rxhash * map-&gt;len) &gt;&gt; 32];
            // 如果cpu是online的，则返回计算出的这个cpu，否则跳出循环。
            if (cpu_online(tcpu)) {
                cpu = tcpu;
                goto done;
            }
        }

    done:
        rcu_read_unlock();
        // 如果上面失败，则返回-1.
        return cpu;
    }
</code></pre>

<p>然后是enqueue_to_backlog这个方法，首先我们知道在每个cpu都有一个softnet结构，而他有一个input_pkt_queue的队列，以前这个主要是用于非NAPi的驱动的，而这个patch则将这个队列也用与NAPI的处理中了。也就是每个cpu现在都会有一个input_pkt_queue队列，用于保存需要处理的数据包队列。这个队列作用现在是，如果发现不属于当前cpu处理的数据包，则我们可以直接将数据包挂载到他所属的cpu的input_pkt_queue中。</p>

<p>enqueue_to_backlog接受一个skb和cpu为参数，通过cpu来判断skb如何处理。要么加入所属的input_pkt_queue中，要么schecule 软中断。</p>

<p>还有个要注意就是我们知道NAPI为了兼容非NAPI模式，有个backlog的napi_struct结构，也就是非NAPI驱动会schedule backlog这个napi结构，而在enqueue_to_backlog中则是利用了这个结构，也就是它会schedule backlog，因为它会将数据放到input_pkt_queue中，而backlog的pool方法process_backlog就是从input_pkt_queue中取得数据然后交给上层处理。</p>

<p>这里还有一个会用到结构就是 rps_remote_softirq_cpus，它主要是保存了当前cpu上需要去另外的cpu schedule 软中断的cpu 掩码。因为我们可能将要处理的数据包放到了另外的cpu的input queue上，因此我们需要schedule 另外的cpu上的napi(也就是软中断),所以我们需要保存对应的cpu掩码，以便于后面遍历，然后schedule。</p>

<p>而这里为什么mask有两个元素，注释写的很清楚：
<code>
    /*
     * This structure holds the per-CPU mask of CPUs for which IPIs are scheduled
     * to be sent to kick remote softirq processing.  There are two masks since
     * the sending of IPIs must be done with interrupts enabled.  The select field
     * indicates the current mask that enqueue_backlog uses to schedule IPIs.
     * select is flipped before net_rps_action is called while still under lock,
     * net_rps_action then uses the non-selected mask to send the IPIs and clears
     * it without conflicting with enqueue_backlog operation.
     */
    struct rps_remote_softirq_cpus {
        // 对应的cpu掩码
        cpumask_t mask[2];
        // 表示应该使用的数组索引
        int select;
    };
</code></p>

<pre><code>    static int enqueue_to_backlog(struct sk_buff *skb, int cpu)
    {
        struct softnet_data *queue;
        unsigned long flags;
        // 取出传递进来的cpu的softnet-data结构
        queue = &amp;per_cpu(softnet_data, cpu);

        local_irq_save(flags);
        __get_cpu_var(netdev_rx_stat).total++;
        // 自旋锁
        spin_lock(&amp;queue-&gt;input_pkt_queue.lock);
        // 如果保存的队列还没到上限
        if (queue-&gt;input_pkt_queue.qlen &lt;= netdev_max_backlog) {
        // 如果当前队列的输入队列长度不为空
            if (queue-&gt;input_pkt_queue.qlen) {
    enqueue:
                // 将数据包加入到input_pkt_queue中,这里会有一个小问题，我们后面再说。
                __skb_queue_tail(&amp;queue-&gt;input_pkt_queue, skb);
                spin_unlock_irqrestore(&amp;queue-&gt;input_pkt_queue.lock,
                    flags);
                return NET_RX_SUCCESS;
            }

            /* Schedule NAPI for backlog device */
            // 如果可以调度软中断
            if (napi_schedule_prep(&amp;queue-&gt;backlog)) {
                // 首先判断数据包该不该当前的cpu处理
                if (cpu != smp_processor_id()) {
                    // 如果不该，
                    struct rps_remote_softirq_cpus *rcpus =
                        &amp;__get_cpu_var(rps_remote_softirq_cpus);

                    cpu_set(cpu, rcpus-&gt;mask[rcpus-&gt;select]);
                    __raise_softirq_irqoff(NET_RX_SOFTIRQ);
                } else
                    // 如果就是应该当前cpu处理，则直接schedule 软中断，这里可以看到传递进去的是backlog
                    __napi_schedule(&amp;queue-&gt;backlog);
            }
            goto enqueue;
        }

        spin_unlock(&amp;queue-&gt;input_pkt_queue.lock);

        __get_cpu_var(netdev_rx_stat).dropped++;
        local_irq_restore(flags);

        kfree_skb(skb);
        return NET_RX_DROP;
    }
</code></pre>

<p>这里会有一个小问题，那就是假设此时一个属于cpu0的包进入处理，此时我们运行在cpu1,此时将数据包加入到input队列，然后cpu0上面刚好又来了一个cpu0需要处理的数据包，此时由于qlen不为0则又将数据包加入到input队列中，我们会发现cpu0上的napi没机会进行调度了。</p>

<p>google的patch对这个是这样处理的，在软中断处理函数中当数据包处理完毕，会调用net_rps_action来调度前面保存到其他cpu上的input队列。</p>

<p>下面就是代码片断（net_rx_action）</p>

<pre><code>    // 得到对应的rcpus.
    rcpus = &amp;__get_cpu_var(rps_remote_softirq_cpus);
        select = rcpus-&gt;select;
        // 翻转select，防止和enqueue_backlog冲突
        rcpus-&gt;select ^= 1;

        // 打开中断，此时下面的调度才会起作用.
        local_irq_enable();
        // 这个函数里面调度对应的远程cpu的napi.
        net_rps_action(&amp;rcpus-&gt;mask[select]);
</code></pre>

<p>然后就是net_rps_action，这个函数很简单，就是遍历所需要处理的cpu，然后调度napi
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
<span class='line-number'>100</span>
<span class='line-number'>101</span>
<span class='line-number'>102</span>
<span class='line-number'>103</span>
<span class='line-number'>104</span>
<span class='line-number'>105</span>
<span class='line-number'>106</span>
<span class='line-number'>107</span>
<span class='line-number'>108</span>
<span class='line-number'>109</span>
<span class='line-number'>110</span>
<span class='line-number'>111</span>
<span class='line-number'>112</span>
<span class='line-number'>113</span>
<span class='line-number'>114</span>
<span class='line-number'>115</span>
<span class='line-number'>116</span>
<span class='line-number'>117</span>
<span class='line-number'>118</span>
<span class='line-number'>119</span>
<span class='line-number'>120</span>
<span class='line-number'>121</span>
<span class='line-number'>122</span>
<span class='line-number'>123</span>
<span class='line-number'>124</span>
<span class='line-number'>125</span>
<span class='line-number'>126</span>
<span class='line-number'>127</span>
<span class='line-number'>128</span>
<span class='line-number'>129</span>
<span class='line-number'>130</span>
<span class='line-number'>131</span>
<span class='line-number'>132</span>
<span class='line-number'>133</span>
<span class='line-number'>134</span>
<span class='line-number'>135</span>
<span class='line-number'>136</span>
<span class='line-number'>137</span>
<span class='line-number'>138</span>
<span class='line-number'>139</span>
<span class='line-number'>140</span>
<span class='line-number'>141</span>
<span class='line-number'>142</span>
<span class='line-number'>143</span>
<span class='line-number'>144</span>
<span class='line-number'>145</span>
<span class='line-number'>146</span>
<span class='line-number'>147</span>
<span class='line-number'>148</span>
<span class='line-number'>149</span>
<span class='line-number'>150</span>
<span class='line-number'>151</span>
<span class='line-number'>152</span>
<span class='line-number'>153</span>
<span class='line-number'>154</span>
<span class='line-number'>155</span>
<span class='line-number'>156</span>
<span class='line-number'>157</span>
<span class='line-number'>158</span>
<span class='line-number'>159</span>
<span class='line-number'>160</span>
<span class='line-number'>161</span>
<span class='line-number'>162</span>
<span class='line-number'>163</span>
<span class='line-number'>164</span>
<span class='line-number'>165</span>
<span class='line-number'>166</span>
<span class='line-number'>167</span>
<span class='line-number'>168</span>
<span class='line-number'>169</span>
<span class='line-number'>170</span>
<span class='line-number'>171</span>
<span class='line-number'>172</span>
<span class='line-number'>173</span>
<span class='line-number'>174</span>
<span class='line-number'>175</span>
<span class='line-number'>176</span>
<span class='line-number'>177</span>
<span class='line-number'>178</span>
<span class='line-number'>179</span>
<span class='line-number'>180</span>
<span class='line-number'>181</span>
<span class='line-number'>182</span>
<span class='line-number'>183</span>
<span class='line-number'>184</span>
<span class='line-number'>185</span>
<span class='line-number'>186</span>
<span class='line-number'>187</span>
<span class='line-number'>188</span>
<span class='line-number'>189</span>
<span class='line-number'>190</span>
<span class='line-number'>191</span>
<span class='line-number'>192</span>
<span class='line-number'>193</span>
<span class='line-number'>194</span>
<span class='line-number'>195</span>
<span class='line-number'>196</span>
<span class='line-number'>197</span>
<span class='line-number'>198</span>
<span class='line-number'>199</span>
<span class='line-number'>200</span>
<span class='line-number'>201</span>
<span class='line-number'>202</span>
<span class='line-number'>203</span>
<span class='line-number'>204</span>
<span class='line-number'>205</span>
<span class='line-number'>206</span>
<span class='line-number'>207</span>
<span class='line-number'>208</span>
<span class='line-number'>209</span>
<span class='line-number'>210</span>
<span class='line-number'>211</span>
<span class='line-number'>212</span>
<span class='line-number'>213</span>
<span class='line-number'>214</span>
<span class='line-number'>215</span>
<span class='line-number'>216</span>
<span class='line-number'>217</span>
<span class='line-number'>218</span>
<span class='line-number'>219</span>
<span class='line-number'>220</span>
<span class='line-number'>221</span>
<span class='line-number'>222</span>
<span class='line-number'>223</span>
<span class='line-number'>224</span>
<span class='line-number'>225</span>
<span class='line-number'>226</span>
<span class='line-number'>227</span>
<span class='line-number'>228</span>
<span class='line-number'>229</span>
<span class='line-number'>230</span>
<span class='line-number'>231</span>
<span class='line-number'>232</span>
<span class='line-number'>233</span>
<span class='line-number'>234</span>
<span class='line-number'>235</span>
<span class='line-number'>236</span>
<span class='line-number'>237</span>
<span class='line-number'>238</span>
<span class='line-number'>239</span>
<span class='line-number'>240</span>
<span class='line-number'>241</span>
<span class='line-number'>242</span>
<span class='line-number'>243</span>
<span class='line-number'>244</span>
<span class='line-number'>245</span>
<span class='line-number'>246</span>
<span class='line-number'>247</span>
<span class='line-number'>248</span>
<span class='line-number'>249</span>
<span class='line-number'>250</span>
<span class='line-number'>251</span>
<span class='line-number'>252</span>
<span class='line-number'>253</span>
<span class='line-number'>254</span>
<span class='line-number'>255</span>
<span class='line-number'>256</span>
<span class='line-number'>257</span>
<span class='line-number'>258</span>
<span class='line-number'>259</span>
<span class='line-number'>260</span>
<span class='line-number'>261</span>
<span class='line-number'>262</span>
<span class='line-number'>263</span>
<span class='line-number'>264</span>
<span class='line-number'>265</span>
<span class='line-number'>266</span>
<span class='line-number'>267</span>
<span class='line-number'>268</span>
<span class='line-number'>269</span>
<span class='line-number'>270</span>
<span class='line-number'>271</span>
<span class='line-number'>272</span>
<span class='line-number'>273</span>
<span class='line-number'>274</span>
<span class='line-number'>275</span>
<span class='line-number'>276</span>
<span class='line-number'>277</span>
<span class='line-number'>278</span>
<span class='line-number'>279</span>
<span class='line-number'>280</span>
<span class='line-number'>281</span>
<span class='line-number'>282</span>
<span class='line-number'>283</span>
<span class='line-number'>284</span>
<span class='line-number'>285</span>
<span class='line-number'>286</span>
<span class='line-number'>287</span>
<span class='line-number'>288</span>
<span class='line-number'>289</span>
<span class='line-number'>290</span>
<span class='line-number'>291</span>
<span class='line-number'>292</span>
<span class='line-number'>293</span>
<span class='line-number'>294</span>
<span class='line-number'>295</span>
<span class='line-number'>296</span>
<span class='line-number'>297</span>
<span class='line-number'>298</span>
<span class='line-number'>299</span>
<span class='line-number'>300</span>
<span class='line-number'>301</span>
<span class='line-number'>302</span>
<span class='line-number'>303</span>
<span class='line-number'>304</span>
<span class='line-number'>305</span>
<span class='line-number'>306</span>
<span class='line-number'>307</span>
<span class='line-number'>308</span>
<span class='line-number'>309</span>
<span class='line-number'>310</span>
<span class='line-number'>311</span>
<span class='line-number'>312</span>
<span class='line-number'>313</span>
<span class='line-number'>314</span>
<span class='line-number'>315</span>
<span class='line-number'>316</span>
<span class='line-number'>317</span>
<span class='line-number'>318</span>
<span class='line-number'>319</span>
<span class='line-number'>320</span>
<span class='line-number'>321</span>
<span class='line-number'>322</span>
<span class='line-number'>323</span>
<span class='line-number'>324</span>
<span class='line-number'>325</span>
<span class='line-number'>326</span>
<span class='line-number'>327</span>
<span class='line-number'>328</span>
<span class='line-number'>329</span>
<span class='line-number'>330</span>
<span class='line-number'>331</span>
<span class='line-number'>332</span>
<span class='line-number'>333</span>
<span class='line-number'>334</span>
<span class='line-number'>335</span>
<span class='line-number'>336</span>
<span class='line-number'>337</span>
<span class='line-number'>338</span>
<span class='line-number'>339</span>
<span class='line-number'>340</span>
<span class='line-number'>341</span>
<span class='line-number'>342</span>
<span class='line-number'>343</span>
<span class='line-number'>344</span>
<span class='line-number'>345</span>
<span class='line-number'>346</span>
<span class='line-number'>347</span>
<span class='line-number'>348</span>
<span class='line-number'>349</span>
<span class='line-number'>350</span>
<span class='line-number'>351</span>
<span class='line-number'>352</span>
<span class='line-number'>353</span>
<span class='line-number'>354</span>
<span class='line-number'>355</span>
<span class='line-number'>356</span>
<span class='line-number'>357</span>
<span class='line-number'>358</span>
<span class='line-number'>359</span>
<span class='line-number'>360</span>
<span class='line-number'>361</span>
<span class='line-number'>362</span>
<span class='line-number'>363</span>
<span class='line-number'>364</span>
<span class='line-number'>365</span>
<span class='line-number'>366</span>
<span class='line-number'>367</span>
<span class='line-number'>368</span>
<span class='line-number'>369</span>
<span class='line-number'>370</span>
<span class='line-number'>371</span>
<span class='line-number'>372</span>
<span class='line-number'>373</span>
<span class='line-number'>374</span>
<span class='line-number'>375</span>
<span class='line-number'>376</span>
<span class='line-number'>377</span>
<span class='line-number'>378</span>
<span class='line-number'>379</span>
<span class='line-number'>380</span>
<span class='line-number'>381</span>
<span class='line-number'>382</span>
<span class='line-number'>383</span>
<span class='line-number'>384</span>
<span class='line-number'>385</span>
<span class='line-number'>386</span>
<span class='line-number'>387</span>
<span class='line-number'>388</span>
<span class='line-number'>389</span>
<span class='line-number'>390</span>
<span class='line-number'>391</span>
<span class='line-number'>392</span>
<span class='line-number'>393</span>
<span class='line-number'>394</span>
<span class='line-number'>395</span>
<span class='line-number'>396</span>
<span class='line-number'>397</span>
<span class='line-number'>398</span>
<span class='line-number'>399</span>
<span class='line-number'>400</span>
<span class='line-number'>401</span>
<span class='line-number'>402</span>
<span class='line-number'>403</span>
<span class='line-number'>404</span>
<span class='line-number'>405</span>
<span class='line-number'>406</span>
<span class='line-number'>407</span>
<span class='line-number'>408</span>
<span class='line-number'>409</span>
<span class='line-number'>410</span>
<span class='line-number'>411</span>
<span class='line-number'>412</span>
<span class='line-number'>413</span>
<span class='line-number'>414</span>
<span class='line-number'>415</span>
<span class='line-number'>416</span>
<span class='line-number'>417</span>
<span class='line-number'>418</span>
<span class='line-number'>419</span>
<span class='line-number'>420</span>
<span class='line-number'>421</span>
<span class='line-number'>422</span>
<span class='line-number'>423</span>
<span class='line-number'>424</span>
<span class='line-number'>425</span>
<span class='line-number'>426</span>
<span class='line-number'>427</span>
<span class='line-number'>428</span>
<span class='line-number'>429</span>
<span class='line-number'>430</span>
<span class='line-number'>431</span>
<span class='line-number'>432</span>
<span class='line-number'>433</span>
<span class='line-number'>434</span>
<span class='line-number'>435</span>
<span class='line-number'>436</span>
<span class='line-number'>437</span>
<span class='line-number'>438</span>
<span class='line-number'>439</span>
<span class='line-number'>440</span>
<span class='line-number'>441</span>
<span class='line-number'>442</span>
<span class='line-number'>443</span>
<span class='line-number'>444</span>
<span class='line-number'>445</span>
<span class='line-number'>446</span>
<span class='line-number'>447</span>
<span class='line-number'>448</span>
<span class='line-number'>449</span>
<span class='line-number'>450</span>
<span class='line-number'>451</span>
<span class='line-number'>452</span>
<span class='line-number'>453</span>
<span class='line-number'>454</span>
<span class='line-number'>455</span>
<span class='line-number'>456</span>
<span class='line-number'>457</span>
<span class='line-number'>458</span>
<span class='line-number'>459</span>
<span class='line-number'>460</span>
<span class='line-number'>461</span>
<span class='line-number'>462</span>
<span class='line-number'>463</span>
<span class='line-number'>464</span>
<span class='line-number'>465</span>
<span class='line-number'>466</span>
<span class='line-number'>467</span>
<span class='line-number'>468</span>
<span class='line-number'>469</span>
<span class='line-number'>470</span>
<span class='line-number'>471</span>
<span class='line-number'>472</span>
<span class='line-number'>473</span>
<span class='line-number'>474</span>
<span class='line-number'>475</span>
<span class='line-number'>476</span>
<span class='line-number'>477</span>
<span class='line-number'>478</span>
<span class='line-number'>479</span>
<span class='line-number'>480</span>
<span class='line-number'>481</span>
<span class='line-number'>482</span>
<span class='line-number'>483</span>
<span class='line-number'>484</span>
<span class='line-number'>485</span>
<span class='line-number'>486</span>
<span class='line-number'>487</span>
<span class='line-number'>488</span>
<span class='line-number'>489</span>
<span class='line-number'>490</span>
<span class='line-number'>491</span>
<span class='line-number'>492</span>
<span class='line-number'>493</span>
<span class='line-number'>494</span>
<span class='line-number'>495</span>
<span class='line-number'>496</span>
<span class='line-number'>497</span>
<span class='line-number'>498</span>
<span class='line-number'>499</span>
<span class='line-number'>500</span>
<span class='line-number'>501</span>
<span class='line-number'>502</span>
<span class='line-number'>503</span>
<span class='line-number'>504</span>
<span class='line-number'>505</span>
<span class='line-number'>506</span>
<span class='line-number'>507</span>
<span class='line-number'>508</span>
<span class='line-number'>509</span>
<span class='line-number'>510</span>
<span class='line-number'>511</span>
<span class='line-number'>512</span>
<span class='line-number'>513</span>
<span class='line-number'>514</span>
<span class='line-number'>515</span>
<span class='line-number'>516</span>
<span class='line-number'>517</span>
<span class='line-number'>518</span>
<span class='line-number'>519</span>
<span class='line-number'>520</span>
<span class='line-number'>521</span>
<span class='line-number'>522</span>
<span class='line-number'>523</span>
<span class='line-number'>524</span>
<span class='line-number'>525</span>
<span class='line-number'>526</span>
<span class='line-number'>527</span>
<span class='line-number'>528</span>
<span class='line-number'>529</span>
<span class='line-number'>530</span>
<span class='line-number'>531</span>
<span class='line-number'>532</span>
<span class='line-number'>533</span>
<span class='line-number'>534</span>
<span class='line-number'>535</span>
<span class='line-number'>536</span>
<span class='line-number'>537</span>
<span class='line-number'>538</span>
<span class='line-number'>539</span>
<span class='line-number'>540</span>
<span class='line-number'>541</span>
<span class='line-number'>542</span>
<span class='line-number'>543</span>
<span class='line-number'>544</span>
<span class='line-number'>545</span>
<span class='line-number'>546</span>
<span class='line-number'>547</span>
<span class='line-number'>548</span>
<span class='line-number'>549</span>
<span class='line-number'>550</span>
<span class='line-number'>551</span>
<span class='line-number'>552</span>
<span class='line-number'>553</span>
<span class='line-number'>554</span>
<span class='line-number'>555</span>
<span class='line-number'>556</span>
<span class='line-number'>557</span>
<span class='line-number'>558</span>
<span class='line-number'>559</span>
<span class='line-number'>560</span>
<span class='line-number'>561</span>
<span class='line-number'>562</span>
<span class='line-number'>563</span>
<span class='line-number'>564</span>
<span class='line-number'>565</span>
<span class='line-number'>566</span>
<span class='line-number'>567</span>
<span class='line-number'>568</span>
<span class='line-number'>569</span>
<span class='line-number'>570</span>
<span class='line-number'>571</span>
<span class='line-number'>572</span>
<span class='line-number'>573</span>
<span class='line-number'>574</span>
<span class='line-number'>575</span>
<span class='line-number'>576</span>
<span class='line-number'>577</span>
<span class='line-number'>578</span>
<span class='line-number'>579</span>
<span class='line-number'>580</span>
<span class='line-number'>581</span>
<span class='line-number'>582</span>
<span class='line-number'>583</span>
<span class='line-number'>584</span>
<span class='line-number'>585</span>
<span class='line-number'>586</span>
<span class='line-number'>587</span>
<span class='line-number'>588</span>
<span class='line-number'>589</span>
<span class='line-number'>590</span>
<span class='line-number'>591</span>
<span class='line-number'>592</span>
<span class='line-number'>593</span>
<span class='line-number'>594</span>
<span class='line-number'>595</span>
<span class='line-number'>596</span>
<span class='line-number'>597</span>
<span class='line-number'>598</span>
<span class='line-number'>599</span>
<span class='line-number'>600</span>
<span class='line-number'>601</span>
<span class='line-number'>602</span>
<span class='line-number'>603</span>
<span class='line-number'>604</span>
<span class='line-number'>605</span>
<span class='line-number'>606</span>
<span class='line-number'>607</span>
<span class='line-number'>608</span>
<span class='line-number'>609</span>
<span class='line-number'>610</span>
<span class='line-number'>611</span>
<span class='line-number'>612</span>
<span class='line-number'>613</span>
<span class='line-number'>614</span>
<span class='line-number'>615</span>
<span class='line-number'>616</span>
<span class='line-number'>617</span>
<span class='line-number'>618</span>
<span class='line-number'>619</span>
<span class='line-number'>620</span>
<span class='line-number'>621</span>
<span class='line-number'>622</span>
<span class='line-number'>623</span>
<span class='line-number'>624</span>
<span class='line-number'>625</span>
<span class='line-number'>626</span>
<span class='line-number'>627</span>
<span class='line-number'>628</span>
<span class='line-number'>629</span>
<span class='line-number'>630</span>
<span class='line-number'>631</span>
<span class='line-number'>632</span>
<span class='line-number'>633</span>
<span class='line-number'>634</span>
<span class='line-number'>635</span>
<span class='line-number'>636</span>
<span class='line-number'>637</span>
<span class='line-number'>638</span>
<span class='line-number'>639</span>
<span class='line-number'>640</span>
<span class='line-number'>641</span>
<span class='line-number'>642</span>
<span class='line-number'>643</span>
<span class='line-number'>644</span>
<span class='line-number'>645</span>
<span class='line-number'>646</span>
<span class='line-number'>647</span>
<span class='line-number'>648</span>
<span class='line-number'>649</span>
<span class='line-number'>650</span>
<span class='line-number'>651</span>
<span class='line-number'>652</span>
<span class='line-number'>653</span>
<span class='line-number'>654</span>
<span class='line-number'>655</span>
<span class='line-number'>656</span>
<span class='line-number'>657</span>
<span class='line-number'>658</span>
<span class='line-number'>659</span>
<span class='line-number'>660</span>
<span class='line-number'>661</span>
<span class='line-number'>662</span>
<span class='line-number'>663</span>
<span class='line-number'>664</span>
<span class='line-number'>665</span>
<span class='line-number'>666</span>
<span class='line-number'>667</span>
<span class='line-number'>668</span>
<span class='line-number'>669</span>
<span class='line-number'>670</span>
<span class='line-number'>671</span>
<span class='line-number'>672</span>
<span class='line-number'>673</span>
<span class='line-number'>674</span>
<span class='line-number'>675</span>
<span class='line-number'>676</span>
<span class='line-number'>677</span>
<span class='line-number'>678</span>
<span class='line-number'>679</span>
<span class='line-number'>680</span>
<span class='line-number'>681</span>
<span class='line-number'>682</span>
<span class='line-number'>683</span>
<span class='line-number'>684</span>
<span class='line-number'>685</span>
<span class='line-number'>686</span>
<span class='line-number'>687</span>
<span class='line-number'>688</span>
<span class='line-number'>689</span>
<span class='line-number'>690</span>
<span class='line-number'>691</span>
<span class='line-number'>692</span>
<span class='line-number'>693</span>
<span class='line-number'>694</span>
<span class='line-number'>695</span>
<span class='line-number'>696</span>
<span class='line-number'>697</span>
<span class='line-number'>698</span>
<span class='line-number'>699</span>
<span class='line-number'>700</span>
<span class='line-number'>701</span>
<span class='line-number'>702</span>
<span class='line-number'>703</span>
<span class='line-number'>704</span>
<span class='line-number'>705</span>
<span class='line-number'>706</span>
<span class='line-number'>707</span>
<span class='line-number'>708</span>
<span class='line-number'>709</span>
<span class='line-number'>710</span>
<span class='line-number'>711</span>
<span class='line-number'>712</span>
<span class='line-number'>713</span>
<span class='line-number'>714</span>
<span class='line-number'>715</span>
<span class='line-number'>716</span>
<span class='line-number'>717</span>
<span class='line-number'>718</span>
<span class='line-number'>719</span>
<span class='line-number'>720</span>
<span class='line-number'>721</span>
<span class='line-number'>722</span>
<span class='line-number'>723</span>
<span class='line-number'>724</span>
<span class='line-number'>725</span>
<span class='line-number'>726</span>
<span class='line-number'>727</span>
<span class='line-number'>728</span>
<span class='line-number'>729</span>
<span class='line-number'>730</span>
<span class='line-number'>731</span>
<span class='line-number'>732</span>
<span class='line-number'>733</span>
<span class='line-number'>734</span>
<span class='line-number'>735</span>
<span class='line-number'>736</span>
<span class='line-number'>737</span>
<span class='line-number'>738</span>
<span class='line-number'>739</span>
<span class='line-number'>740</span>
<span class='line-number'>741</span>
<span class='line-number'>742</span>
<span class='line-number'>743</span>
<span class='line-number'>744</span>
<span class='line-number'>745</span>
<span class='line-number'>746</span>
<span class='line-number'>747</span>
<span class='line-number'>748</span>
<span class='line-number'>749</span>
<span class='line-number'>750</span>
<span class='line-number'>751</span>
<span class='line-number'>752</span>
<span class='line-number'>753</span>
<span class='line-number'>754</span>
<span class='line-number'>755</span>
<span class='line-number'>756</span>
<span class='line-number'>757</span>
<span class='line-number'>758</span>
<span class='line-number'>759</span>
<span class='line-number'>760</span>
<span class='line-number'>761</span>
<span class='line-number'>762</span>
<span class='line-number'>763</span>
<span class='line-number'>764</span>
<span class='line-number'>765</span>
<span class='line-number'>766</span>
<span class='line-number'>767</span>
<span class='line-number'>768</span>
<span class='line-number'>769</span>
<span class='line-number'>770</span>
<span class='line-number'>771</span>
<span class='line-number'>772</span>
<span class='line-number'>773</span>
<span class='line-number'>774</span>
<span class='line-number'>775</span>
<span class='line-number'>776</span>
<span class='line-number'>777</span>
<span class='line-number'>778</span>
<span class='line-number'>779</span>
<span class='line-number'>780</span>
<span class='line-number'>781</span>
<span class='line-number'>782</span>
<span class='line-number'>783</span>
<span class='line-number'>784</span>
<span class='line-number'>785</span>
<span class='line-number'>786</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>static void net_rps_action(cpumask_t *mask)
</span><span class='line'>{
</span><span class='line'>    int cpu;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    /* Send pending IPI's to kick RPS processing on remote cpus. */
</span><span class='line'>// 遍历
</span><span class='line'>for_each_cpu_mask_nr(cpu, *mask) {
</span><span class='line'>    struct softnet_data *queue = &amp;per_cpu(softnet_data, cpu);
</span><span class='line'>    if (cpu_online(cpu))
</span><span class='line'>        // 到对应的cpu调用csd方法。
</span><span class='line'>        __smp_call_function_single(cpu, &amp;queue-&gt;csd, 0);
</span><span class='line'>}
</span><span class='line'>// 清理mask
</span><span class='line'>cpus_clear(*mask);
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;
</span><span class='line'>
</span><span class='line'>上面我们看到会调用csd方法，而上面的csd回掉就是被初始化为trigger_softirq函数。
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;static void trigger_softirq(void *data)
</span><span class='line'>{
</span><span class='line'>struct softnet_data *queue = data;
</span><span class='line'>// 调度napi可以看到依旧是backlog 这个napi结构体。
</span><span class='line'>__napi_schedule(&amp;queue-&gt;backlog);
</span><span class='line'>__get_cpu_var(netdev_rx_stat).received_rps++;
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;
</span><span class='line'>
</span><span class='line'>上面的函数都分析完毕了，剩下的就很简单了。
</span><span class='line'>
</span><span class='line'>首先来看netif_rx如何被修改的，它被修改的很简单，首先是得到当前skb所应该被处理的cpu id，然后再通过比较这个cpu和当前正在处理的cpu id进行比较来做不同的处理。
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;int netif_rx(struct sk_buff *skb)
</span><span class='line'>{
</span><span class='line'>int cpu;
</span><span class='line'>
</span><span class='line'>/* if netpoll wants it, pretend we never saw it */
</span><span class='line'>if (netpoll_rx(skb))
</span><span class='line'>    return NET_RX_DROP;
</span><span class='line'>
</span><span class='line'>if (!skb-&gt;tstamp.tv64)
</span><span class='line'>    net_timestamp(skb);
</span><span class='line'>// 得到cpu id。
</span><span class='line'>cpu = get_rps_cpu(skb-&gt;dev, skb);
</span><span class='line'>if (cpu &lt; 0)
</span><span class='line'>    cpu = smp_processor_id();
</span><span class='line'>// 通过cpu进行队列不同的处理
</span><span class='line'>return enqueue_to_backlog(skb, cpu);
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;
</span><span class='line'>
</span><span class='line'>然后是netif_receive_skb,这里patch将内核本身的这个函数改写为__netif_receive_skb。然后当返回值小于0,则说明不需要对队列进行处理，此时直接发送到3层。
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;int netif_receive_skb(struct sk_buff *skb)
</span><span class='line'>{
</span><span class='line'>int cpu;
</span><span class='line'>
</span><span class='line'>cpu = get_rps_cpu(skb-&gt;dev, skb);
</span><span class='line'>
</span><span class='line'>if (cpu &lt; 0)
</span><span class='line'>    return __netif_receive_skb(skb);
</span><span class='line'>else
</span><span class='line'>    return enqueue_to_backlog(skb, cpu);
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;```&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;最后来总结一下，可以看到input_pkt_queue是一个FIFO的队列，而且如果当qlen有值的时候，也就是在另外的cpu有数据包放到input_pkt_queue中，则当前cpu不会调度napi，而是将数据包放到input_pkt_queue中，然后等待trigger_softirq来调度napi。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;因此这个patch完美的解决了软中断在多核下的均衡问题，并且没有由于是同一个连接会map到相同的cpu，并且input_pkt_queue的使用，因此乱序的问题也不会出现。&lt;/p&gt;
</span><span class='line'>]]&gt;&lt;/content&gt;
</span><span class='line'>  &lt;/entry&gt;
</span><span class='line'>  
</span><span class='line'>  &lt;entry&gt;
</span><span class='line'>&lt;title type="html"&gt;&lt;![CDATA[内核协议栈tcp层的内存管理]]&gt;&lt;/title&gt;
</span><span class='line'>&lt;link href="http://abcdxyzk.github.io/blog/2015/06/03/kernel-net-mem/"/&gt;
</span><span class='line'>&lt;updated&gt;2015-06-03T14:25:00+08:00&lt;/updated&gt;
</span><span class='line'>&lt;id&gt;http://abcdxyzk.github.io/blog/2015/06/03/kernel-net-mem&lt;/id&gt;
</span><span class='line'>&lt;content type="html"&gt;&lt;![CDATA[&lt;p&gt;&lt;a href="http://simohayha.iteye.com/blog/532450"&gt;http://simohayha.iteye.com/blog/532450&lt;/a&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;&lt;a href="http://www.ibm.com/developerworks/cn/linux/l-hisock.html#table1"&gt;http://www.ibm.com/developerworks/cn/linux/l-hisock.html#table1&lt;/a&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;&lt;a href="http://blog.csdn.net/russell_tao/article/details/18711023"&gt;http://blog.csdn.net/russell_tao/article/details/18711023&lt;/a&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;我们先来看tcp内存管理相关的几个内核参数,这些都能通过proc文件系统来修改:
</span><span class='line'>&lt;code&gt;
</span><span class='line'>// 内核写buf的最大值.
</span><span class='line'>extern __u32 sysctl_wmem_max;
</span><span class='line'>// 协议栈读buf的最大值
</span><span class='line'>extern __u32 sysctl_rmem_max;
</span><span class='line'>&lt;/code&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;这两个值在/proc/sys/net/core 下。这里要注意，这两个值的单位是字节。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;它们的初始化在sk_init里面,这里可以看到这两个值的大小是依赖于num_physpages的，而这个值应该是物理页数。也就是说这两个值依赖于物理内存：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    void __init sk_init(void)
</span><span class='line'>{
</span><span class='line'>    if (num_physpages &lt;= 4096) {
</span><span class='line'>        sysctl_wmem_max = 32767;
</span><span class='line'>        sysctl_rmem_max = 32767;
</span><span class='line'>        sysctl_wmem_default = 32767;
</span><span class='line'>        sysctl_rmem_default = 32767;
</span><span class='line'>    } else if (num_physpages &gt;= 131072) {
</span><span class='line'>        sysctl_wmem_max = 131071;
</span><span class='line'>        sysctl_rmem_max = 131071;
</span><span class='line'>    }
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;而我通过搜索源码，只有设置套接口选项的时候，才会用到这两个值，也就是setsockopt，optname为SO_SNDBUF或者SO_RCVBUF时，来限制设置的值:&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    case SO_SNDBUF:
</span><span class='line'>        if (val &gt; sysctl_wmem_max)
</span><span class='line'>            val = sysctl_wmem_max;
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;接下来就是整个tcp协议栈的socket的buf限制(也就是所有的socket).
</span><span class='line'>这里要注意，这个东西的单位都是以页为单位的，我们下面就会看到。
</span><span class='line'>&lt;code&gt;
</span><span class='line'>其中sysctl_tcp_mem[0]表示整个tcp sock的buf限制.
</span><span class='line'>sysctl_tcp_mem[1]也就是tcp sock内存使用的警戒线.
</span><span class='line'>sysctl_tcp_mem[2]也就是tcp sock内存使用的hard limit,当超过这个限制,我们就要禁止再分配buf.
</span><span class='line'>extern int sysctl_tcp_mem[3];
</span><span class='line'>&lt;/code&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;接下来就是针对每个sock的读写buf限制。
</span><span class='line'>&lt;code&gt;
</span><span class='line'>// 其中依次为最小buf,中等buf,以及最大buf.
</span><span class='line'>extern int sysctl_tcp_wmem[3];
</span><span class='line'>extern int sysctl_tcp_rmem[3];
</span><span class='line'>&lt;/code&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h4&gt;tcp_init&lt;/h4&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;这几个值的初始化在tcp_init里面，这里就能清晰的看到sysctl_tcp_mem的单位是页。而sysctl_tcp_wmem和sysctl_tcp_rmem的单位是字节。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    void __init tcp_init(void)
</span><span class='line'>{
</span><span class='line'>    .................................
</span><span class='line'>    // nr_pages就是页。
</span><span class='line'>    nr_pages = totalram_pages - totalhigh_pages;
</span><span class='line'>    limit = min(nr_pages, 1UL&lt;&lt;(28-PAGE_SHIFT)) &gt;&gt; (20-PAGE_SHIFT);
</span><span class='line'>    limit = (limit * (nr_pages &gt;&gt; (20-PAGE_SHIFT))) &gt;&gt; (PAGE_SHIFT-11);
</span><span class='line'>    limit = max(limit, 128UL);
</span><span class='line'>    sysctl_tcp_mem[0] = limit / 4 * 3;
</span><span class='line'>    sysctl_tcp_mem[1] = limit;
</span><span class='line'>    sysctl_tcp_mem[2] = sysctl_tcp_mem[0] * 2;
</span><span class='line'>
</span><span class='line'>    /* Set per-socket limits to no more than 1/128 the pressure threshold */
</span><span class='line'>    // 转换为字节。
</span><span class='line'>    limit = ((unsigned long)sysctl_tcp_mem[1]) &lt;&lt; (PAGE_SHIFT - 7);
</span><span class='line'>    max_share = min(4UL*1024*1024, limit);
</span><span class='line'>
</span><span class='line'>    sysctl_tcp_wmem[0] = SK_MEM_QUANTUM;
</span><span class='line'>    sysctl_tcp_wmem[1] = 16*1024;
</span><span class='line'>    sysctl_tcp_wmem[2] = max(64*1024, max_share);
</span><span class='line'>
</span><span class='line'>    sysctl_tcp_rmem[0] = SK_MEM_QUANTUM;
</span><span class='line'>    sysctl_tcp_rmem[1] = 87380;
</span><span class='line'>    sysctl_tcp_rmem[2] = max(87380, max_share);
</span><span class='line'>    ................................
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;然后就是读写buf的最小值
</span><span class='line'>&lt;code&gt;
</span><span class='line'>#define SOCK_MIN_SNDBUF 2048
</span><span class='line'>#define SOCK_MIN_RCVBUF 256
</span><span class='line'>&lt;/code&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;最后就是当前tcp协议栈已经分配了的buf的总大小。这里要注意，这个值也是以页为单位的。
</span><span class='line'>&lt;code&gt;
</span><span class='line'>atomic_t tcp_memory_allocated
</span><span class='line'>&lt;/code&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;而上面的这些值如何与协议栈关联起来呢，我们来看tcp_prot结构，可以看到这些值的地址都被放到对应的tcp_prot的域。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    struct proto tcp_prot = {
</span><span class='line'>    .name = "TCP",
</span><span class='line'>    .owner = THIS_MODULE,
</span><span class='line'>    ...................................................
</span><span class='line'>    .enter_memory_pressure = tcp_enter_memory_pressure,
</span><span class='line'>    .sockets_allocated = &amp;tcp_sockets_allocated,
</span><span class='line'>    .orphan_count = &amp;tcp_orphan_count,
</span><span class='line'>    .memory_allocated = &amp;tcp_memory_allocated,
</span><span class='line'>    .memory_pressure = &amp;tcp_memory_pressure,
</span><span class='line'>    .sysctl_mem = sysctl_tcp_mem,
</span><span class='line'>    .sysctl_wmem = sysctl_tcp_wmem,
</span><span class='line'>    .sysctl_rmem = sysctl_tcp_rmem,
</span><span class='line'>    ........................................................
</span><span class='line'>};
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;而对应的sock域中的几个值，这几个域非常重要，我们来看他们表示的含义&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;sk_rcvbuf和sk_sndbuf,这两个值分别代表每个sock的读写buf的最大限制&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;sk_rmem_alloc和sk_wmem_alloc这两个值分别代表已经提交的数据包的字节数。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;读buf意味着进入tcp层的数据大小，而当数据提交给用户空间之后，这个值会相应的减去提交的大小（也就类似写buf的sk_wmem_queued)。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;写buf意味着提交给ip层。可以看到这个值的增加是在tcp_transmit_skb中进行的。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;而sk_wmem_queued也就代表skb的写队列write_queue的大小。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;还有一个sk_forward_alloc，这个值表示一个预分配置，也就是整个tcp协议栈的内存cache，第一次为一个缓冲区分配buf的时候，我们不会直接分配精确的大小，而是按页来分配，而分配的大小就是这个值，下面我们会看到这个。并且这个值初始是0.&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    struct sock {
</span><span class='line'>    int sk_rcvbuf;
</span><span class='line'>    atomic_t sk_rmem_alloc;
</span><span class='line'>    atomic_t sk_wmem_alloc;
</span><span class='line'>    int sk_forward_alloc;
</span><span class='line'>    ..........................
</span><span class='line'>    int sk_sndbuf;
</span><span class='line'>    // 这个表示写buf已经分配的字节长度
</span><span class='line'>    int sk_wmem_queued;
</span><span class='line'>    ...........................
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;sk_sndbuf和sk_rcvbuf,这两个的初始化在这里：
</span><span class='line'>&lt;code&gt;
</span><span class='line'>static int tcp_v4_init_sock(struct sock *sk)
</span><span class='line'>{
</span><span class='line'>    ..................................
</span><span class='line'>    sk-&gt;sk_sndbuf = sysctl_tcp_wmem[1];
</span><span class='line'>    sk-&gt;sk_rcvbuf = sysctl_tcp_rmem[1];
</span><span class='line'>    ..........................
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;而当进入establish状态之后,sock会自己调整sndbuf和rcvbuf.他是通过tcp_init_buffer_space来进行调整的.这个函数会调用tcp_fixup_rcvbuf和tcp_fixup_sndbuf来调整读写buf的大小.&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;这里有用到sk_userlock这个标记，这个标记主要就是用来标记SO_SNDBUF 和SO_RCVBUF套接口选项是否被设置。而是否设置对应的值为：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    #define SOCK_SNDBUF_LOCK    1
</span><span class='line'>#define SOCK_RCVBUF_LOCK    2
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;我们可以看下面的设置SO_SNDBUF 和SO_RCVBUF的代码片断：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    // 首先设置sk_userlocks.
</span><span class='line'>sk-&gt;sk_userlocks |= SOCK_SNDBUF_LOCK;
</span><span class='line'>if ((val * 2) &lt; SOCK_MIN_SNDBUF)
</span><span class='line'>    sk-&gt;sk_sndbuf = SOCK_MIN_SNDBUF;
</span><span class='line'>else
</span><span class='line'>    sk-&gt;sk_sndbuf = val * 2;
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;因此内核里面的处理是这样的，如果用户已经通过套接字选项设置了读或者写buf的大小，那么这里将不会调整读写buf的大小，否则就进入tcp_fixup_XXX来调整大小。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;还有一个要注意的就是MAX_TCP_HEADER，这个值表示了TCP + IP + link layer headers 以及option的长度。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;我们来看代码。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    static void tcp_init_buffer_space(struct sock *sk)
</span><span class='line'>{
</span><span class='line'>    struct tcp_sock *tp = tcp_sk(sk);
</span><span class='line'>    int maxwin;
</span><span class='line'>
</span><span class='line'>    // 判断sk_userlocks，来决定是否需要fix缓冲区大小。
</span><span class='line'>    if (!(sk-&gt;sk_userlocks &amp; SOCK_RCVBUF_LOCK))
</span><span class='line'>        tcp_fixup_rcvbuf(sk);
</span><span class='line'>    if (!(sk-&gt;sk_userlocks &amp; SOCK_SNDBUF_LOCK))
</span><span class='line'>        tcp_fixup_sndbuf(sk);
</span><span class='line'>......................................
</span><span class='line'>
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;接下来来看这两个函数如何来调整读写buf的大小，不过这里还有些疑问，就是为什么是要和3&lt;em&gt;sndmem以及4&lt;/em&gt;rcvmem：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    static void tcp_fixup_sndbuf(struct sock *sk)
</span><span class='line'>{
</span><span class='line'>    // 首先通过mss，tcp头，以及sk_buff的大小，得到一个最小范围的sndmem。
</span><span class='line'>    int sndmem = tcp_sk(sk)-&gt;rx_opt.mss_clamp + MAX_TCP_HEADER + 16 +sizeof(struct sk_buff);
</span><span class='line'>
</span><span class='line'>    // 然后取sysctl_tcp_wmem[2]和3倍的sndmem之间的最小值。
</span><span class='line'>    if (sk-&gt;sk_sndbuf &lt; 3 * sndmem)
</span><span class='line'>        sk-&gt;sk_sndbuf = min(3 * sndmem, sysctl_tcp_wmem[2]);
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>static void tcp_fixup_rcvbuf(struct sock *sk)
</span><span class='line'>{
</span><span class='line'>    struct tcp_sock *tp = tcp_sk(sk);
</span><span class='line'>    // 这里和上面类似，也是先得到最小的一个rcvmem段。
</span><span class='line'>    int rcvmem = tp-&gt;advmss + MAX_TCP_HEADER + 16 + sizeof(struct sk_buff);
</span><span class='line'>
</span><span class='line'>    /* Try to select rcvbuf so that 4 mss-sized segments
</span><span class='line'>     * will fit to window and corresponding skbs will fit to our rcvbuf.
</span><span class='line'>     * (was 3; 4 is minimum to allow fast retransmit to work.)
</span><span class='line'>     */
</span><span class='line'>    // 这里则是通过sysctl_tcp_adv_win_scale来调整rcvmem的值。
</span><span class='line'>    while (tcp_win_from_space(rcvmem) &lt; tp-&gt;advmss)
</span><span class='line'>        rcvmem += 128;
</span><span class='line'>    if (sk-&gt;sk_rcvbuf &lt; 4 * rcvmem)
</span><span class='line'>        sk-&gt;sk_rcvbuf = min(4 * rcvmem, sysctl_tcp_rmem[2]);
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;ok，看完初始化，我们来看协议栈具体如何管理内存的，先来看发送端，发送端的主要实现是在tcp_sendmsg里面，这个函数我们前面已经详细的分析过了，我们这次只分析里面几个与内存相关的东西。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;来看代码片断：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    int tcp_sendmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg,
</span><span class='line'>        size_t size)
</span><span class='line'>{
</span><span class='line'>    ..................................
</span><span class='line'>
</span><span class='line'>    if (copy &lt;= 0) {
</span><span class='line'>new_segment:
</span><span class='line'>        if (!sk_stream_memory_free(sk))
</span><span class='line'>            goto wait_for_sndbuf;
</span><span class='line'>
</span><span class='line'>        skb = sk_stream_alloc_skb(sk, select_size(sk),
</span><span class='line'>        sk-&gt;sk_allocation);
</span><span class='line'>        if (sk-&gt;sk_route_caps &amp; NETIF_F_ALL_CSUM)
</span><span class='line'>            skb-&gt;ip_summed = CHECKSUM_PARTIAL;
</span><span class='line'>
</span><span class='line'>        skb_entail(sk, skb);
</span><span class='line'>        copy = size_goal;
</span><span class='line'>        max = size_goal;
</span><span class='line'>    ..................
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;可以看到这里第一个sk_stream_memory_free用来判断是否还有空间来供我们分配，如果没有则跳到wait_for_sndbuf来等待buf的释放。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;然后如果有空间供我们分配，则调用sk_stream_alloc_skb来分配一个skb，然后这个大小的选择是通过select_size。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;最后调用skb_entail来更新相关的域。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;现在我们就来详细看上面的四个函数,先来看第一个：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    static inline int sk_stream_memory_free(struct sock *sk)
</span><span class='line'>{
</span><span class='line'>    return sk-&gt;sk_wmem_queued &lt; sk-&gt;sk_sndbuf;
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;sk_stream_memory_free实现很简单，就是判断当前已经分配的写缓冲区的大小(sk_wmem_queued)是否小于当前写缓冲区(sk_sndbuf)的最大限制。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;然后是skb_entail，这个函数主要是当我们分配完buf后，进行一些相关域的更新，以及添加skb到writequeue。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    static inline void skb_entail(struct sock *sk, struct sk_buff *skb)
</span><span class='line'>{
</span><span class='line'>    struct tcp_sock *tp = tcp_sk(sk);
</span><span class='line'>    struct tcp_skb_cb *tcb = TCP_SKB_CB(skb);
</span><span class='line'>    ............................
</span><span class='line'>    skb_header_release(skb);
</span><span class='line'>    tcp_add_write_queue_tail(sk, skb);
</span><span class='line'>    // 增加sk_wmem_queued.
</span><span class='line'>    sk-&gt;sk_wmem_queued += skb-&gt;truesize;
</span><span class='line'>    // 这里调整sk_forward_alloc的大小，也就是预分配buf的大小(减小).
</span><span class='line'>    sk_mem_charge(sk, skb-&gt;truesize);
</span><span class='line'>    if (tp-&gt;nonagle &amp; TCP_NAGLE_PUSH)
</span><span class='line'>        tp-&gt;nonagle &amp;= ~TCP_NAGLE_PUSH;
</span><span class='line'>}
</span><span class='line'>// 这个函数很简单，就是将sk_forward_alloc - size.
</span><span class='line'>static inline void sk_mem_charge(struct sock *sk, int size)
</span><span class='line'>{
</span><span class='line'>    if (!sk_has_account(sk))
</span><span class='line'>        return;
</span><span class='line'>    sk-&gt;sk_forward_alloc -= size;
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;然后是select_size，在看这个之前我们先来坎SKB_MAX_HEAD的实现.
</span><span class='line'>SKB_MAX_HEAD主要是得到要分配的tcp数据段（不包括头)在一页中最大为多少。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    #define SKB_WITH_OVERHEAD(X)    \
</span><span class='line'>    ((X) - SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))
</span><span class='line'>#define SKB_MAX_ORDER(X, ORDER) \
</span><span class='line'>    SKB_WITH_OVERHEAD((PAGE_SIZE &lt;&lt; (ORDER)) - (X))
</span><span class='line'>#define SKB_MAX_HEAD(X)  (SKB_MAX_ORDER((X), 0))
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;我们带入代码来看，我们下面的代码是SKB_MAX_HEAD(MAX_TCP_HEADER)，展开这个宏可以看到就是PAGE_SIZE-MAX_TCP_HEADER-SKB_DATA_ALIGN(sizeof(struct skb_shared_info).其实也就是一页还能容纳多少tcp的数据。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    static inline int select_size(struct sock *sk)
</span><span class='line'>{
</span><span class='line'>    struct tcp_sock *tp = tcp_sk(sk);
</span><span class='line'>    // 首先取得存储的mss。
</span><span class='line'>    int tmp = tp-&gt;mss_cache;
</span><span class='line'>
</span><span class='line'>    // 然后判断是否使用scatter–gather(前面blog有介绍)
</span><span class='line'>    if (sk-&gt;sk_route_caps &amp; NETIF_F_SG) {
</span><span class='line'>        if (sk_can_gso(sk))
</span><span class='line'>            tmp = 0;
</span><span class='line'>        else {
</span><span class='line'>            // 然后开始计算buf的长度。
</span><span class='line'>            int pgbreak = SKB_MAX_HEAD(MAX_TCP_HEADER);
</span><span class='line'>
</span><span class='line'>            // 如果mss大于pgbreak,那么说明我们一页放不下当前需要的tcp数据，因此我们将会在skb的页区域分配，而skb的页区域是有限制的，因此tmp必须小于这个值。
</span><span class='line'>            if (tmp &gt;= pgbreak &amp;&amp;
</span><span class='line'>                    tmp &lt;= pgbreak + (MAX_SKB_FRAGS - 1) * PAGE_SIZE)
</span><span class='line'>                tmp = pgbreak;
</span><span class='line'>        }
</span><span class='line'>    }
</span><span class='line'>
</span><span class='line'>    return tmp;
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;h4&gt;sk_stream_alloc_skb&lt;/h4&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;接下来来看sk_stream_alloc_skb的实现。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;1 它会调用alloc_skb_fclone来分配内存，这个函数就不详细分析了，我们只需要知道它会从slab里分配一块内存，而大小为size+max_header(上面的分析我们知道slect_size只计算数据段).&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;2 如果分配成功，则调用sk_wmem_schedule来判断我们所分配的skb的大小是否精确，是的话，就调整指针，然后返回。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;3 否则调用tcp_enter_memory_pressure设置标志进入TCP memory pressure zone。然后再调用sk_stream_moderate_sndbuf调整sndbuf(缩小sndbuf)。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    struct sk_buff *sk_stream_alloc_skb(struct sock *sk, int size, gfp_t gfp)
</span><span class='line'>{
</span><span class='line'>    struct sk_buff *skb;
</span><span class='line'>
</span><span class='line'>    // 4字节对其
</span><span class='line'>    size = ALIGN(size, 4);
</span><span class='line'>    // 分配skb。
</span><span class='line'>    skb = alloc_skb_fclone(size + sk-&gt;sk_prot-&gt;max_header, gfp);
</span><span class='line'>    if (skb) {
</span><span class='line'>        // 得到精确的大小。
</span><span class='line'>        if (sk_wmem_schedule(sk, skb-&gt;truesize)) {
</span><span class='line'>            // 返回skb。
</span><span class='line'>            skb_reserve(skb, skb_tailroom(skb) - size);
</span><span class='line'>                return skb;
</span><span class='line'>        }
</span><span class='line'>        __kfree_skb(skb);
</span><span class='line'>    } else {
</span><span class='line'>        // 否则设置全局标记进入pressure zone
</span><span class='line'>        sk-&gt;sk_prot-&gt;enter_memory_pressure(sk);
</span><span class='line'>        sk_stream_moderate_sndbuf(sk);
</span><span class='line'>    }
</span><span class='line'>    return NULL;
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;ok,现在就来看上面的几个函数的实现。先来看几个简单的。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;首先是tcp_enter_memory_pressure,这个函数很简单，就是判断全局标记tcp_memory_pressure,然后设置这个标记。这个标记主要是用来通知其他模块调整的，比如窗口大小等等，详细的话自己搜索这个值，就知道了。
</span><span class='line'>&lt;code&gt;
</span><span class='line'>void tcp_enter_memory_pressure(struct sock *sk)
</span><span class='line'>{
</span><span class='line'>    if (!tcp_memory_pressure) {
</span><span class='line'>        NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPMEMORYPRESSURES);
</span><span class='line'>        // 设置压力标志。
</span><span class='line'>        tcp_memory_pressure = 1;
</span><span class='line'>    }
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;然后是sk_stream_moderate_sndbuf，这个函数也是要使用sk_userlocks,来判断是否已经被用户设置了。可以看到如果我们自己设置过了snd_buf的话，内核就不会帮我们调整它的大小了。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    static inline void sk_stream_moderate_sndbuf(struct sock *sk)
</span><span class='line'>{
</span><span class='line'>    if (!(sk-&gt;sk_userlocks &amp; SOCK_SNDBUF_LOCK)) {
</span><span class='line'>        // 它的大小调整为大于最小值，小于sk-&gt;sk_wmem_queued &gt;&gt; 1。
</span><span class='line'>        sk-&gt;sk_sndbuf = min(sk-&gt;sk_sndbuf, sk-&gt;sk_wmem_queued &gt;&gt; 1);
</span><span class='line'>        sk-&gt;sk_sndbuf = max(sk-&gt;sk_sndbuf, SOCK_MIN_SNDBUF);
</span><span class='line'>    }
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;h4&gt;sk_wmem_schedule&lt;/h4&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;最后来看最核心的一个函数sk_wmem_schedule，这个函数只是对&lt;code&gt;__sk_mem_schedule&lt;/code&gt;的简单封装。这里要知道传递进来的size是skb-&gt;truesize，也就是所分配的skb的真实大小。并且第一次进入这个函数，也就是分配第一个缓冲区包时，sk_forward_alloc是为0的，也就是说，第一次必然会执行&lt;code&gt;__sk_mem_schedule&lt;/code&gt;函数。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    static inline int sk_wmem_schedule(struct sock *sk, int size)
</span><span class='line'>{
</span><span class='line'>    if (!sk_has_account(sk))
</span><span class='line'>        return 1;
</span><span class='line'>    // 先比较size(也就是skb-&gt;truesize)和预分配的内存大小。如果小于等于预分配的大小，则直接返回，否则调用__sk_mem_schedule进行调整。
</span><span class='line'>    return size &lt;= sk-&gt;sk_forward_alloc ||
</span><span class='line'>        __sk_mem_schedule(sk, size, SK_MEM_SEND);
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;来看&lt;code&gt;__sk_mem_schedule&lt;/code&gt;，这个函数的功能注释写的很清楚：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;increase sk_forward_alloc and memory_allocated&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;然后来看源码。这里在看之前，我们要知道，协议栈通过读写buf的使用量，划分了3个区域，或者说标志。不同标志进行不同处理。这里的区域的划分是通过sysctl_tcp_mem，也就是prot-&gt;sysctl_mem这个数组进行的。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    // 页的大小
</span><span class='line'>#define SK_MEM_QUANTUM ((int)PAGE_SIZE)
</span><span class='line'>
</span><span class='line'>int __sk_mem_schedule(struct sock *sk, int size, int kind)
</span><span class='line'>{
</span><span class='line'>    struct proto *prot = sk-&gt;sk_prot;
</span><span class='line'>    // 首先得到size占用几个内存页。
</span><span class='line'>    int amt = sk_mem_pages(size);
</span><span class='line'>    int allocated;
</span><span class='line'>    // 更新sk_forward_alloc，可以看到这个值是页的大小的倍数。
</span><span class='line'>    sk-&gt;sk_forward_alloc += amt * SK_MEM_QUANTUM;
</span><span class='line'>
</span><span class='line'>    // amt+memory_allocated也就是当前的总得内存使用量加上将要分配的内存的话，现在的tcp协议栈的总得内存使用量。（可以看到是以页为单位的。
</span><span class='line'>    allocated = atomic_add_return(amt, prot-&gt;memory_allocated);
</span><span class='line'>
</span><span class='line'>    // 然后开始判断，将会落入哪一个区域。通过上面的分析我们知道sysctl_mem也就是sysctl_tcp_mem.
</span><span class='line'>
</span><span class='line'>    // 先判断是否小于等于内存最小使用限额。
</span><span class='line'>    if (allocated &lt;= prot-&gt;sysctl_mem[0]) {
</span><span class='line'>        // 这里取消memory_pressure，然后返回。
</span><span class='line'>        if (prot-&gt;memory_pressure &amp;&amp; *prot-&gt;memory_pressure)
</span><span class='line'>            *prot-&gt;memory_pressure = 0;
</span><span class='line'>        return 1;
</span><span class='line'>    }
</span><span class='line'>
</span><span class='line'>    // 然后判断Under pressure。
</span><span class='line'>    if (allocated &gt; prot-&gt;sysctl_mem[1])
</span><span class='line'>        // 大于sysctl_mem[1]说明，已经进入pressure，一次你需要调用tcp_enter_memory_pressure来设置标志。
</span><span class='line'>        if (prot-&gt;enter_memory_pressure)
</span><span class='line'>            prot-&gt;enter_memory_pressure(sk);
</span><span class='line'>
</span><span class='line'>    // 如果超过的hard limit。则进入另外的处理。
</span><span class='line'>    if (allocated &gt; prot-&gt;sysctl_mem[2])
</span><span class='line'>        goto suppress_allocation;
</span><span class='line'>
</span><span class='line'>    // 判断类型，这里只有两种类型，读和写。总的内存大小判断完，这里开始判断单独的sock的读写内存。
</span><span class='line'>    if (kind == SK_MEM_RECV) {
</span><span class='line'>        if (atomic_read(&amp;sk-&gt;sk_rmem_alloc) &lt; prot-&gt;sysctl_rmem[0])
</span><span class='line'>            return 1;
</span><span class='line'>    } else { /* SK_MEM_SEND */
</span><span class='line'>        // 这里当为tcp的时候，写队列的大小只有当对端数据确认后才会更新，因此我们要用sk_wmem_queued来判断。
</span><span class='line'>        if (sk-&gt;sk_type == SOCK_STREAM) {
</span><span class='line'>            if (sk-&gt;sk_wmem_queued &lt; prot-&gt;sysctl_wmem[0])
</span><span class='line'>                return 1;
</span><span class='line'>        } else if (atomic_read(&amp;sk-&gt;sk_wmem_alloc) &lt;
</span><span class='line'>               prot-&gt;sysctl_wmem[0])
</span><span class='line'>                return 1;
</span><span class='line'>    }
</span><span class='line'>
</span><span class='line'>    // 程序到达这里说明总的内存大小在sysctl_mem[0]和sysctl_mem[2]之间，因此我们再次判断memory_pressure
</span><span class='line'>    if (prot-&gt;memory_pressure) {
</span><span class='line'>        int alloc;
</span><span class='line'>
</span><span class='line'>        // 如果没有在memory_pressure区域，则我们直接返回1。
</span><span class='line'>        if (!*prot-&gt;memory_pressure)
</span><span class='line'>            return 1;
</span><span class='line'>        // 这个其实也就是计算整个系统分配的socket的多少。
</span><span class='line'>        alloc = percpu_counter_read_positive(prot-&gt;sockets_allocated);
</span><span class='line'>        // 这里假设其余的每个sock所占用的buf都和当前的sock一样大的时候，如果他们的总和小于sysctl_mem[2],也就是hard limit。那么我们也认为这次内存请求是成功的。
</span><span class='line'>        if (prot-&gt;sysctl_mem[2] &gt; alloc *
</span><span class='line'>            sk_mem_pages(sk-&gt;sk_wmem_queued +
</span><span class='line'>             atomic_read(&amp;sk-&gt;sk_rmem_alloc) +
</span><span class='line'>                 sk-&gt;sk_forward_alloc))
</span><span class='line'>            return 1;
</span><span class='line'>    }
</span><span class='line'>
</span><span class='line'>suppress_allocation:
</span><span class='line'>
</span><span class='line'>    // 到达这里说明，我们超过了hard limit或者说处于presure 区域。
</span><span class='line'>    if (kind == SK_MEM_SEND &amp;&amp; sk-&gt;sk_type == SOCK_STREAM) {
</span><span class='line'>        // 调整sk_sndbuf(减小).这个函数前面已经分析过了。
</span><span class='line'>        sk_stream_moderate_sndbuf(sk);
</span><span class='line'>        // 然后比较和sk_sndbuf的大小，如果大于的话，就说明下次我们再次要分配buf的时候会在tcp_memory_free阻塞住，因此这次我们返回1.
</span><span class='line'>        if (sk-&gt;sk_wmem_queued + size &gt;= sk-&gt;sk_sndbuf)
</span><span class='line'>            return 1;
</span><span class='line'>    }
</span><span class='line'>
</span><span class='line'>    /* Alas. Undo changes. */
</span><span class='line'>    // 到达这里说明，请求内存是不被接受的，因此undo所有的操作。然后返回0.
</span><span class='line'>    sk-&gt;sk_forward_alloc -= amt * SK_MEM_QUANTUM;
</span><span class='line'>    atomic_sub(amt, prot-&gt;memory_allocated);
</span><span class='line'>    return 0;
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;接下来来看个很重要的函数skb_set_owner_w。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;顾名思义，这个函数也就是将一个skb和scok关联起来。只不过关联的时候更新sock相应的域。我们来看源码：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    static inline void skb_set_owner_w(struct sk_buff *skb, struct sock *sk)
</span><span class='line'>{
</span><span class='line'>    skb_orphan(skb);
</span><span class='line'>    // 与传递进来的sock关联起来
</span><span class='line'>    skb-&gt;sk = sk;
</span><span class='line'>    // 设置skb的析构函数
</span><span class='line'>    skb-&gt;destructor = sock_wfree;
</span><span class='line'>    // 更新sk_wmem_alloc域，就是sk_wmem_alloc+truesize.
</span><span class='line'>    atomic_add(skb-&gt;truesize, &amp;sk-&gt;sk_wmem_alloc);
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;ok，接下来来看个scok_wfree函数，这个函数做得基本和上面函数相反。这个函数都是被kfree_skb自动调用的。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    void sock_wfree(struct sk_buff *skb)
</span><span class='line'>{
</span><span class='line'>    struct sock *sk = skb-&gt;sk;
</span><span class='line'>    int res;
</span><span class='line'>
</span><span class='line'>    // 更新sk_wmem_alloc,减去skb的大小。
</span><span class='line'>    res = atomic_sub_return(skb-&gt;truesize, &amp;sk-&gt;sk_wmem_alloc);
</span><span class='line'>    if (!sock_flag(sk, SOCK_USE_WRITE_QUEUE))
</span><span class='line'>    // 唤醒等待队列，也就是唤醒等待内存分配。
</span><span class='line'>        sk-&gt;sk_write_space(sk);
</span><span class='line'>    if (res == 0)
</span><span class='line'>        __sk_free(sk);
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;而skb_set_owner_w是什么时候被调用呢，我们通过搜索代码可以看到，它是在tcp_transmit_skb中被调用的。而tcp_transmit_skb我们知道是传递数据包到ip层的函数。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;而kfree_skb被调用也就是在对端已经确认完我们发送的包后才会被调用来释放skb。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h4&gt;tcp_rcv_established&lt;/h4&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;接下来来看接收数据的内存管理。我们主要来看tcp_rcv_established这个函数，我前面的blog已经断断续续的分析过了，因此这里我们只看一些重要的代码片断。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;这里我们要知道，代码能到达下面的位置，则说明，数据并没有直接拷贝到用户空间。否则的话，是不会进入下面的片断的。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    if (!eaten) {
</span><span class='line'>    ..........................................
</span><span class='line'>
</span><span class='line'>    // 如果skb的大小大于预分配的值,如果大于则要另外处理。
</span><span class='line'>    if ((int)skb-&gt;truesize &gt; sk-&gt;sk_forward_alloc)
</span><span class='line'>            goto step5;
</span><span class='line'>    __skb_pull(skb, tcp_header_len);
</span><span class='line'>    __skb_queue_tail(&amp;sk-&gt;sk_receive_queue, skb);
</span><span class='line'>    // 这里关联skb和对应的sk，并且更新相关的域，我们下面会分析这个函数。
</span><span class='line'>    skb_set_owner_r(skb, sk);
</span><span class='line'>    tp-&gt;rcv_nxt = TCP_SKB_CB(skb)-&gt;end_seq;
</span><span class='line'>}
</span><span class='line'>...............................................
</span><span class='line'>
</span><span class='line'>step5:
</span><span class='line'>    if (th-&gt;ack &amp;&amp; tcp_ack(sk, skb, FLAG_SLOWPATH) &lt; 0)
</span><span class='line'>        goto discard;
</span><span class='line'>
</span><span class='line'>    tcp_rcv_rtt_measure_ts(sk, skb);
</span><span class='line'>
</span><span class='line'>    /* Process urgent data. */
</span><span class='line'>    tcp_urg(sk, skb, th);
</span><span class='line'>
</span><span class='line'>    /* step 7: process the segment text */
</span><span class='line'>    // 最核心的函数就是这个。我们接下来会详细分析这个函数。
</span><span class='line'>    tcp_data_queue(sk, skb);
</span><span class='line'>
</span><span class='line'>    tcp_data_snd_check(sk);
</span><span class='line'>    tcp_ack_snd_check(sk);
</span><span class='line'>    return 0;
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;先来看skb_set_owner_r函数，这个函数关联skb和sk其实它和skb_set_owner_w类似：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    static inline void skb_set_owner_r(struct sk_buff *skb, struct sock *sk)
</span><span class='line'>{
</span><span class='line'>    skb_orphan(skb);
</span><span class='line'>    // 关联sk
</span><span class='line'>    skb-&gt;sk = sk;
</span><span class='line'>    // 设置析构函数
</span><span class='line'>    skb-&gt;destructor = sock_rfree;
</span><span class='line'>    // 更新rmem_alloc
</span><span class='line'>    atomic_add(skb-&gt;truesize, &amp;sk-&gt;sk_rmem_alloc);
</span><span class='line'>    // 改变forward_alloc.
</span><span class='line'>    sk_mem_charge(sk, skb-&gt;truesize);
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;h4&gt;tcp_data_queue&lt;/h4&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;然后是tcp_data_queue，这个函数主要用来排队接收数据，并update相关的读buf。由于这个函数比较复杂，我们只关心我们感兴趣的部分：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    static void tcp_data_queue(struct sock *sk, struct sk_buff *skb)
</span><span class='line'>{
</span><span class='line'>    struct tcphdr *th = tcp_hdr(skb);
</span><span class='line'>    struct tcp_sock *tp = tcp_sk(sk);
</span><span class='line'>    int eaten = -1;
</span><span class='line'>    .......................................
</span><span class='line'>    // 首先判断skb的开始序列号和我们想要接收的序列号。如果相等开始处理这个数据包(也就是拷贝到用户空间).
</span><span class='line'>    if (TCP_SKB_CB(skb)-&gt;seq == tp-&gt;rcv_nxt) {
</span><span class='line'>        if (tcp_receive_window(tp) == 0)
</span><span class='line'>            goto out_of_window;
</span><span class='line'>
</span><span class='line'>        // tp的ucopy我前面的blog已经详细分析过了。这里就不解释了。
</span><span class='line'>        if (tp-&gt;ucopy.task == current &amp;&amp;
</span><span class='line'>            tp-&gt;copied_seq == tp-&gt;rcv_nxt &amp;&amp; tp-&gt;ucopy.len &amp;&amp;sock_owned_by_user(sk) &amp;&amp; !tp-&gt;urg_data)
</span><span class='line'>        {
</span><span class='line'>            // 计算将要拷贝给用户空间的大小。
</span><span class='line'>            int chunk = min_t(unsigned int, skb-&gt;len,tp-&gt;ucopy.len);
</span><span class='line'>
</span><span class='line'>            // 设置状态，说明我们处于进程上下文。
</span><span class='line'>            __set_current_state(TASK_RUNNING);
</span><span class='line'>
</span><span class='line'>            local_bh_enable();
</span><span class='line'>            // 拷贝skb
</span><span class='line'>            if (!skb_copy_datagram_iovec(skb, 0, tp-&gt;ucopy.iov, chunk)) {
</span><span class='line'>                tp-&gt;ucopy.len -= chunk;
</span><span class='line'>                tp-&gt;copied_seq += chunk;
</span><span class='line'>                // 更新eaten，它的默认值为-1.
</span><span class='line'>                eaten = (chunk == skb-&gt;len &amp;&amp; !th-&gt;fin);
</span><span class='line'>                tcp_rcv_space_adjust(sk);
</span><span class='line'>            }
</span><span class='line'>            local_bh_disable();
</span><span class='line'>        }
</span><span class='line'>
</span><span class='line'>        // 如果小于0则说明没有拷贝成功，或者说就没有进行拷贝。此时需要更新sock的相关域。
</span><span class='line'>        if (eaten &lt;= 0) {
</span><span class='line'>queue_and_out:
</span><span class='line'>            // 最关键的tcp_try_rmem_schedule函数。接下来会详细分析。
</span><span class='line'>            if (eaten &lt; 0 &amp;&amp;
</span><span class='line'>                    tcp_try_rmem_schedule(sk, skb-&gt;truesize))
</span><span class='line'>                goto drop;
</span><span class='line'>
</span><span class='line'>            // 关联skb和sk。到达这里说明tcp_try_rmem_schedule成功，也就是返回0.
</span><span class='line'>            skb_set_owner_r(skb, sk);
</span><span class='line'>            // 加skb到receive_queue.
</span><span class='line'>            __skb_queue_tail(&amp;sk-&gt;sk_receive_queue, skb);
</span><span class='line'>        }
</span><span class='line'>        // 更新期待序列号。
</span><span class='line'>        tp-&gt;rcv_nxt = TCP_SKB_CB(skb)-&gt;end_seq;
</span><span class='line'>        ..............................................
</span><span class='line'>
</span><span class='line'>        .....................................
</span><span class='line'>
</span><span class='line'>        tcp_fast_path_check(sk);
</span><span class='line'>
</span><span class='line'>        if (eaten &gt; 0)
</span><span class='line'>            __kfree_skb(skb);
</span><span class='line'>        else if (!sock_flag(sk, SOCK_DEAD))
</span><span class='line'>            sk-&gt;sk_data_ready(sk, 0);
</span><span class='line'>        return;
</span><span class='line'>    }
</span><span class='line'>    // 下面就是处理乱序包。以后会详细分析。
</span><span class='line'>    ......................................
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;h4&gt;tcp_try_rmem_schedule&lt;/h4&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;接下来我们就来看tcp_try_rmem_schedule这个函数,这个函数如果返回0则说明sk_rmem_schedule返回1,而sk_rmem_schedule和sk_wmem_schedule是一样的。也就是看当前的skb加入后有没有超过读buf的限制。并更新相关的域。：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    static inline int tcp_try_rmem_schedule(struct sock *sk, unsigned int size)
</span><span class='line'>{
</span><span class='line'>    // 首先判断rmem_alloc(当前的读buf字节数)是否大于最大buf字节数，如果大于则调用tcp_prune_queue调整分配的buf。否则调用sk_rmem_schedule来调整相关域（sk_forward_alloc）。
</span><span class='line'>    if (atomic_read(&amp;sk-&gt;sk_rmem_alloc) &gt; sk-&gt;sk_rcvbuf ||!sk_rmem_schedule(sk, size)) {
</span><span class='line'>
</span><span class='line'>        // 调整分配的buf。
</span><span class='line'>        if (tcp_prune_queue(sk) &lt; 0)
</span><span class='line'>            return -1;
</span><span class='line'>        // 更新sk的相关域。
</span><span class='line'>        if (!sk_rmem_schedule(sk, size)) {
</span><span class='line'>            if (!tcp_prune_ofo_queue(sk))
</span><span class='line'>                return -1;
</span><span class='line'>
</span><span class='line'>            if (!sk_rmem_schedule(sk, size))
</span><span class='line'>                return -1;
</span><span class='line'>        }
</span><span class='line'>    }
</span><span class='line'>    return 0;
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;来看sk_rmem_schedule，这个函数很简单，就是封装了&lt;code&gt;__sk_mem_schedule&lt;/code&gt;。而这个函数我们上面已经分析过了。
</span><span class='line'>&lt;code&gt;
</span><span class='line'>static inline int sk_rmem_schedule(struct sock *sk, int size)
</span><span class='line'>{
</span><span class='line'>    if (!sk_has_account(sk))
</span><span class='line'>        return 1;
</span><span class='line'>    return size &lt;= sk-&gt;sk_forward_alloc ||
</span><span class='line'>        __sk_mem_schedule(sk, size, SK_MEM_RECV);
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h4&gt;tcp_prune_queue&lt;/h4&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;最后是tcp_prune_queue，这个函数主要是用来丢掉一些skb，因为到这个函数就说明我们的内存使用已经到极限了，因此我们要合并一些buf。这个合并也就是将序列号连续的段进行合并。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;这里我们要知道tcp的包是有序的，因此内核中tcp专门有一个队列来保存那些Out of order segments。因此我们这里会先处理这个队列里面的skb。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;然后调用tcp_collapse来处理接收队列里面的skb。和上面的类似。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;这里要注意，合并的话都是按页来合并，也就是先分配一页大小的内存，然后将老的skb复制进去，最后free掉老的buf。</span></code></pre></td></tr></table></div></figure>
    static int tcp_prune_queue(struct sock <em>sk)
    {
        struct tcp_sock </em>tp = tcp_sk(sk);
        &hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;.
        // 如果rmem_alloc过于大，则重新计算窗口的大小。一半都会缩小窗口。
        if (atomic_read(&amp;sk->sk_rmem_alloc) >= sk->sk_rcvbuf)
            tcp_clamp_window(sk);
        // 如果处于pressure区域，则调整窗口大小。这里也是缩小窗口。
        else if (tcp_memory_pressure)
            tp->rcv_ssthresh = min(tp->rcv_ssthresh, 4U * tp->advmss);</p>

<pre><code>    // 处理ofo队列。
    tcp_collapse_ofo_queue(sk);
    // 如果接收队列为非空，则调用tcp_collapse来处理sk_receive_queue
    if (!skb_queue_empty(&amp;sk-&gt;sk_receive_queue))
        tcp_collapse(sk, &amp;sk-&gt;sk_receive_queue,
                 skb_peek(&amp;sk-&gt;sk_receive_queue),
                 NULL,
                 tp-&gt;copied_seq, tp-&gt;rcv_nxt);
    // 更新全局的已分配内存的大小，也就是memory_allocated，接下来会详细介绍这个函数。
    sk_mem_reclaim(sk);

    // 如果调整后小于sk_rcvbuf,则返回0.
    if (atomic_read(&amp;sk-&gt;sk_rmem_alloc) &lt;= sk-&gt;sk_rcvbuf)
        return 0;

    ......................................
    return -1;
}
</code></pre>

<pre><code>
#### tcp_collapse_ofo_queue 尝试减小ofo queue占内存的大小
</code></pre>

<pre><code>/* Collapse ofo queue. Algorithm: select contiguous sequence of skbs
 * and tcp_collapse() them until all the queue is collapsed.
 */
static void tcp_collapse_ofo_queue(struct sock *sk)
{
    struct tcp_sock *tp = tcp_sk(sk);
    struct sk_buff *skb = skb_peek(&amp;tp-&gt;out_of_order_queue);
    struct sk_buff *head;
    u32 start, end;

    if (skb == NULL)
        return;

    start = TCP_SKB_CB(skb)-&gt;seq;
    end = TCP_SKB_CB(skb)-&gt;end_seq;
    head = skb;

    for (;;) {
        struct sk_buff *next = NULL;

        if (!skb_queue_is_last(&amp;tp-&gt;out_of_order_queue, skb))
            next = skb_queue_next(&amp;tp-&gt;out_of_order_queue, skb);
        skb = next;

        /* Segment is terminated when we see gap or when
         * we are at the end of all the queue. */
        if (!skb ||
            after(TCP_SKB_CB(skb)-&gt;seq, end) ||
            before(TCP_SKB_CB(skb)-&gt;end_seq, start)) {  // 找到ofo queue中连续的一段skb，即 prev-&gt;end_seq &gt;= next-&gt;seq
            tcp_collapse(sk, &amp;tp-&gt;out_of_order_queue,
                     head, skb, start, end);            // 尝试减小这一段连续skb占用的内存
            head = skb;
            if (!skb)
                break;
            /* Start new segment */
            start = TCP_SKB_CB(skb)-&gt;seq;               // 下个skb就是新的一段的开始
            end = TCP_SKB_CB(skb)-&gt;end_seq;
        } else {
            if (before(TCP_SKB_CB(skb)-&gt;seq, start))    // 这种情况只可能是tcp_collapse中大包拆成小包，拆到一半内存不够，没拆完导致。
                start = TCP_SKB_CB(skb)-&gt;seq;
            if (after(TCP_SKB_CB(skb)-&gt;end_seq, end))
                end = TCP_SKB_CB(skb)-&gt;end_seq;
        }
    }
}
</code></pre>

<pre><code>
#### tcp_collapse，gro上来的包有可能是大于4k的包，所以这个函数有时是在拆包，利弊难定
</code></pre>

<pre><code>// 删除一个skb，返回下个skb
static struct sk_buff *tcp_collapse_one(struct sock *sk, struct sk_buff *skb,
                    struct sk_buff_head *list)
{
    struct sk_buff *next = NULL;

    if (!skb_queue_is_last(list, skb))
        next = skb_queue_next(list, skb);

    __skb_unlink(skb, list);
    __kfree_skb(skb);
    NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPRCVCOLLAPSED);

    return next;
}

/* Collapse contiguous sequence of skbs head..tail with
 * sequence numbers start..end.
 *
 * If tail is NULL, this means until the end of the list.
 *
 * Segments with FIN/SYN are not collapsed (only because this
 * simplifies code)
 */
static void
tcp_collapse(struct sock *sk, struct sk_buff_head *list,
         struct sk_buff *head, struct sk_buff *tail,
         u32 start, u32 end)
{
    struct sk_buff *skb, *n;
    bool end_of_skbs;

    /* First, check that queue is collapsible and find
     * the point where collapsing can be useful. */
    skb = head;
restart:
    end_of_skbs = true;
    skb_queue_walk_from_safe(list, skb, n) {
        if (skb == tail)
            break;
        /* No new bits? It is possible on ofo queue. */
        if (!before(start, TCP_SKB_CB(skb)-&gt;end_seq)) { // 这种情况现在是不会出现的，以前代码有可能出现？？
            skb = tcp_collapse_one(sk, skb, list);
            if (!skb)
                break;
            goto restart;
        }

        /* The first skb to collapse is:
         * - not SYN/FIN and
         * - bloated or contains data before "start" or
         *   overlaps to the next one.
         */
        if (!tcp_hdr(skb)-&gt;syn &amp;&amp; !tcp_hdr(skb)-&gt;fin &amp;&amp;         // SYN，FIN 不合并，简化操作
            (tcp_win_from_space(skb-&gt;truesize) &gt; skb-&gt;len ||    // 合并后可能减小空间的情况才合并
             before(TCP_SKB_CB(skb)-&gt;seq, start))) {            // seq到start的数据已经被读走了，有减小空间的可能
            end_of_skbs = false;
            break;
        }

        if (!skb_queue_is_last(list, skb)) {
            struct sk_buff *next = skb_queue_next(list, skb);
            if (next != tail &amp;&amp;
                TCP_SKB_CB(skb)-&gt;end_seq != TCP_SKB_CB(next)-&gt;seq) { // 两个skb之间有交集，有减小空间可能
                end_of_skbs = false;
                break;
            }
        }

        /* Decided to skip this, advance start seq. */
        start = TCP_SKB_CB(skb)-&gt;end_seq;     // 否则向后继续找可能减小空间的第一个skb
    }
    if (end_of_skbs || tcp_hdr(skb)-&gt;syn || tcp_hdr(skb)-&gt;fin)
        return;

    while (before(start, end)) {  // 落在在start到end的包就是这次要合并的
        struct sk_buff *nskb;
        unsigned int header = skb_headroom(skb); // skb中协议头的大小
        int copy = SKB_MAX_ORDER(header, 0);     // 一个页（4k）中出去协议头空间的大小，也就是能容下的数据大小

        /* Too big header? This can happen with IPv6. */
        if (copy &lt; 0)
            return;
        if (end - start &lt; copy)
            copy = end - start;
        nskb = alloc_skb(copy + header, GFP_ATOMIC);
        if (!nskb)
            return;

        skb_set_mac_header(nskb, skb_mac_header(skb) - skb-&gt;head);
        skb_set_network_header(nskb, (skb_network_header(skb) -
                          skb-&gt;head));
        skb_set_transport_header(nskb, (skb_transport_header(skb) -
                        skb-&gt;head));
        skb_reserve(nskb, header);
        memcpy(nskb-&gt;head, skb-&gt;head, header);
        memcpy(nskb-&gt;cb, skb-&gt;cb, sizeof(skb-&gt;cb));
        TCP_SKB_CB(nskb)-&gt;seq = TCP_SKB_CB(nskb)-&gt;end_seq = start;
        __skb_queue_before(list, skb, nskb);
        skb_set_owner_r(nskb, sk);

        /* Copy data, releasing collapsed skbs. */
        while (copy &gt; 0) {    // 如果copy = 0，这里就会出BUG，但如果没有认为改，是不会的。ipv6会吗？？？。后面版本改进这函数了，也不会出现copy=0了
            int offset = start - TCP_SKB_CB(skb)-&gt;seq;
            int size = TCP_SKB_CB(skb)-&gt;end_seq - start;

            BUG_ON(offset &lt; 0);
            if (size &gt; 0) { // copy旧的skb数据到新的skb上
                size = min(copy, size);
                if (skb_copy_bits(skb, offset, skb_put(nskb, size), size))
                    BUG();
                TCP_SKB_CB(nskb)-&gt;end_seq += size;
                copy -= size;
                start += size;
            }
            if (!before(start, TCP_SKB_CB(skb)-&gt;end_seq)) { // 旧的skb被copy完了就删掉
                skb = tcp_collapse_one(sk, skb, list);
                if (!skb ||
                    skb == tail ||
                    tcp_hdr(skb)-&gt;syn ||
                    tcp_hdr(skb)-&gt;fin)
                    return;
            }
        }
    }
}
</code></pre>

<pre><code>
来看sk_mem_reclaim函数，它只是简单的封装了`__sk_mem_reclaim`：
</code></pre>

<pre><code>static inline void sk_mem_reclaim(struct sock *sk)
{
    if (!sk_has_account(sk))
        return;
    // 如果sk_forward_alloc大于1页则调用__sk_mem_reclaim，我们知道sk_forward_alloc是以页为单位的，因此这里也就是和大于0一样。
    if (sk-&gt;sk_forward_alloc &gt;= SK_MEM_QUANTUM)
        __sk_mem_reclaim(sk);
}
</code></pre>

<pre><code>
`__sk_mem_reclaim`就是真正操作的函数，它会更新memory_allocated：
</code></pre>

<pre><code>void __sk_mem_reclaim(struct sock *sk)
{
    struct proto *prot = sk-&gt;sk_prot;
    // 更新memory_allocated，这里我们知道memory_allocated也是以页为单位的，因此需要将sk_forward_alloc转化为页。
    atomic_sub(sk-&gt;sk_forward_alloc &gt;&gt; SK_MEM_QUANTUM_SHIFT,prot-&gt;memory_allocated);

    // 更新这个sk的sk_forward_alloc为一页。
    sk-&gt;sk_forward_alloc &amp;= SK_MEM_QUANTUM - 1;
    // 判断是否处于pressure区域，是的话更新memory_pressure变量。
    if (prot-&gt;memory_pressure &amp;&amp; *prot-&gt;memory_pressure &amp;&amp;(atomic_read(prot-&gt;memory_allocated) &lt; （prot-&gt;sysctl_mem[0]))
        *prot-&gt;memory_pressure = 0;
}
</code></pre>

<pre><code>

最后看一下读buf的释放。这个函数会在kfree_skb中被调用。
</code></pre>

<pre><code>void sock_rfree(struct sk_buff *skb)
{

    struct sock *sk = skb-&gt;sk;
    // 更新rmem_alloc
    atomic_sub(skb-&gt;truesize, &amp;sk-&gt;sk_rmem_alloc);
    // 更新forward_alloc.
    sk_mem_uncharge(skb-&gt;sk, skb-&gt;truesize);
}
</code></pre>

<p>```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TCP三次握手源码详解]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/06/01/kernel-net-shark-hand/"/>
    <updated>2015-06-01T14:24:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/06/01/kernel-net-shark-hand</id>
    <content type="html"><![CDATA[<p><a href="http://blog.csdn.net/qy532846454/article/details/7882819">http://blog.csdn.net/qy532846454/article/details/7882819</a></p>

<p><a href="http://m.bianceng.cn/OS/Linux/201301/35179_6.htm">http://m.bianceng.cn/OS/Linux/201301/35179_6.htm</a></p>

<p>内核：2.6.34</p>

<p>TCP是应用最广泛的传输层协议，其提供了面向连接的、可靠的字节流服务，但也正是因为这些特性，使得TCP较之UDP异常复杂，还是分两部分[创建与使用]来进行分析。这篇主要包括TCP的创建及三次握手的过程。</p>

<p>编程时一般用如下语句创建TCP Socket：</p>

<pre><code>    socket(AF_INET, SOCK_DGRAM, IPPROTO_TCP)  
</code></pre>

<p>由此开始分析，调用接口[net/socket.c]: SYSCALL_DEFINE3(socket)</p>

<p>其中执行两步关键操作：sock_create()与sock_map_fd()</p>

<pre><code>    retval = sock_create(family, type, protocol, &amp;sock);  
    if (retval &lt; 0)  
        goto out;  
    retval = sock_map_fd(sock, flags &amp; (O_CLOEXEC | O_NONBLOCK));  
    if (retval &lt; 0)  
        goto out_release;  
</code></pre>

<p>  sock_create()用于创建socket，sock_map_fd()将之映射到文件描述符，使socket能通过fd进行访问，着重分析sock_create()的创建过程。</p>

<pre><code>    sock_create() -&gt; __sock_create()
</code></pre>

<p>  从__sock_create()代码看到创建包含两步：sock_alloc()和pf->create()。sock_alloc()分配了sock内存空间并初始化inode；pf->create()初始化了sk。</p>

<pre><code>    sock = sock_alloc();  
    sock-&gt;type = type;  
    ……  
    pf = rcu_dereference(net_families[family]);  
    ……  
    pf-&gt;create(net, sock, protocol, kern);  
</code></pre>

<h4>sock_alloc()</h4>

<p>  分配空间，通过new_inode()分配了节点(包括socket)，然后通过SOCKET_I宏获得sock，实际上inode和sock是在new_inode()中一起分配的，结构体叫作sock_alloc。</p>

<pre><code>    inode = new_inode(sock_mnt-&gt;mnt_sb);  
    sock = SOCKET_I(inode);  
</code></pre>

<p>  设置inode的参数，并返回sock。</p>

<pre><code>    inode-&gt;i_mode = S_IFSOCK | S_IRWXUGO;  
    inode-&gt;i_uid = current_fsuid();  
    inode-&gt;i_gid = current_fsgid();  
    return sock;  
</code></pre>

<p>  继续往下看具体的创建过程：new_inode()，在分配后，会设置i_ino和i_state的值。</p>

<pre><code>    struct inode *new_inode(struct super_block *sb)  
    {  
        ……  
        inode = alloc_inode(sb);  
        if (inode) {  
            spin_lock(&amp;inode_lock);  
            __inode_add_to_lists(sb, NULL, inode);  
            inode-&gt;i_ino = ++last_ino;  
            inode-&gt;i_state = 0;  
            spin_unlock(&amp;inode_lock);  
        }  
        return inode;  
    }  
</code></pre>

<p>  其中的alloc_inode() -> sb->s_op->alloc_inode()，sb是sock_mnt->mnt_sb，所以alloc_inode()指向的是sockfs的操作函数sock_alloc_inode。</p>

<pre><code>    static const struct super_operations sockfs_ops = {  
        .alloc_inode = sock_alloc_inode,  
        .destroy_inode =sock_destroy_inode,  
        .statfs = simple_statfs,  
    };  
</code></pre>

<p>  sock_alloc_inode()中通过kmem_cache_alloc()分配了struct socket_alloc结构体大小的空间，而struct socket_alloc结构体定义如下，但只返回了inode，实际上socket和inode都已经分配了空间，在之后就可以通过container_of取到socket。</p>

<pre><code>    static struct inode *sock_alloc_inode(struct super_block *sb)  
    {  
        struct socket_alloc *ei;  
        ei = kmem_cache_alloc(sock_inode_cachep, GFP_KERNEL);  
        ......  
        return &amp;ei-&gt;vfs_inode;  
    }  
    struct socket_alloc {  
        struct socket socket;  
        struct inode vfs_inode;  
    };  

    net_families[AF_INET]:  
    static const struct net_proto_family inet_family_ops = {  
        .family = PF_INET,  
        .create = inet_create,  
        .owner = THIS_MODULE,  
    };  
</code></pre>

<p>err = pf->create(net, sock, protocol, kern); ==> inet_create()
这段代码就是从inetsw[]中取到适合的协议类型answer，sock->type就是传入socket()函数的type参数SOCK_DGRAM，最终取得结果answer->ops==inet_stream_ops，从上面这段代码还可以看出以下问题：</p>

<p>  socket(AF_INET, SOCK_RAW, IPPROTO_IP)这样是不合法的，因为SOCK_RAW没有默认的协议类型；同样socket(AF_INET, SOCK_DGRAM, IPPROTO_IP)与socket(AF_INET, SOCK_DGRAM, IPPROTO_TCP)是一样的，因为TCP的默认协议类型是IPPTOTO_TCP；SOCK_STREAM与IPPROTO_UDP同上。</p>

<pre><code>    sock-&gt;state = SS_UNCONNECTED;  
    list_for_each_entry_rcu(answer, &amp;inetsw[sock-&gt;type], list) {  
        err = 0;  
        /* Check the non-wild match. */  
        if (protocol == answer-&gt;protocol) {  
            if (protocol != IPPROTO_IP)  
                break;  
        } else {  
            /* Check for the two wild cases. */  
            if (IPPROTO_IP == protocol) {  
                protocol = answer-&gt;protocol;  
                break;  
            }  
            if (IPPROTO_IP == answer-&gt;protocol)  
                break;  
        }  
        err = -EPROTONOSUPPORT;  
    }  
</code></pre>

<p>sock->ops指向inet_stream_ops，然后创建sk，sk->proto指向tcp_prot，注意这里分配的大小是struct tcp_sock，而不仅仅是struct sock大小</p>

<pre><code>    sock-&gt;ops = answer-&gt;ops;  
    answer_prot = answer-&gt;prot;  
    ……  
    sk = sk_alloc(net, PF_INET, GFP_KERNEL, answer_prot);  
</code></pre>

<p>然后设置inet的一些参数，这里直接将sk类型转换为inet，因为在sk_alloc()中分配的是struct tcp_sock结构大小，返回的是struct sock，利用了第一个成员的特性，三者之间的关系如下图：
<code>
    inet = inet_sk(sk);  
    ……  
    inet-&gt;inet_id = 0;  
    sock_init_data(sock, sk);  
</code>
其中有些设置是比较重要的，如
<code>
    sk-&gt;sk_state = TCP_CLOSE;  
    sk_set_socket(sk, sock);  
    sk-&gt;sk_protocol = protocol;  
    sk-&gt;sk_backlog_rcv = sk-&gt;sk_prot-&gt;backlog_rcv;  
</code></p>

<p>创建socket后，接下来的流程会因为客户端或服务器的不同而有所差异，下面着重于分析建立连接的三次握手过程。典型的客户端流程：<br/>
connect() -> send() -> recv()</p>

<p>典型的服务器流程：<br/>
bind() -> listen() -> accept() -> recv() -> send()</p>

<h4>客户端流程</h4>

<p>发送SYN报文，向服务器发起tcp连接</p>

<pre><code>            connect(fd, servaddr, addrlen);
                -&gt; SYSCALL＿DEFINE3() 
                -&gt; sock-&gt;ops-&gt;connect() == inet_stream_connect (sock-&gt;ops即inet_stream_ops)
                -&gt; tcp_v4_connect()
</code></pre>

<p>查找到达[daddr, dport]的路由项，路由项的查找与更新与”路由表”章节所述一样。要注意的是由于是作为客户端调用，创建socket后调用connect，因而saddr, sport都是0，同样在未查找路由前，要走的出接口oif也是不知道的，因此也是0。在查找完路由表后(注意不是路由缓存)，可以得知出接口，但并未存储到sk中。因此插入的路由缓存是特别要注意的：它的键值与实际值是不相同的，这个不同点就在于oif与saddr，键值是[saddr=0, sport=0, daddr, dport, oif=0]，而缓存项值是[saddr, sport=0, daddr, dport, oif]。</p>

<pre><code>    tmp = ip_route_connect(&amp;rt, nexthop, inet-&gt;inet_saddr,  
                            RT_CONN_FLAGS(sk), sk-&gt;sk_bound_dev_if,  
                            IPPROTO_TCP,  
                            inet-&gt;inet_sport, usin-&gt;sin_port, sk, 1);  
    if (tmp &lt; 0) {  
        if (tmp == -ENETUNREACH)  
            IP_INC_STATS_BH(sock_net(sk), IPSTATS_MIB_OUTNOROUTES);  
        return tmp;  
    }  
</code></pre>

<p>通过查找到的路由项，对inet进行赋值，可以看到，除了sport，都赋予了值，sport的选择复杂点，因为它要随机从未使用的本地端口中选择一个。</p>

<pre><code>    if (!inet-&gt;inet_saddr)  
        inet-&gt;inet_saddr = rt_rt_src;   
    inet-&gt;inet_rcv_addr = inet-&gt;inet_saddr;  
    ……  
    inet-&gt;inet_dport = usin-&gt;sin_port;  
    inet-&gt;inet_daddr = daddr;  
</code></pre>

<p>状态从CLOSING转到TCP_SYN_SENT，也就是我们熟知的TCP的状态转移图。</p>

<pre><code>    tcp_set_state(sk, TCP_SYN_SENT);  
</code></pre>

<p>插入到bind链表中</p>

<pre><code>    err = inet_hash_connect(&amp;tcp_death_row, sk); //== &gt; __inet_hash_connect()  
</code></pre>

<p>当snum==0时，表明此时源端口没有指定，此时会随机选择一个空闲端口作为此次连接的源端口。low和high分别表示可用端口的下限和上限，remaining表示可用端口的数，注意这里的可用只是指端口可以用作源端口，其中部分端口可能已经作为其它socket的端口号在使用了，所以要循环1~remaining，直到查找到空闲的源端口。</p>

<pre><code>    if (!snum) {  
        inet_get_local_port_range(&amp;low, &amp;high);  
        remaining = (high - low) + 1;  
        ……  
        for (i = 1; i &lt;= remaining; i++) {  
            ……// choose a valid port  
        }  
    }  
</code></pre>

<p>下面来看下对每个端口的检查，即//choose a valid port部分的代码。这里要先了解下tcp的内核表组成，udp的表内核表udptable只是一张hash表，tcp的表则稍复杂，它的名字是tcp_hashinfo，在tcp_init()中被初始化，这个数据结构定义如下(省略了不相关的数据)：</p>

<pre><code>    struct inet_hashinfo {  
        struct inet_ehash_bucket *ehash;  
        ……  
        struct inet_bind_hashbucket *bhash;  
        ……  
        struct inet_listen_hashbucket  listening_hash[INET_LHTABLE_SIZE]  
                        ____cacheline_aligned_in_smp;  
    };  
</code></pre>

<p>从定义可以看出，tcp表又分成了三张表ehash, bhash, listening_hash，其中ehash, listening_hash对应于socket处在TCP的ESTABLISHED, LISTEN状态，bhash对应于socket已绑定了本地地址。三者间并不互斥，如一个socket可同时在bhash和ehash中，由于TIME_WAIT是一个比较特殊的状态，所以ehash又分成了chain和twchain，为TIME_WAIT的socket单独形成一张表。</p>

<p>回到刚才的代码，现在还只是建立socket连接，使用的就应该是tcp表中的bhash。首先取得内核tcp表的bind表 – bhash，查看是否已有socket占用：<br/>
  如果没有，则调用inet_bind_bucket_create()创建一个bind表项tb，并插入到bind表中，跳转至goto ok代码段；<br/>
  如果有，则跳转至goto ok代码段。<br/>
  进入ok代码段表明已找到合适的bind表项(无论是创建的还是查找到的)，调用inet_bind_hash()赋值源端口inet_num。</p>

<pre><code>    for (i = 1; i &lt;= remaining; i++) {  
        port = low + (i + offset) % remaining;  
        head = &amp;hinfo-&gt;bhash[inet_bhashfn(net, port, hinfo-&gt;bhash_size)];  
        ……  
        inet_bind_bucket_for_each(tb, node, &amp;head-&gt;chain) {  
            if (net_eq(ib_net(tb), net) &amp;&amp; tb-&gt;port == port) {  
                if (tb-&gt;fastreuse &gt;= 0)  
                    goto next_port;  
                WARN_ON(hlist_empty(&amp;tb-&gt;owners));  
                if (!check_established(death_row, sk, port, &amp;tw))  
                    goto ok;  
                goto next_port;  
            }  
        }  

        tb = inet_bind_bucket_create(hinfo-&gt;bind_bucket_cachep, net, head, port);  
        ……  
        next_port:  
            spin_unlock(&amp;head-&gt;lock);  
    }  

    ok:  
        ……  
    inet_bind_hash(sk, tb, port);  
        ……  
        goto out;  
</code></pre>

<p>在获取到合适的源端口号后，会重建路由项来进行更新：</p>

<pre><code>    err = ip_route_newports(&amp;rt, IPPROTO_TCP, inet-&gt;inet_sport, inet-&gt;inet_dport, sk);  
</code></pre>

<p>函数比较简单，在获取sport前已经查找过一次路由表，并插入了key=[saddr=0, sport=0, daddr, dport, oif=0]的路由缓存项；现在获取到了sport，调用ip_route_output_flow()再次更新路由缓存表，它会添加key=[saddr=0, sport, daddr, dport, oif=0]的路由缓存项。这里可以看出一个策略选择，查询路由表->获取sport->查询路由表，为什么不是获取sport->查询路由表的原因可能是效率的问题。</p>

<pre><code>    if (sport != (*rp)-&gt;fl.fl_ip_sport ||  
                    dport != (*rp)-&gt;fl.fl_ip_dport) {  
        struct flowi fl;  

        memcpy(&amp;fl, &amp;(*rp)-&gt;fl, sizeof(fl));  
        fl.fl_ip_sport = sport;  
        fl.fl_ip_dport = dport;  
        fl.proto = protocol;  
        ip_rt_put(*rp);  
        *rp = NULL;  
        security_sk_classify_flow(sk, &amp;fl);  
        return ip_route_output_flow(sock_net(sk), rp, &amp;fl, sk, 0);  
    }  
</code></pre>

<p>write_seq相当于第一次发送TCP报文的ISN，如果为0，则通过计算获取初始值，否则延用上次的值。在获取完源端口号，并查询过路由表后，TCP正式发送SYN报文，注意在这之前TCP状态已经更新成了TCP_SYN_SENT，而在函数最后才调用tcp_connect(sk)发送SYN报文，这中间是有时差的。</p>

<pre><code>    if (!tp-&gt;write_seq)  
        tp-&gt;write_seq = secure_tcp_sequence_number(inet-&gt;inet_saddr,  
                                        inet-&gt;inet_daddr,  
                                        inet-&gt;inet_sport,  
                                        usin-&gt;sin_port);  
    inet-&gt;inet_id = tp-&gt;write_seq ^ jiffies;  
    err = tcp_connect(sk);  
</code></pre>

<h5>tcp_connect()　发送SYN报文</h5>

<p>几步重要的代码如下，tcp_connect_init()中设置了tp->rcv_nxt=0，tcp_transmit_skb()负责发送报文，其中seq=tcb->seq=tp->write_seq，ack_seq=tp->rcv_nxt。</p>

<pre><code>    tcp_connect_init(sk);  
    tp-&gt;snd_nxt = tp-&gt;write_seq;  
    ……  
    tcp_transmit_skb(sk, buff, 1, sk-&gt;sk_allocation);  
</code></pre>

<h5>收到服务端的SYN+ACK，发送ACK</h5>

<h5>tcp_rcv_synsent_state_process()</h5>

<p>此时已接收到对方的ACK，状态变迁到TCP_ESTABLISHED。最后发送对方SYN的ACK报文。</p>

<pre><code>    tcp_set_state(sk, TCP_ESTABLISHED);  
    tcp_send_ack(sk);  
</code></pre>

<h4>服务端流程</h4>

<h5>bind() -> inet_bind()</h5>

<p>  bind操作的主要作用是将创建的socket与给定的地址相绑定，这样创建的服务才能公开的让外部调用。当然对于socket服务器的创建来说，这一步不是必须的，在listen()时如果没有绑定地址，系统会选择一个随机可用地址作为服务器地址。</p>

<p>  一个socket地址分为ip和port，inet->inet_saddr赋值了传入的ip，snum是传入的port，对于端口，要检查它是否已被占用，这是由sk->sk_prot->get_port()完成的(这个函数前面已经分析过，在传入port时它检查是否被占用；传入port=0时它选择未用的端口)。如果没有被占用，inet->inet_sport被赋值port，因为是服务监听端，不需要远端地址，inet_daddr和inet_dport都置0。</p>

<p>  注意bind操作不会改变socket的状态，仍为创建时的TCP_CLOSE。</p>

<pre><code>    snum = ntohs(addr-&gt;sin_port);  
    ……  
    inet-&gt;inet_rcv_saddr = inet-&gt;inet_saddr = addr-&gt;sin_addr.s_addr;  
    if (sk-&gt;sk_prot-&gt;get_port(sk, snum)) {  
        inet-&gt;inet_saddr = inet-&gt;inet_rcv_saddr = 0;  
        err = -EADDRINUSE;  
        goto out_release_sock;  
    }  
    ……  
    inet-&gt;inet_sport = htons(inet-&gt;inet_num);  
    inet-&gt;inet_daddr = 0;  
    inet-&gt;inet_dport = 0;  
</code></pre>

<h5>listen() -> inet_listen()</h5>

<p>  listen操作开始服务器的监听，此时服务就可以接受到外部连接了。在开始监听前，要检查状态是否正确，sock->state==SS_UNCONNECTED确保仍是未连接的socket，sock->type==SOCK_STREAM确保是TCP协议，old_state确保此时状态是TCP_CLOSE或TCP_LISTEN，在其它状态下进行listen都是错误的。</p>

<pre><code>    if (sock-&gt;state != SS_UNCONNECTED || sock-&gt;type != SOCK_STREAM)  
        goto out;  
    old_state = sk-&gt;sk_state;  
    if (!((1 &lt;&lt; old_state) &amp; (TCPF_CLOSE | TCPF_LISTEN)))  
        goto out;  
</code></pre>

<p>  如果已是TCP_LISTEN态，则直接跳过，不用再执行listen了，而只是重新设置listen队列长度sk_max_ack_backlog，改变listen队列长也是多次执行listen的作用。如果还没有执行listen，则还要调用inet_csk_listen_start()开始监听。</p>

<p>  inet_csk_listen_start()变迁状态至TCP_LISTEN，分配监听队列，如果之前没有调用bind()绑定地址，则这里会分配一个随机地址。</p>

<pre><code>    if (old_state != TCP_LISTEN) {  
        err = inet_csk_listen_start(sk, backlog);  
        if (err)  
            goto out;  
    }  
    sk-&gt;sk_max_ack_backlog = backlog;  
</code></pre>

<h5>accept()</h5>

<p>accept() -> sys_accept4() -> inet_accept() -> inet_csk_accept()</p>

<p>  accept()实际要做的事件并不多，它的作用是返回一个已经建立连接的socket(即经过了三次握手)，这个过程是异步的，accept()并不亲自去处理三次握手过程，而只是监听icsk_accept_queue队列，当有socket经过了三次握手，它就会被加到icsk_accept_queue中，所以accept要做的就是等待队列中插入socket，然后被唤醒并返回这个socket。而三次握手的过程完全是协议栈本身去完成的。换句话说，协议栈相当于写者，将socket写入队列，accept()相当于读者，将socket从队列读出。这个过程从listen就已开始，所以即使不调用accept()，客户仍可以和服务器建立连接，但由于没有处理，队列很快会被占满。</p>

<pre><code>    if (reqsk_queue_empty(&amp;icsk-&gt;icsk_accept_queue)) {  
        long timeo = sock_rcvtimeo(sk, flags &amp; O_NONBLOCK);  
        ……  
        error = inet_csk_wait_for_connect(sk, timeo);  
        ……  
    }  

    newsk = reqsk_queue_get_child(&amp;icsk-&gt;icsk_accept_queue, sk);  
</code></pre>

<p>  协议栈向队列中加入socket的过程就是完成三次握手的过程，客户端通过向已知的listen fd发起连接请求，对于到来的每个连接，都会创建一个新的sock，当它经历了TCP_SYN_RCV -> TCP_ESTABLISHED后，就会被添加到icsk_accept_queue中，而监听的socket状态始终为TCP_LISTEN，保证连接的建立不会影响socket的接收。</p>

<h4>接收客户端发来的SYN，发送SYN+ACK</h4>

<h5>tcp_v4_do_rcv()</h5>

<p>  tcp_v4_do_rcv()是TCP模块接收的入口函数，客户端发起请求的对象是listen fd，所以sk->sk_state == TCP_LISTEN，调用tcp_v4_hnd_req()来检查是否处于半连接，只要三次握手没有完成，这样的连接就称为半连接，具体而言就是收到了SYN，但还没有收到ACK的连接，所以对于这个查找函数，如果是SYN报文，则会返回listen的socket(连接尚未创建)；如果是ACK报文，则会返回SYN报文处理中插入的半连接socket。其中存储这些半连接的数据结构是syn_table，它在listen()调用时被创建，大小由sys_ctl_max_syn_backlog和listen()传入的队列长度决定。</p>

<p>此时是收到SYN报文，tcp_v4_hnd_req()返回的仍是sk，调用tcp_rcv_state_process()来接收SYN报文，并发送SYN+ACK报文，同时向syn_table中插入一项表明此次连接的sk。</p>

<pre><code>    if (sk-&gt;sk_state == TCP_LISTEN) {  
        struct sock *nsk = tcp_v4_hnd_req(sk, skb);  
        if (!nsk)  
            goto discard;  
        if (nsk != sk) {  
            if (tcp_child_process(sk, nsk, skb)) {  
                rsk = nsk;  
                goto reset;  
            }  
            return 0;  
        }  
    }  
    TCP_CHECK_TIMER(sk);  
    if (tcp_rcv_state_process(sk, skb, tcp_hdr(skb), skb-&gt;len)) {  
        rsk = sk;  
        goto reset;  
    }  
</code></pre>

<p>  tcp_rcv_state_process()处理各个状态上socket的情况。下面是处于TCP_LISTEN的代码段，处于TCP_LISTEN的socket不会再向其它状态变迁，它负责监听，并在连接建立时创建新的socket。实际上，当收到第一个SYN报文时，会执行这段代码，conn_request() => tcp_v4_conn_request。</p>

<pre><code>    case TCP_LISTEN:  
    ……  
        if (th-&gt;syn) {  
            if (icsk-&gt;icsk_af_ops-&gt;conn_request(sk, skb) &lt; 0)  
                return 1;  
            kfree_skb(skb);  
            return 0;  
        }  
</code></pre>

<p>  tcp_v4_conn_request()中注意两个函数就可以了：tcp_v4_send_synack()向客户端发送了SYN+ACK报文，inet_csk_reqsk_queue_hash_add()将sk添加到了syn_table中，填充了该客户端相关的信息。这样，再次收到客户端的ACK报文时，就可以在syn_table中找到相应项了。</p>

<pre><code>    if (tcp_v4_send_synack(sk, dst, req, (struct request_values *)&amp;tmp_ext) || want_cookie)  
        goto drop_and_free;  
    inet_csk_reqsk_queue_hash_add(sk, req, TCP_TIMEOUT_INIT);  
</code></pre>

<h4>接收客户端发来的ACK</h4>

<h5>tcp_v4_do_rcv()</h5>

<p>  过程与收到SYN报文相同，不同点在于syn_table中已经插入了有关该连接的条目，tcp_v4_hnd_req()会返回一个新的sock: nsk，然后会调用tcp_child_process()来进行处理。在tcp_v4_hnd_req()中会创建新的sock，下面详细看下这个函数。</p>

<pre><code>    if (sk-&gt;sk_state == TCP_LISTEN) {  
        struct sock *nsk = tcp_v4_hnd_req(sk, skb);  
        if (!nsk)  
            goto discard;  
        if (nsk != sk) {  
            if (tcp_child_process(sk, nsk, skb)) {  
                rsk = nsk;  
                goto reset;  
            }  
            return 0;  
        }  
    }  
</code></pre>

<h5>tcp_v4_hnd_req()</h5>

<p>之前已经分析过，inet_csk_search_req()会在syn_table中找到req，此时进入tcp_check_req()</p>

<pre><code>    struct request_sock *req = inet_csk_search_req(sk, &amp;prev, th-&gt;source, iph-&gt;saddr, iph-&gt;daddr);  
    if (req)  
        return tcp_check_req(sk, skb, req, prev);  
</code></pre>

<h5>tcp_check_req()</h5>

<p>  syn_recv_sock() -> tcp_v4_syn_recv_sock()会创建一个新的sock并返回，创建的sock状态被直接设置为TCP_SYN_RECV，然后因为此时socket已经建立，将它添加到icsk_accept_queue中。</p>

<p>  状态TCP_SYN_RECV的设置可能比较奇怪，按照TCP的状态转移图，在服务端收到SYN报文后变迁为TCP_SYN_RECV，但看到在实现中收到ACK后才有了状态TCP_SYN_RECV，并且马上会变为TCP_ESTABLISHED，所以这个状态变得无足轻重。这样做的原因是listen和accept返回的socket是不同的，而只有真正连接建立时才会创建这个新的socket，在收到SYN报文时新的socket还没有建立，就无从谈状态变迁了。这里同样是一个平衡的存在，你也可以在收到SYN时创建一个新的socket，代价就是无用的socket大大增加了。</p>

<pre><code>    child = inet_csk(sk)-&gt;icsk_af_ops-&gt;syn_recv_sock(sk, skb, req, NULL);  
    if (child == NULL)  
        goto listen_overflow;  
    inet_csk_reqsk_queue_unlink(sk, req, prev);  
    inet_csk_reqsk_queue_removed(sk, req);  
    inet_csk_reqsk_queue_add(sk, req, child);  
</code></pre>

<h5>tcp_child_process()</h5>

<p>如果此时sock: child被用户进程锁住了，那么就先添加到backlog中__sk_add_backlog()，待解锁时再处理backlog上的sock；如果此时没有被锁住，则先调用tcp_rcv_state_process()进行处理，处理完后，如果child状态到达TCP_ESTABLISHED，则表明其已就绪，调用sk_data_ready()唤醒等待在isck_accept_queue上的函数accept()。</p>

<pre><code>    if (!sock_owned_by_user(child)) {  
        ret = tcp_rcv_state_process(child, skb, tcp_hdr(skb), skb-&gt;len);  
        if (state == TCP_SYN_RECV &amp;&amp; child-&gt;sk_state != state)  
            parent-&gt;sk_data_ready(parent, 0);  
    } else {  
        __sk_add_backlog(child, skb);  
    }  
</code></pre>

<p>  tcp_rcv_state_process()处理各个状态上socket的情况。下面是处于TCP_SYN_RECV的代码段，注意此时传入函数的sk已经是新创建的sock了(在tcp_v4_hnd_req()中)，并且状态是TCP_SYN_RECV，而不再是listen socket，在收到ACK后，sk状态变迁为TCP_ESTABLISHED，而在tcp_v4_hnd_req()中也已将sk插入到了icsk_accept_queue上，此时它就已经完全就绪了，回到tcp_child_process()便可执行sk_data_ready()。</p>

<pre><code>    case TCP_SYN_RECV:  
        if (acceptable) {  
            ……  
            tcp_set_state(sk, TCP_ESTABLISHED);  
            sk-&gt;sk_state_change(sk);  
            ……  
            tp-&gt;snd_una = TCP_SKB_CB(skb)-&gt;ack_seq;  
            tp-&gt;snd_wnd = ntohs(th-&gt;window) &lt;&lt; tp-&gt;rx_opt.snd_wscale;  
            tcp_init_wl(tp, TCP_SKB_CB(skb)-&gt;seq);   
            ……  
    }  
</code></pre>

<p>最后总结三次握手的过程</p>

<p><img src="/images/kernel/2015-06-01.png" alt="" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ipv6初始化和处理流程分析]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/05/15/kernel-net-ipv6/"/>
    <updated>2015-05-15T15:57:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/05/15/kernel-net-ipv6</id>
    <content type="html"><![CDATA[<p><a href="/download/kernel/ipv6%E5%88%9D%E5%A7%8B%E5%8C%96%E5%92%8C%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90.pdf">ipv6初始化和处理流程分析.pdf</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TCP的URG标志和内核实现]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/05/15/kernel-net-tcp_urg/"/>
    <updated>2015-05-15T13:51:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/05/15/kernel-net-tcp_urg</id>
    <content type="html"><![CDATA[<p><a href="http://blog.csdn.net/phenix_lord/article/details/42012931">TCP的URG标志和内核实现之一：协议</a><br/>
<a href="http://blog.csdn.net/phenix_lord/article/details/42046125">TCP的URG标志和内核实现之二：发送的实现</a><br/>
<a href="http://blog.csdn.net/phenix_lord/article/details/42065897">TCP的URG标志和内核实现之三：接收的实现</a></p>

<hr />

<h3>TCP的URG标志和内核实现之一：协议</h3>

<p>定义urgent数据的目的：<br/>
urgent机制，是用于通知应用层需要接收urgent data，在urgent data接收完成后，通知应用层urgent data数据接收完毕。相关协议文本RFC793 RFC1122 RFC6093</p>

<h4>哪些数据是urgent data？</h4>

<h5>协议规定</h5>

<p>在TCP报头的URG位有效的时候，通过TCP报头中的urgent pointer来标识urgent data的位置，但是在urgent pointer的解析方式上各个协议文本的描述有差异：</p>

<p>解读一：RFC793  P17，描述是“The urgent pointer points to the sequence number of the octet following the urgent data.”，在P41有描述“This mechanism permits a point in the data stream to be designated as the end of urgent information. Whenever this point is in advance of the receive sequence number (RCV.NXT) at the receiving TCP, that TCP must tell the user to go into &ldquo;urgent mode&rdquo;; when the receive sequence number catches up to the urgent pointer, the TCP must tell user to go”，可以认为是：当前接收的报文中SEQ在SEG.SEQ+Urgent Pointer之前的都是,而urgent pointer是第一个非urgent data（ TCP已经接受，但是还没有提交给应用的数据是不是呢？）</p>

<p>解读二：在P56的描述是“If the urgent flag is set, then SND.UP &lt;-SND.NXT-1 and set the urgent pointer in the outgoing segments”，也就是urgent pointer是最后一个urgent data字节。而在RFC1122中消除了这一歧义：在P84中说明“the urgent pointer points to the sequence number of the LAST octet (not LAST+1) in a sequence of urgent data”</p>

<h5>linux实现</h5>

<p>虽然在RFC1122中消除了这一歧义，linux仍然使用了解读一的解析方式，如果要使用解读二定义的方式，需要使用tcp_stdurg这个配置项。</p>

<h4>urgent data数据能有多长？</h4>

<h5>协议规定</h5>

<p>按照RFC793 P41的描述，长度不受限，RFC1122 P84中，更是明确了“A TCP MUST support a sequence of urgent data of any length”</p>

<h5>linux实现</h5>

<p>其实，linux只支持1BYTE的urgent data</p>

<h4>urgent data与OOB数据</h4>

<p>OOB数据说的是带外数据，也就是这些数据不是放到TCP流供读取的，而是通过额外的接口来获取，linux默认把urgent data实现为OOB数据；而按照协议的规定，urgent data不是out of band data</p>

<p>由于OOB数据的协议和实现上存在很多不确定因素，因此现在已经不建议使用了</p>

<hr />

<h3>TCP的URG标志和内核实现之二：发送的实现</h3>

<p>Linxu内核在默认情况下，把urgent data实现为OOB数据</p>

<h4>发送URG数据的接口</h4>

<p>在内核态，使用kernel_sendmsg/kernel_sendpage完成发送，只不过需要加上MSG_OOB标志，表示要发送的URG数据。</p>

<h4>URG数据发送接口的实现</h4>

<p>分片主要在kernel_sendmsg中完成，在OOB数据的处理上，它和kernel_sendpage是一致
```
    int tcp_sendmsg(struct kiocb <em>iocb, struct sock </em>sk, struct msghdr <em>msg,<br/>
            size_t size)<br/>
    {<br/>
        。。。。。。。。。。。。。。<br/>
        /</em>如果flags设置了MSG_OOB该接口其实返回的mss_now关闭了TSO功能<em>/<br/>
        mss_now = tcp_send_mss(sk, &amp;size_goal, flags);<br/>
        。。。。。。。。。。。。。。<br/>
        while (&ndash;iovlen >= 0) {<br/>
            size_t seglen = iov->iov_len;<br/>
            unsigned char __user </em>from = iov->iov_base;</p>

<pre><code>        iov++;  

        while (seglen &gt; 0) {  
            int copy = 0;  
            int max = size_goal;  

            skb = tcp_write_queue_tail(sk);  
            if (tcp_send_head(sk)) {  
                if (skb-&gt;ip_summed == CHECKSUM_NONE)  
                    max = mss_now;  
                copy = max - skb-&gt;len;  
            }  

            if (copy &lt;= 0) {  
new_segment:  
                /* Allocate new segment. If the interface is SG, 
                 * allocate skb fitting to single page. 
                 */  
                if (!sk_stream_memory_free(sk))  
                    goto wait_for_sndbuf;  

                skb = sk_stream_alloc_skb(sk,  
                              select_size(sk, sg),  
                              sk-&gt;sk_allocation);  
                if (!skb)  
                    goto wait_for_memory;  

                /* 
                 * Check whether we can use HW checksum. 
                 */  
                if (sk-&gt;sk_route_caps &amp; NETIF_F_ALL_CSUM)  
                    skb-&gt;ip_summed = CHECKSUM_PARTIAL;  

                skb_entail(sk, skb);  
                copy = size_goal;  
                max = size_goal;  
            }  

            /* Try to append data to the end of skb. */  
            if (copy &gt; seglen)  
                copy = seglen;  

            /* Where to copy to? */  
            if (skb_availroom(skb) &gt; 0) {  
                /* We have some space in skb head. Superb! */  
                copy = min_t(int, copy, skb_availroom(skb));  
                err = skb_add_data_nocache(sk, skb, from, copy);  
                if (err)  
                    goto do_fault;  
            } else {  
                int merge = 0;  
                int i = skb_shinfo(skb)-&gt;nr_frags;  
                struct page *page = sk-&gt;sk_sndmsg_page;  
                int off;  

                if (page &amp;&amp; page_count(page) == 1)  
                    sk-&gt;sk_sndmsg_off = 0;  

                off = sk-&gt;sk_sndmsg_off;  

                if (skb_can_coalesce(skb, i, page, off) &amp;&amp;  
                    off != PAGE_SIZE) {  
                    /* We can extend the last page 
                     * fragment. */  
                    merge = 1;  
                } else if (i == MAX_SKB_FRAGS || !sg) {  
                    /* Need to add new fragment and cannot 
                     * do this because interface is non-SG, 
                     * or because all the page slots are 
                     * busy. */  
                    tcp_mark_push(tp, skb);  
                    goto new_segment;  
                } else if (page) {  
                    if (off == PAGE_SIZE) {  
                        put_page(page);  
                        sk-&gt;sk_sndmsg_page = page = NULL;  
                        off = 0;  
                    }  
                } else  
                    off = 0;  

                if (copy &gt; PAGE_SIZE - off)  
                    copy = PAGE_SIZE - off;  
                if (!sk_wmem_schedule(sk, copy))  
                    goto wait_for_memory;  

                if (!page) {  
                    /* Allocate new cache page. */  
                    if (!(page = sk_stream_alloc_page(sk)))  
                        goto wait_for_memory;  
                }  

                /* Time to copy data. We are close to 
                 * the end! */  
                err = skb_copy_to_page_nocache(sk, from, skb,  
                                   page, off, copy);  
                if (err) {  
                    /* If this page was new, give it to the 
                     * socket so it does not get leaked. 
                     */  
                    if (!sk-&gt;sk_sndmsg_page) {  
                        sk-&gt;sk_sndmsg_page = page;  
                        sk-&gt;sk_sndmsg_off = 0;  
                    }  
                    goto do_error;  
                }  

                /* Update the skb. */  
                if (merge) {  
                    skb_frag_size_add(&amp;skb_shinfo(skb)-&gt;frags[i - 1], copy);  
                } else {  
                    skb_fill_page_desc(skb, i, page, off, copy);  
                    if (sk-&gt;sk_sndmsg_page) {  
                        get_page(page);  
                    } else if (off + copy &lt; PAGE_SIZE) {  
                        get_page(page);  
                        sk-&gt;sk_sndmsg_page = page;  
                    }  
                }  

                sk-&gt;sk_sndmsg_off = off + copy;  
            }  

            if (!copied)  
                TCP_SKB_CB(skb)-&gt;tcp_flags &amp;= ~TCPHDR_PSH;  

            tp-&gt;write_seq += copy;  
            TCP_SKB_CB(skb)-&gt;end_seq += copy;  
            skb_shinfo(skb)-&gt;gso_segs = 0;  

            from += copy;  
            copied += copy;  
            if ((seglen -= copy) == 0 &amp;&amp; iovlen == 0)  
                goto out;  
            /*对于OOB数据，即使一个分片用光，如果还有 
            send_buff和OOB数据，就继续积累分片*/  
            if (skb-&gt;len &lt; max || (flags &amp; MSG_OOB))  
                continue;  

            if (forced_push(tp)) {  
                tcp_mark_push(tp, skb);  
                __tcp_push_pending_frames(sk, mss_now, TCP_NAGLE_PUSH);  
            } else if (skb == tcp_send_head(sk))  
                tcp_push_one(sk, mss_now);  
            continue;  

wait_for_sndbuf:  
            set_bit(SOCK_NOSPACE, &amp;sk-&gt;sk_socket-&gt;flags);  
wait_for_memory:  
            if (copied)  
                tcp_push(sk, flags &amp; ~MSG_MORE, mss_now, TCP_NAGLE_PUSH);  

            if ((err = sk_stream_wait_memory(sk, &amp;timeo)) != 0)  
                goto do_error;  

            mss_now = tcp_send_mss(sk, &amp;size_goal, flags);  
        }  
    }  

out:  
    if (copied)  
        tcp_push(sk, flags, mss_now, tp-&gt;nonagle);  
    release_sock(sk);  
    return copied;  

do_fault:  
    if (!skb-&gt;len) {  
        tcp_unlink_write_queue(skb, sk);  
        /* It is the one place in all of TCP, except connection 
         * reset, where we can be unlinking the send_head. 
         */  
        tcp_check_send_head(sk, skb);  
        sk_wmem_free_skb(sk, skb);  
    }  

do_error:  
    if (copied)  
        goto out;  
out_err:  
    err = sk_stream_error(sk, flags, err);  
    release_sock(sk);  
    return err;  
}  
</code></pre>

<pre><code>
tcp_sendmsg中，涉及对OOB数据的处理主要有：

##### 1、在调用tcp_send_mss确定分片大小的时候：
</code></pre>

<pre><code>static int tcp_send_mss(struct sock *sk,int *size_goal, int flags)
{
    intmss_now;
    mss_now= tcp_current_mss(sk);

    /*如果是OOB数据，large_allowed=0，关闭TSO*/
    *size_goal= tcp_xmit_size_goal(sk, mss_now, !(flags &amp; MSG_OOB));
    returnmss_now;
}
</code></pre>

<pre><code>如果是OOB数据，其实是关闭了TSO功能，这样做的原因是：天知道各个网卡芯片在执行分片的时候咋个处理TCP报头中的URG标志和urgent point

##### 2、在确定何时开始执行分片的发送的时候：

如果是OOB数据，即使当前已经积累了一整个分片，也不会想普通的数据一样执行发送(tcp_push)，而是继续积累直到用户下发的数据全部分片或者snd_buf/内存用尽。

##### 3、执行tcp_push的时候：

在用户下发的数据全部分片或者snd_buf/内存用尽后，进入tcp_push执行发送操作(所有的OOB数据，都会通过这个接口来执行发送)
</code></pre>

<pre><code>static inline void tcp_push(struct sock*sk, int flags, int mss_now,
                         int nonagle)
{
    if(tcp_send_head(sk)) {
        structtcp_sock *tp = tcp_sk(sk);
        if(!(flags &amp; MSG_MORE) || forced_push(tp))
            tcp_mark_push(tp,tcp_write_queue_tail(sk));    
              /*tcp_mark_urg设置tp-&gt;snd_up，标识进入OOB数据发送模式，设置urgent point
              指向urgentdata接受后的第一个字符*/
        tcp_mark_urg(tp,flags);
        __tcp_push_pending_frames(sk,mss_now,
                      (flags &amp; MSG_MORE) ? TCP_NAGLE_CORK :nonagle);
    }
}
</code></pre>

<pre><code>
#### 发送处理

使用struct tcp_sock中的snd_up来标识当前的urgent point，同时也使用该数据来判断当前是否处于urgent data发送模式，在普通数据的发送模式中tcp_sock::snd_up总是和tcp_sock::snd_una相等，只有在有urgent data发送的时候，才在tcp_push---&gt;tcp_mark_urg中设置为urgentpoint，进入到urgent data的处理模式

在tcp_transmit_skb中的以下代码段负责urgent data相关的处理：
</code></pre>

<pre><code>if (unlikely(tcp_urg_mode(tp) &amp;&amp; before(tcb-&gt;seq, tp-&gt;snd_up))) {  
    if (before(tp-&gt;snd_up, tcb-&gt;seq + 0x10000)) {  
        th-&gt;urg_ptr = htons(tp-&gt;snd_up - tcb-&gt;seq);  
        th-&gt;urg = 1;  
    } else if (after(tcb-&gt;seq + 0xFFFF, tp-&gt;snd_nxt)) {  
        th-&gt;urg_ptr = htons(0xFFFF);  
        th-&gt;urg = 1;  
    }  
}  
</code></pre>

<pre><code>
只要当前待发送的skb的seq在tcp_sock记录的urgent point前面，就需要在报头中对URG标志置位，同时如果tcp_sock记录的urgent point。如果该报文的seq距离大于16为能表示的最大值，就置TCP报头中的urgent point为65535。

#### 切换回普通模式：

在收到对方ACK的处理流程tcp_ack---&gt;tcp_clean_rtx_queue中：
</code></pre>

<pre><code>if (likely(between(tp-&gt;snd_up, prior_snd_una, tp-&gt;snd_una)))  
    tp-&gt;snd_up = tp-&gt;snd_una;  
</code></pre>

<pre><code>
#### 报文体现
根据对发送代码的分析，可以看到：如果用户使用MSG_OOB数据发送一段比较长(若干个MSS)的数据，那么线路上的报文应该是分成了若干组，每组由若干个长度为MSS的报文构成，组内的每个报文有一样的urgent pointer，指向下一组报文的起始seq，每一组的长度最长为65535。

----------
### TCP的URG标志和内核实现之三：接收的实现

大致的处理过程

TCP的接收流程：在tcp_v4_do_rcv中的相关处理(网卡收到报文触发)中，会首先通过tcp_check_urg设置tcp_sock的urg_data为TCP_URG_NOTYET(urgent point指向的可能不是本报文，而是后续报文或者前面收到的乱序报文)，并保存最新的urgent data的sequence和对于的1 BYTE urgent data到tcp_sock的urg_data (如果之前的urgent data没有读取，就会被覆盖)。

用户接收流程：在tcp_recvmsg流程中，如果发现当前的skb的数据中有urgent data，首先拷贝urgent data之前的数据，然后tcp_recvmsg退出，提示用户来接收OOB数据；在用户下一次调用tcp_recvmsg来接收数据的时候，会跳过urgent data，并设置urgent data数据接收完成。
相关的数据结构和定义

tcp_sock结构：

1、 urg_data成员，其高8bit为urgent data的接收状态；其低8位为保存的1BYTE urgent数据。urgent data的接收状态对应的宏的含义描述：
</code></pre>

<pre><code>#defineTCP_URG_VALID    0x0100  /*urgent data已经读到了tcp_sock::urg_data*/

#defineTCP_URG_NOTYET   0x0200  /*已经发现有urgent data，还没有读取到tcp_sock::urg_data*/

#defineTCP_URG_READ     0x0400  /*urgent data已经被用户通过MSG_OOB读取了*/
</code></pre>

<pre><code>
2、 urg_seq成员，为当前的urgent data的sequence

流程详情

#### TCP的接收过程

在tcp_rcv_established的slow_path中
</code></pre>

<pre><code>slow_path:  
    if (len &lt; (th-&gt;doff &lt;&lt; 2) || tcp_checksum_complete_user(sk, skb))  
        goto csum_error;  
    /* 
     *  Standard slow path. 
     */  
    if (!tcp_validate_incoming(sk, skb, th, 1))  
        return 0;  
step5:  
    if (th-&gt;ack &amp;&amp;  
        tcp_ack(sk, skb, FLAG_SLOWPATH | FLAG_UPDATE_TS_RECENT) &lt; 0)  
        goto discard;  
    tcp_rcv_rtt_measure_ts(sk, skb);  
    /* 处理紧急数据. */  
    tcp_urg(sk, skb, th);  
</code></pre>

<pre><code>
也就是在报文的CRC验证和sequence验证完成后，就会通过tcp_urg来处理接收到的urgent data ：
</code></pre>

<pre><code>static void tcp_urg(struct sock *sk, struct sk_buff *skb, const struct tcphdr *th)  
{  
    struct tcp_sock *tp = tcp_sk(sk);  

    /*收到了urgent data,则检查和设置urg_data和urg_seq成员*/  
    if (th-&gt;urg)  
        tcp_check_urg(sk, th);  

    /* Do we wait for any urgent data? - normally not... 
    发现了有urgent data，但是还没有保存到tp-&gt;urg_data*/  
    if (tp-&gt;urg_data == TCP_URG_NOTYET) {  
        u32 ptr = tp-&gt;urg_seq - ntohl(th-&gt;seq) + (th-&gt;doff * 4) -  
              th-&gt;syn;  

        /* Is the urgent pointer pointing into this packet? */  
        if (ptr &lt; skb-&gt;len) {  
            u8 tmp;  
            if (skb_copy_bits(skb, ptr, &amp;tmp, 1))  
                BUG();  
            tp-&gt;urg_data = TCP_URG_VALID | tmp;  
            if (!sock_flag(sk, SOCK_DEAD))  
                sk-&gt;sk_data_ready(sk, 0);  
        }  
    }  
}  
</code></pre>

<pre><code>
检查和设置urg_data和urg_seq成员的处理函数tcp_check_urg的具体流程
</code></pre>

<pre><code>static void tcp_check_urg(struct sock *sk, const struct tcphdr *th)  
{  
    struct tcp_sock *tp = tcp_sk(sk);  
    u32 ptr = ntohs(th-&gt;urg_ptr);  
    /*两种urgent point的解析方式: 
    一是指向urgent data之后的第一个字节 
    二是执行urgent data的结束字节(RFC1122) 
    sysctl_tcp_stdurg被设置表示当前采用的是第二种模式 
    不需要把urgent point -1来指向urgent data的结束字节*/  
    if (ptr &amp;&amp; !sysctl_tcp_stdurg)  
        ptr--;  
    ptr += ntohl(th-&gt;seq);  

    /* Ignore urgent data that we've already seen and read.  
    如果copied_seq已经大于urgent point，那么对于从tcp_rcv_established 
    来执行的，前面的tcp_validate_incoming已经拒绝了这种报文( 
    接收窗口外)，这里要处理的是哪种情形?*/  
    if (after(tp-&gt;copied_seq, ptr))  
        return;  

    /* Do not replay urg ptr. 
     * 
     * NOTE: interesting situation not covered by specs. 
     * Misbehaving sender may send urg ptr, pointing to segment, 
     * which we already have in ofo queue. We are not able to fetch 
     * such data and will stay in TCP_URG_NOTYET until will be eaten 
     * by recvmsg(). Seems, we are not obliged to handle such wicked 
     * situations. But it is worth to think about possibility of some 
     * DoSes using some hypothetical application level deadlock. 
     */  
    /*  这种情况什么时候发生?没搞明白*/  
    if (before(ptr, tp-&gt;rcv_nxt))  
        return;  

    /* Do we already have a newer (or duplicate) urgent pointer?  
    如果当前已经进入urg数据读取模式，且urgent point不大于当前 
    保存的值，那么之前已经开始了读取tp-&gt;urg_seq对应的 
    urgent 数据，无需重复处理了*/  
    if (tp-&gt;urg_data &amp;&amp; !after(ptr, tp-&gt;urg_seq))  
        return;  

    /* Tell the world about our new urgent pointer.*/  
    sk_send_sigurg(sk);  

    /* We may be adding urgent data when the last byte read was 
     * urgent. To do this requires some care. We cannot just ignore 
     * tp-&gt;copied_seq since we would read the last urgent byte again 
     * as data, nor can we alter copied_seq until this data arrives 
     * or we break the semantics of SIOCATMARK (and thus sockatmark()) 
     * 
     * NOTE. Double Dutch. Rendering to plain English: author of comment 
     * above did something sort of  send("A", MSG_OOB); send("B", MSG_OOB); 
     * and expect that both A and B disappear from stream. This is _wrong_. 
     * Though this happens in BSD with high probability, this is occasional. 
     * Any application relying on this is buggy. Note also, that fix "works" 
     * only in this artificial test. Insert some normal data between A and B and we will 
     * decline of BSD again. Verdict: it is better to remove to trap 
     * buggy users. 
     */  
     /*用户下一次要读取的数据就是用户还没有读取的urgent数据 
    且当前存在新的用户未读取数据*/  
    if (tp-&gt;urg_seq == tp-&gt;copied_seq &amp;&amp; tp-&gt;urg_data &amp;&amp;  
        !sock_flag(sk, SOCK_URGINLINE) &amp;&amp; tp-&gt;copied_seq != tp-&gt;rcv_nxt) {  
        struct sk_buff *skb = skb_peek(&amp;sk-&gt;sk_receive_queue);  
        tp-&gt;copied_seq++;  
        if (skb &amp;&amp; !before(tp-&gt;copied_seq, TCP_SKB_CB(skb)-&gt;end_seq)) {  
            __skb_unlink(skb, &amp;sk-&gt;sk_receive_queue);  
            __kfree_skb(skb);  
        }  
    }  

    tp-&gt;urg_data = TCP_URG_NOTYET;  
    tp-&gt;urg_seq = ptr;  

    /* Disable header prediction. */  
    tp-&gt;pred_flags = 0;  
}  
</code></pre>

<pre><code>
#### 用户接收数据接口
##### 用户接收URG数据的接口
在用户接收数据的tcp_recvmsg函数中，如果用户通过MSG_OOB来接收数据，会进入tcp_recv_urg处理
</code></pre>

<pre><code>static int tcp_recv_urg(struct sock *sk, struct msghdr *msg, int len, int flags)  
{  
    struct tcp_sock *tp = tcp_sk(sk);  

    /* No URG data to read.  
    用户已经读取过了*/  
    if (sock_flag(sk, SOCK_URGINLINE) || !tp-&gt;urg_data ||  
        tp-&gt;urg_data == TCP_URG_READ)  
        return -EINVAL; /* Yes this is right ! */  

    if (sk-&gt;sk_state == TCP_CLOSE &amp;&amp; !sock_flag(sk, SOCK_DONE))  
        return -ENOTCONN;  
    /*当前的tp-&gt;urg_data为合法的数据，可以读取*/  
    if (tp-&gt;urg_data &amp; TCP_URG_VALID) {  
        int err = 0;  
        char c = tp-&gt;urg_data;  
        /*标识urgent data已读*/  
        if (!(flags &amp; MSG_PEEK))  
            tp-&gt;urg_data = TCP_URG_READ;  

        /* Read urgent data. */  
        msg-&gt;msg_flags |= MSG_OOB;  

        if (len &gt; 0) {  
            if (!(flags &amp; MSG_TRUNC))  
                err = memcpy_toiovec(msg-&gt;msg_iov, &amp;c, 1);  
            len = 1;  
        } else  
            msg-&gt;msg_flags |= MSG_TRUNC;  

        return err ? -EFAULT : len;  
    }  

    if (sk-&gt;sk_state == TCP_CLOSE || (sk-&gt;sk_shutdown &amp; RCV_SHUTDOWN))  
        return 0;  

    /* Fixed the recv(..., MSG_OOB) behaviour.  BSD docs and 
     * the available implementations agree in this case: 
     * this call should never block, independent of the 
     * blocking state of the socket. 
     * Mike &lt;pall@rz.uni-karlsruhe.de&gt; 
     */  
    return -EAGAIN;  
}  
</code></pre>

<pre><code>
##### 用户接收普通数据的接口中的相关处理

在用户接收数据的tcp_recvmsg函数中，在查找到待拷贝的skb后，首先拷贝urgent data数据前的数据，然后退出接收过程，在用户下一次执行tcp_recvmsg的时候跳过urgent data，设置urgent data读取结束

查找到准备拷贝的skb后的处理：
</code></pre>

<pre><code>found_ok_skb:  
/* Ok so how much can we use? */  
used = skb-&gt;len - offset;  
if (len &lt; used)  
    used = len;  

/* 当前有urg_data数据*/  
if (tp-&gt;urg_data) {  
    u32 urg_offset = tp-&gt;urg_seq - *seq;  
    /*urgent data在当前待拷贝的数据范围内*/  
    if (urg_offset &lt; used) {  
        if (!urg_offset) {/*待拷贝的数据就是urgent data，跨过该urgent data， 
        只给用户读取后面的数据*/  
            if (!sock_flag(sk, SOCK_URGINLINE)) {  
                ++*seq;  
                urg_hole++;  
                offset++;  
                used--;  
                if (!used)  
                    goto skip_copy;  
            }  
        }   
        } else/*指定只拷贝urgent data数据之前的，完成后在下一次循环 
        开始的位置，会退出循环，返回用户；下一次用户调用tcp_recvmsg 
        就进入到上面的分支了*/  
            used = urg_offset;  
    }  
}   
</code></pre>

<pre><code></code></pre>

<pre><code>skip_copy:  
        /*用户读取的数据跨过了urgent point，设置读取结束 
        开启fast path*/  
        if (tp-&gt;urg_data &amp;&amp; after(tp-&gt;copied_seq, tp-&gt;urg_seq)) {  
            tp-&gt;urg_data = 0;  
            tcp_fast_path_check(sk);  
        }  
        if (used + offset &lt; skb-&gt;len)  
            continue;  
</code></pre>

<pre><code>
在接收完urgent data数据前的所有数据之后， tcp_recvmsg的以下代码片段得到执行，这段代码退出当前接收过程，提示用户有urgent data数据到来，需要用MSG_OOB来接收
</code></pre>

<pre><code>if (tp-&gt;urg_data &amp;&amp; tp-&gt;urg_seq == *seq) {  
    if (copied)  
        break;  
    if (signal_pending(current)) {  
        copied = timeo ? sock_intr_errno(timeo) : -EAGAIN;  
        break;  
    }  
}  
</code></pre>

<p>```</p>

<h3>后记</h3>

<p>TCP的urg数据，由于定义和实现上的混乱，当前已经不建议使用，但是为了兼容之前已经已经存在的实现，该机制会长期在内核中存在，如果不了解该机制及其内核行为，有可能就很难解释一些奇怪的问题：比如某段代码不小心地造成send接口事实上设置了MSG_OOB，就会造成接收端少了一个BYTE。</p>
]]></content>
  </entry>
  
</feed>
