<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: kernel~mm | kk Blog —— 通用基础]]></title>
  <link href="http://abcdxyzk.github.io/blog/cats/kernel~mm/atom.xml" rel="self"/>
  <link href="http://abcdxyzk.github.io/"/>
  <updated>2014-12-11T01:25:43+08:00</updated>
  <id>http://abcdxyzk.github.io/</id>
  <author>
    <name><![CDATA[kk]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Linux的进程内核栈]]></title>
    <link href="http://abcdxyzk.github.io/blog/2014/05/06/kernel-mm-stack/"/>
    <updated>2014-05-06T14:38:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2014/05/06/kernel-mm-stack</id>
    <content type="html"><![CDATA[<p><a href="http://blog.chinaunix.net/uid-20543672-id-2996319.html">http://blog.chinaunix.net/uid-20543672-id-2996319.html</a></p>

<ul>
<li>内核具有非常小的栈，它可能只和一个4096或8192字节大小的页那样小</li>
</ul>


<h4>什么是进程的“内核栈”？</h4>

<p>  在每一个进程的生命周期中，必然会通过到系统调用陷入内核。在执行系统调用陷入内核之后，这些内核代码所使用的栈并不是原先用户空间中的栈，而是一个内核空间的栈，这个称作进程的“内核栈”。</p>

<p>  比如，有一个简单的字符驱动实现了open方法。在这个驱动挂载后，应用程序对那个驱动所对应的设备节点执行open操作，这个应用程序的open其实 就通过glib库调用了Linux的open系统调用，执行系统调用陷入内核后，处理器转换为了特权模式（具体的转换机制因构架而异，对于ARM来说普通 模式和用户模式的的栈针（SP）是不同的寄存器），此时使用的栈指针就是内核栈指针，他指向内核为每个进程分配的内核栈空间。</p>

<h4>内核栈的作用</h4>

<p>  我个人的理解是：在陷入内核后，系统调用中也是存在函数调用和自动变量，这些都需要栈支持。用户空间的栈显然不安全，需要内核栈的支持。此外，内核栈同时用于保存一些系统调用前的应用层信息（如用户空间栈指针、系统调用参数）。</p>

<h4>内核栈与进程结构体的关联</h4>

<p>  每个进程在创建的时候都会得到一个内核栈空间，内核栈和进程的对应关系是通过2个结构体中的指针成员来完成的：</p>

<h5>（1）struct task_struct</h5>

<p>  在学习Linux进程管理肯定要学的结构体，在内核中代表了一个进程，其中记录的进程的所有状态信息，定义在Sched.h (include\linux)。<br/>
  其中有一个成员：void *stack;就是指向下面的内核栈结构体的“栈底”。<br/>
  在系统运行的时候，宏current获得的就是当前进程的struct task_struct结构体。</p>

<h5>（2）内核栈结构体union thread_union</h5>

<pre><code>    union thread_union {
        struct thread_info thread_info;
        unsigned long stack[THREAD_SIZE/sizeof(long)];
    };
</code></pre>

<p> 其中struct thread_info是记录部分进程信息的结构体，其中包括了进程上下文信息:
<code>
    /*
     * low level task data that entry.S needs immediate access to.
     * __switch_to() assumes cpu_context follows immediately after cpu_domain.
     */
    struct thread_info {
        unsigned long    flags;    /* low level flags */
        int      preempt_count;  /* 0 =&gt; preemptable, &lt;0 =&gt; bug */
        mm_segment_t    addr_limit;  /* address limit */
        struct task_struct  *task;    /* main task structure */
        struct exec_domain  *exec_domain;  /* execution domain */
        __u32      cpu;    /* cpu */
        __u32      cpu_domain;  /* cpu domain */
        struct cpu_context_save  cpu_context;  /* cpu context */
        __u32      syscall;  /* syscall number */
        __u8      used_cp[16];  /* thread used copro */
        unsigned long    tp_value;
        struct crunch_state  crunchstate;
        union fp_state    fpstate __attribute__((aligned(8)));
        union vfp_state    vfpstate;
    #ifdef CONFIG_ARM_THUMBEE
        unsigned long    thumbee_state;  /* ThumbEE Handler Base register */
        #endif
        struct restart_block  restart_block;
    };
</code>
  关键是其中的task成员，指向的是所创建的进程的struct task_struct结构体</p>

<p>  而其中的stack成员就是内核栈。从这里可以看出内核栈空间和 thread_info是共用一块空间的。如果内核栈溢出， thread_info就会被摧毁，系统崩溃了～～～</p>

<p>内核栈&mdash;struct thread_info&mdash;-struct task_struct三者的关系入下图：
<img src="/images/kernel/20140506.jpg" alt="" /></p>

<h4>内核栈的产生</h4>

<p>  在进程被创建的时候，fork族的系统调用中会分别为内核栈和struct task_struct分配空间，调用过程是：
fork族的系统调用&mdash;>do_fork&mdash;>copy_process&mdash;>dup_task_struct
在dup_task_struct函数中：
```
    static struct task_struct <em>dup_task_struct(struct task_struct </em>orig)
    {
        struct task_struct <em>tsk;
        struct thread_info </em>ti;
        unsigned long *stackend;</p>

<pre><code>    int err;

    prepare_to_copy(orig);

    tsk = alloc_task_struct();
    if (!tsk)
        return NULL;

    ti = alloc_thread_info(tsk);
    if (!ti) {
        free_task_struct(tsk);
        return NULL;
    }

    err = arch_dup_task_struct(tsk, orig);
    if (err)
        goto out;

    tsk-&gt;stack = ti;

    err = prop_local_init_single(&amp;tsk-&gt;dirties);
    if (err)
        goto out;

    setup_thread_stack(tsk, orig);
    ......
</code></pre>

<pre><code>其中alloc_task_struct使用内核的slab分配器去为所要创建的进程分配struct task_struct的空间  
而alloc_thread_info使用内核的伙伴系统去为所要创建的进程分配内核栈（union thread_union ）空间

#### 注意：
后面的tsk-&gt;stack = ti;语句，这就是关联了struct task_struct和内核栈
而在setup_thread_stack(tsk, orig);中，关联了内核栈和struct task_struct：
</code></pre>

<pre><code>static inline void setup_thread_stack(struct task_struct *p, struct task_struct *org)
{
    *task_thread_info(p) = *task_thread_info(org);
    task_thread_info(p)-&gt;task = p;
}
</code></pre>

<pre><code>
#### 内核栈的大小
  由于是每一个进程都分配一个内核栈空间，所以不可能分配很大。这个大小是构架相关的，一般以页为单位。其实也就是上面我们看到的THREAD_SIZE， 这个值一般为4K或者8K。对于ARM构架，这个定义在Thread_info.h (arch\arm\include\asm)，
</code></pre>

<pre><code>#define THREAD_SIZE_ORDER  1
#define THREAD_SIZE   8192
#define THREAD_START_SP   (THREAD_SIZE - 8)
</code></pre>

<p>```
所以ARM的内核栈是8KB
在（内核）驱动编程时需要注意的问题：
  由于栈空间的限制，在编写的驱动（特别是被系统调用使用的底层函数）中要注意避免对栈空间消耗较大的代码，比如递归算法、局部自动变量定义的大小等等</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linux内核获取当前进程指针]]></title>
    <link href="http://abcdxyzk.github.io/blog/2014/04/30/kernel-mm-current-rbp/"/>
    <updated>2014-04-30T16:10:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2014/04/30/kernel-mm-current-rbp</id>
    <content type="html"><![CDATA[<h4>一、内存数据表示：</h4>

<p>  我们在教材或阅读中，经常需要直观的用图示来展示数据在内存中的分布，那么数据是如何在内存中组织的呢？不同的机器有不同的表示法，我们以最常见的Intel X86系列计算机为例来说明这个问题。</p>

<p><img src="/images/kernel/20140430-1.jpg" alt="" /></p>

<p>  如上图示内存示意图：内存低址在上。内存高址在下，内存单位为16bit。对于基于intel i386架构的计算机，系统采用小端字节序来存放数据，所谓小端字节序是指低序字节低地址，高序字节高地址(内存地址增大方向)，大端字节序反之，给定系统所用的字节序称为主机字节序；CPU也以小端字节序形式读取数据，如上图所示，如果变量num是16位的short短整类型，则CPU从内存中读出的num=0x1234；如果num是32位的int类型，则CPU从内存中读出的是num=0x56781234,其中num地址是0x12345678，即&amp;num=0x12345678</p>

<h4>二、linux内核获取进程任务结构的指针</h4>

<p>  明白了系统内存数据表示，我们现在来看看linux内核是如何获取当前进程的任务结构指针的，以下代码均参照linux内核2.4.0的源码。<br/>
  在include\asm-i386\ current.h中
<code>
    #ifndef _I386_CURRENT_H
    #define _I386_CURRENT_H
    struct task_struct;
    static inline struct task_struct * get_current(void)
    {
        struct task_struct *current;
        __asm__("andl %%esp,%0; ":"=r" (current) : "0" (~8191UL));
        return current;
    }
    #define current get_current()
    #endif /* !(_I386_CURRENT_H) */
</code>
  每个进程都有一个task_struct任务结构，和一片用于系统空间堆栈的存储空间，他们在物理内存空间中也是联系在一起的，当给进程申请task_struct任务结构空间时，系统将连同系统的堆栈空间一起分配，如下图为某个进程切换时刻的内存图：</p>

<p><img src="/images/kernel/20140430-2.jpg" alt="" /></p>

<p>  下面针对代码实现来分析一下系统如何通过一系列操作获取进程在内核中的任务结构指针的：
  由于linux内核分配进程任务结构空间时，是以8KB(2个页面空间，即2<sup>1</sup>*4KB，linux对物理内存空间和虚拟内存空间管理时，均规定其页面单位的尺寸为4KB)为单位来分配的，所以内存应用地址是8KB(2<sup>13</sup>)的整数倍，即指针地址的低13位全为0，所以根据小端字节序，分配内存返回地址应该是指向struct task_struct结构，如图中的0xc2342000地址所指，至于为何采用代码中的做法而不是直接将此指针保存在全局变量中以供应用，内核是从其自身的效率方面来考虑的，我们在此只针对代码解释：
  根据上图，此刻内存esp内容必定在0xc2342000和0xc2344000之间的一个数值，我们假设取0xc2343ffe(即堆栈压栈EIP、返回地址、内部数据等相关数据了，地址值要减小；只要符合0xc2342xxx 、0xc2343xxx的地址指针都是正确的)，来通过代码运算看是否current的指针是0xc2342000。
<code>
    __asm__("andl %%esp,%0; ":"=r" (current) : "0" (~8191UL));
</code>
语句的意思是将ESP的内容与8191UL的反码按位进行与操作，之后再把结果赋值给current，其中8191UL=8192-1=2<sup>13</sup>-1,计算过程如下：
<code>
    8192UL=2^13 0000 0000 0000 0000 0010 0000 0000 0000
    8191UL 0000 0000 0000 0000 0001 1111 1111 1111
    ~8191UL(反码) 1111 1111 1111 1111 1110 0000 0000 0000
    0xc2343ffe 1100 0010 0011 0100 0011 1111 1111 1110
    andl结果： 1100 0010 0011 0100 0010 0000 0000 0000
    || (对照着看)
    0x c 2 3 4 2 0 0 0
</code>
  所以按位与操作之后的结果位0xc2342000，正好是struct task_struct结构的地址指针.通过观察可知，只要符合0xc2342xxx 、0xc2343xxx的地址指针经过相同的计算，都可以得到内核进程任务结构的指针。<br/>
  另外，在进入中断或系统调用时所引用的宏操作(include\asm-i386\ hw_irq.h):
<code>
    #define GET_CURRENT \
        "movl %esp, %ebx\n\t" \
        "andl $-8192, %ebx\n\t"
</code>
  其原理与上述描述也是一致的。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[linux内存分配]]></title>
    <link href="http://abcdxyzk.github.io/blog/2014/04/30/kernel-mm-map/"/>
    <updated>2014-04-30T16:04:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2014/04/30/kernel-mm-map</id>
    <content type="html"><![CDATA[<h5>关于虚拟内存有三点需要注意：</h5>

<p>  1、4G的进程地址空间被人为的分为两个部分&ndash;用户空间与内核空间。用户空间从0到3G（0xc0000000）,内核空间占据3G到4G。用户进程通常情况下只能访问用户空间的虚拟地址，不能访问内核空间的虚拟地址。例外情况只有用户进程进行系统调用（代表用户进程在内核态执行）等时刻可以访问到内核空间。<br/>
  2、用户空间对应进程，所以每当进程切换，用户空间就会跟着变化；而内核空间是由内核负责映射，它并不会跟着进程变化，是固定的。内核空间地址有自己对应的页表，用户进程各自有不同的页表。<br/>
  3、每个进程的用户空间都是完全独立、互不相干的。</p>

<h4>一、4G地址空间解析图</h4>

<p><img src="/images/kernel/20140430-1.jpeg" alt="" /></p>

<p>  上图展示了整个进程地址空间的分布，其中4G的地址空间分为两部分，在用户空间内，对应了内存分布的五个段：数据段、代码段、BSS段、堆、栈。在上篇文章中有详细的介绍。</p>

<p>二、虚拟地址空间分配及其与物理内存对应图</p>

<p><img src="/images/kernel/20140430-2.jpeg" alt="" /></p>

<p>  这个图示内核用户空间的划分，图中最重要的就是高端内存的映射<br/>
  其中kmalloc和vmalloc函数申请的空间对应着不同的区域，同时又不同的含义。</p>

<p>三、物理内存分配图</p>

<p><img src="/images/kernel/20140430-3.jpeg" alt="" /></p>

<p>  这张图中页解释了三者的不同关系，和上篇文章中的内容有相似之处。</p>

<h5>伙伴算法：</h5>

<p>  一种物理内存分配和回收的方法，物理内存所有空闲页都记录在BUDDY链表中。首选，系统建立一个链表，链表中的每个元素代表一类大小的物理内存，分别为2的0次方、1次方、2次方，个页大小，对应4K、8K、16K的内存，没一类大小的内存又有一个链表，表示目前可以分配的物理内存。例如现在仅存需要分配8K的物理内存，系统首先从8K那个链表中查询有无可分配的内存，若有直接分配；否则查找16K大小的链表，若有，首先将16K一分为二，将其中一个分配给进程，另一个插入8K的链表中，若无，继续查找32K，若有，首先把32K一分为二，其中一个16K大小的内存插入16K链表中，然后另一个16K继续一分为二，将其中一个插入8K的链表中，另一个分配给进程&hellip;&hellip;..以此类推。当内存释放时，查看相邻内存有无空闲，若存在两个联系的8K的空闲内存，直接合并成一个16K的内存，插入16K链表中。（伙伴算法用于物理内存分配方案）</p>

<h5>SLAB算法：</h5>

<p>  是一种对伙伴算的一种补充，对于用户进程的内存分配，伙伴算法已经够好了，但对于内核进程，还需要存在一类很小的数据（字节大小，比如进程描述符、虚拟内存描述符等），若每次给几个字节的数据分配一个4KB的页，实在太浪费，于是就有了SLBA算法，SLAB算法其实就是把一个页用力劈成一小块一小块，然后再分配。</p>
]]></content>
  </entry>
  
</feed>
