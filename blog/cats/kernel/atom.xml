<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: kernel | kk Blog —— 通用基础]]></title>
  <link href="http://abcdxyzk.github.io/blog/cats/kernel/atom.xml" rel="self"/>
  <link href="http://abcdxyzk.github.io/"/>
  <updated>2015-06-01T15:48:01+08:00</updated>
  <id>http://abcdxyzk.github.io/</id>
  <author>
    <name><![CDATA[kk]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[TCP三次握手源码详解]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/06/01/kernel-net-shark-hand/"/>
    <updated>2015-06-01T14:24:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/06/01/kernel-net-shark-hand</id>
    <content type="html"><![CDATA[<p><a href="http://blog.csdn.net/qy532846454/article/details/7882819">http://blog.csdn.net/qy532846454/article/details/7882819</a></p>

<p><a href="http://m.bianceng.cn/OS/Linux/201301/35179_6.htm">http://m.bianceng.cn/OS/Linux/201301/35179_6.htm</a></p>

<p>内核：2.6.34</p>

<p>TCP是应用最广泛的传输层协议，其提供了面向连接的、可靠的字节流服务，但也正是因为这些特性，使得TCP较之UDP异常复杂，还是分两部分[创建与使用]来进行分析。这篇主要包括TCP的创建及三次握手的过程。</p>

<p>编程时一般用如下语句创建TCP Socket：</p>

<pre><code>    socket(AF_INET, SOCK_DGRAM, IPPROTO_TCP)  
</code></pre>

<p>由此开始分析，调用接口[net/socket.c]: SYSCALL_DEFINE3(socket)</p>

<p>其中执行两步关键操作：sock_create()与sock_map_fd()</p>

<pre><code>    retval = sock_create(family, type, protocol, &amp;sock);  
    if (retval &lt; 0)  
        goto out;  
    retval = sock_map_fd(sock, flags &amp; (O_CLOEXEC | O_NONBLOCK));  
    if (retval &lt; 0)  
        goto out_release;  
</code></pre>

<p>  sock_create()用于创建socket，sock_map_fd()将之映射到文件描述符，使socket能通过fd进行访问，着重分析sock_create()的创建过程。</p>

<pre><code>    sock_create() -&gt; __sock_create()
</code></pre>

<p>  从__sock_create()代码看到创建包含两步：sock_alloc()和pf->create()。sock_alloc()分配了sock内存空间并初始化inode；pf->create()初始化了sk。</p>

<pre><code>    sock = sock_alloc();  
    sock-&gt;type = type;  
    ……  
    pf = rcu_dereference(net_families[family]);  
    ……  
    pf-&gt;create(net, sock, protocol, kern);  
</code></pre>

<h4>sock_alloc()</h4>

<p>  分配空间，通过new_inode()分配了节点(包括socket)，然后通过SOCKET_I宏获得sock，实际上inode和sock是在new_inode()中一起分配的，结构体叫作sock_alloc。</p>

<pre><code>    inode = new_inode(sock_mnt-&gt;mnt_sb);  
    sock = SOCKET_I(inode);  
</code></pre>

<p>  设置inode的参数，并返回sock。</p>

<pre><code>    inode-&gt;i_mode = S_IFSOCK | S_IRWXUGO;  
    inode-&gt;i_uid = current_fsuid();  
    inode-&gt;i_gid = current_fsgid();  
    return sock;  
</code></pre>

<p>  继续往下看具体的创建过程：new_inode()，在分配后，会设置i_ino和i_state的值。</p>

<pre><code>    struct inode *new_inode(struct super_block *sb)  
    {  
        ……  
        inode = alloc_inode(sb);  
        if (inode) {  
            spin_lock(&amp;inode_lock);  
            __inode_add_to_lists(sb, NULL, inode);  
            inode-&gt;i_ino = ++last_ino;  
            inode-&gt;i_state = 0;  
            spin_unlock(&amp;inode_lock);  
        }  
        return inode;  
    }  
</code></pre>

<p>  其中的alloc_inode() -> sb->s_op->alloc_inode()，sb是sock_mnt->mnt_sb，所以alloc_inode()指向的是sockfs的操作函数sock_alloc_inode。</p>

<pre><code>    static const struct super_operations sockfs_ops = {  
        .alloc_inode = sock_alloc_inode,  
        .destroy_inode =sock_destroy_inode,  
        .statfs = simple_statfs,  
    };  
</code></pre>

<p>  sock_alloc_inode()中通过kmem_cache_alloc()分配了struct socket_alloc结构体大小的空间，而struct socket_alloc结构体定义如下，但只返回了inode，实际上socket和inode都已经分配了空间，在之后就可以通过container_of取到socket。</p>

<pre><code>    static struct inode *sock_alloc_inode(struct super_block *sb)  
    {  
        struct socket_alloc *ei;  
        ei = kmem_cache_alloc(sock_inode_cachep, GFP_KERNEL);  
        ......  
        return &amp;ei-&gt;vfs_inode;  
    }  
    struct socket_alloc {  
        struct socket socket;  
        struct inode vfs_inode;  
    };  

    net_families[AF_INET]:  
    static const struct net_proto_family inet_family_ops = {  
        .family = PF_INET,  
        .create = inet_create,  
        .owner = THIS_MODULE,  
    };  
</code></pre>

<p>err = pf->create(net, sock, protocol, kern); ==> inet_create()
这段代码就是从inetsw[]中取到适合的协议类型answer，sock->type就是传入socket()函数的type参数SOCK_DGRAM，最终取得结果answer->ops==inet_stream_ops，从上面这段代码还可以看出以下问题：</p>

<p>  socket(AF_INET, SOCK_RAW, IPPROTO_IP)这样是不合法的，因为SOCK_RAW没有默认的协议类型；同样socket(AF_INET, SOCK_DGRAM, IPPROTO_IP)与socket(AF_INET, SOCK_DGRAM, IPPROTO_TCP)是一样的，因为TCP的默认协议类型是IPPTOTO_TCP；SOCK_STREAM与IPPROTO_UDP同上。</p>

<pre><code>    sock-&gt;state = SS_UNCONNECTED;  
    list_for_each_entry_rcu(answer, &amp;inetsw[sock-&gt;type], list) {  
        err = 0;  
        /* Check the non-wild match. */  
        if (protocol == answer-&gt;protocol) {  
            if (protocol != IPPROTO_IP)  
                break;  
        } else {  
            /* Check for the two wild cases. */  
            if (IPPROTO_IP == protocol) {  
                protocol = answer-&gt;protocol;  
                break;  
            }  
            if (IPPROTO_IP == answer-&gt;protocol)  
                break;  
        }  
        err = -EPROTONOSUPPORT;  
    }  
</code></pre>

<p>sock->ops指向inet_stream_ops，然后创建sk，sk->proto指向tcp_prot，注意这里分配的大小是struct tcp_sock，而不仅仅是struct sock大小</p>

<pre><code>    sock-&gt;ops = answer-&gt;ops;  
    answer_prot = answer-&gt;prot;  
    ……  
    sk = sk_alloc(net, PF_INET, GFP_KERNEL, answer_prot);  
</code></pre>

<p>然后设置inet的一些参数，这里直接将sk类型转换为inet，因为在sk_alloc()中分配的是struct tcp_sock结构大小，返回的是struct sock，利用了第一个成员的特性，三者之间的关系如下图：
<code>
    inet = inet_sk(sk);  
    ……  
    inet-&gt;inet_id = 0;  
    sock_init_data(sock, sk);  
</code>
其中有些设置是比较重要的，如
<code>
    sk-&gt;sk_state = TCP_CLOSE;  
    sk_set_socket(sk, sock);  
    sk-&gt;sk_protocol = protocol;  
    sk-&gt;sk_backlog_rcv = sk-&gt;sk_prot-&gt;backlog_rcv;  
</code></p>

<p>创建socket后，接下来的流程会因为客户端或服务器的不同而有所差异，下面着重于分析建立连接的三次握手过程。典型的客户端流程：<br/>
connect() -> send() -> recv()</p>

<p>典型的服务器流程：<br/>
bind() -> listen() -> accept() -> recv() -> send()</p>

<h4>客户端流程</h4>

<p>发送SYN报文，向服务器发起tcp连接</p>

<pre><code>            connect(fd, servaddr, addrlen);
                -&gt; SYSCALL＿DEFINE3() 
                -&gt; sock-&gt;ops-&gt;connect() == inet_stream_connect (sock-&gt;ops即inet_stream_ops)
                -&gt; tcp_v4_connect()
</code></pre>

<p>查找到达[daddr, dport]的路由项，路由项的查找与更新与”路由表”章节所述一样。要注意的是由于是作为客户端调用，创建socket后调用connect，因而saddr, sport都是0，同样在未查找路由前，要走的出接口oif也是不知道的，因此也是0。在查找完路由表后(注意不是路由缓存)，可以得知出接口，但并未存储到sk中。因此插入的路由缓存是特别要注意的：它的键值与实际值是不相同的，这个不同点就在于oif与saddr，键值是[saddr=0, sport=0, daddr, dport, oif=0]，而缓存项值是[saddr, sport=0, daddr, dport, oif]。</p>

<pre><code>    tmp = ip_route_connect(&amp;rt, nexthop, inet-&gt;inet_saddr,  
                            RT_CONN_FLAGS(sk), sk-&gt;sk_bound_dev_if,  
                            IPPROTO_TCP,  
                            inet-&gt;inet_sport, usin-&gt;sin_port, sk, 1);  
    if (tmp &lt; 0) {  
        if (tmp == -ENETUNREACH)  
            IP_INC_STATS_BH(sock_net(sk), IPSTATS_MIB_OUTNOROUTES);  
        return tmp;  
    }  
</code></pre>

<p>通过查找到的路由项，对inet进行赋值，可以看到，除了sport，都赋予了值，sport的选择复杂点，因为它要随机从未使用的本地端口中选择一个。</p>

<pre><code>    if (!inet-&gt;inet_saddr)  
        inet-&gt;inet_saddr = rt_rt_src;   
    inet-&gt;inet_rcv_addr = inet-&gt;inet_saddr;  
    ……  
    inet-&gt;inet_dport = usin-&gt;sin_port;  
    inet-&gt;inet_daddr = daddr;  
</code></pre>

<p>状态从CLOSING转到TCP_SYN_SENT，也就是我们熟知的TCP的状态转移图。</p>

<pre><code>    tcp_set_state(sk, TCP_SYN_SENT);  
</code></pre>

<p>插入到bind链表中</p>

<pre><code>    err = inet_hash_connect(&amp;tcp_death_row, sk); //== &gt; __inet_hash_connect()  
</code></pre>

<p>当snum==0时，表明此时源端口没有指定，此时会随机选择一个空闲端口作为此次连接的源端口。low和high分别表示可用端口的下限和上限，remaining表示可用端口的数，注意这里的可用只是指端口可以用作源端口，其中部分端口可能已经作为其它socket的端口号在使用了，所以要循环1~remaining，直到查找到空闲的源端口。</p>

<pre><code>    if (!snum) {  
        inet_get_local_port_range(&amp;low, &amp;high);  
        remaining = (high - low) + 1;  
        ……  
        for (i = 1; i &lt;= remaining; i++) {  
            ……// choose a valid port  
        }  
    }  
</code></pre>

<p>下面来看下对每个端口的检查，即//choose a valid port部分的代码。这里要先了解下tcp的内核表组成，udp的表内核表udptable只是一张hash表，tcp的表则稍复杂，它的名字是tcp_hashinfo，在tcp_init()中被初始化，这个数据结构定义如下(省略了不相关的数据)：</p>

<pre><code>    struct inet_hashinfo {  
        struct inet_ehash_bucket *ehash;  
        ……  
        struct inet_bind_hashbucket *bhash;  
        ……  
        struct inet_listen_hashbucket  listening_hash[INET_LHTABLE_SIZE]  
                        ____cacheline_aligned_in_smp;  
    };  
</code></pre>

<p>从定义可以看出，tcp表又分成了三张表ehash, bhash, listening_hash，其中ehash, listening_hash对应于socket处在TCP的ESTABLISHED, LISTEN状态，bhash对应于socket已绑定了本地地址。三者间并不互斥，如一个socket可同时在bhash和ehash中，由于TIME_WAIT是一个比较特殊的状态，所以ehash又分成了chain和twchain，为TIME_WAIT的socket单独形成一张表。</p>

<p>回到刚才的代码，现在还只是建立socket连接，使用的就应该是tcp表中的bhash。首先取得内核tcp表的bind表 – bhash，查看是否已有socket占用：<br/>
  如果没有，则调用inet_bind_bucket_create()创建一个bind表项tb，并插入到bind表中，跳转至goto ok代码段；<br/>
  如果有，则跳转至goto ok代码段。<br/>
  进入ok代码段表明已找到合适的bind表项(无论是创建的还是查找到的)，调用inet_bind_hash()赋值源端口inet_num。</p>

<pre><code>    for (i = 1; i &lt;= remaining; i++) {  
        port = low + (i + offset) % remaining;  
        head = &amp;hinfo-&gt;bhash[inet_bhashfn(net, port, hinfo-&gt;bhash_size)];  
        ……  
        inet_bind_bucket_for_each(tb, node, &amp;head-&gt;chain) {  
            if (net_eq(ib_net(tb), net) &amp;&amp; tb-&gt;port == port) {  
                if (tb-&gt;fastreuse &gt;= 0)  
                    goto next_port;  
                WARN_ON(hlist_empty(&amp;tb-&gt;owners));  
                if (!check_established(death_row, sk, port, &amp;tw))  
                    goto ok;  
                goto next_port;  
            }  
        }  

        tb = inet_bind_bucket_create(hinfo-&gt;bind_bucket_cachep, net, head, port);  
        ……  
        next_port:  
            spin_unlock(&amp;head-&gt;lock);  
    }  

    ok:  
        ……  
    inet_bind_hash(sk, tb, port);  
        ……  
        goto out;  
</code></pre>

<p>在获取到合适的源端口号后，会重建路由项来进行更新：</p>

<pre><code>    err = ip_route_newports(&amp;rt, IPPROTO_TCP, inet-&gt;inet_sport, inet-&gt;inet_dport, sk);  
</code></pre>

<p>函数比较简单，在获取sport前已经查找过一次路由表，并插入了key=[saddr=0, sport=0, daddr, dport, oif=0]的路由缓存项；现在获取到了sport，调用ip_route_output_flow()再次更新路由缓存表，它会添加key=[saddr=0, sport, daddr, dport, oif=0]的路由缓存项。这里可以看出一个策略选择，查询路由表->获取sport->查询路由表，为什么不是获取sport->查询路由表的原因可能是效率的问题。</p>

<pre><code>    if (sport != (*rp)-&gt;fl.fl_ip_sport ||  
                    dport != (*rp)-&gt;fl.fl_ip_dport) {  
        struct flowi fl;  

        memcpy(&amp;fl, &amp;(*rp)-&gt;fl, sizeof(fl));  
        fl.fl_ip_sport = sport;  
        fl.fl_ip_dport = dport;  
        fl.proto = protocol;  
        ip_rt_put(*rp);  
        *rp = NULL;  
        security_sk_classify_flow(sk, &amp;fl);  
        return ip_route_output_flow(sock_net(sk), rp, &amp;fl, sk, 0);  
    }  
</code></pre>

<p>write_seq相当于第一次发送TCP报文的ISN，如果为0，则通过计算获取初始值，否则延用上次的值。在获取完源端口号，并查询过路由表后，TCP正式发送SYN报文，注意在这之前TCP状态已经更新成了TCP_SYN_SENT，而在函数最后才调用tcp_connect(sk)发送SYN报文，这中间是有时差的。</p>

<pre><code>    if (!tp-&gt;write_seq)  
        tp-&gt;write_seq = secure_tcp_sequence_number(inet-&gt;inet_saddr,  
                                        inet-&gt;inet_daddr,  
                                        inet-&gt;inet_sport,  
                                        usin-&gt;sin_port);  
    inet-&gt;inet_id = tp-&gt;write_seq ^ jiffies;  
    err = tcp_connect(sk);  
</code></pre>

<h5>tcp_connect()　发送SYN报文</h5>

<p>几步重要的代码如下，tcp_connect_init()中设置了tp->rcv_nxt=0，tcp_transmit_skb()负责发送报文，其中seq=tcb->seq=tp->write_seq，ack_seq=tp->rcv_nxt。</p>

<pre><code>    tcp_connect_init(sk);  
    tp-&gt;snd_nxt = tp-&gt;write_seq;  
    ……  
    tcp_transmit_skb(sk, buff, 1, sk-&gt;sk_allocation);  
</code></pre>

<h5>收到服务端的SYN+ACK，发送ACK</h5>

<h5>tcp_rcv_synsent_state_process()</h5>

<p>此时已接收到对方的ACK，状态变迁到TCP_ESTABLISHED。最后发送对方SYN的ACK报文。</p>

<pre><code>    tcp_set_state(sk, TCP_ESTABLISHED);  
    tcp_send_ack(sk);  
</code></pre>

<h4>服务端流程</h4>

<h5>bind() -> inet_bind()</h5>

<p>  bind操作的主要作用是将创建的socket与给定的地址相绑定，这样创建的服务才能公开的让外部调用。当然对于socket服务器的创建来说，这一步不是必须的，在listen()时如果没有绑定地址，系统会选择一个随机可用地址作为服务器地址。</p>

<p>  一个socket地址分为ip和port，inet->inet_saddr赋值了传入的ip，snum是传入的port，对于端口，要检查它是否已被占用，这是由sk->sk_prot->get_port()完成的(这个函数前面已经分析过，在传入port时它检查是否被占用；传入port=0时它选择未用的端口)。如果没有被占用，inet->inet_sport被赋值port，因为是服务监听端，不需要远端地址，inet_daddr和inet_dport都置0。</p>

<p>  注意bind操作不会改变socket的状态，仍为创建时的TCP_CLOSE。</p>

<pre><code>    snum = ntohs(addr-&gt;sin_port);  
    ……  
    inet-&gt;inet_rcv_saddr = inet-&gt;inet_saddr = addr-&gt;sin_addr.s_addr;  
    if (sk-&gt;sk_prot-&gt;get_port(sk, snum)) {  
        inet-&gt;inet_saddr = inet-&gt;inet_rcv_saddr = 0;  
        err = -EADDRINUSE;  
        goto out_release_sock;  
    }  
    ……  
    inet-&gt;inet_sport = htons(inet-&gt;inet_num);  
    inet-&gt;inet_daddr = 0;  
    inet-&gt;inet_dport = 0;  
</code></pre>

<h5>listen() -> inet_listen()</h5>

<p>  listen操作开始服务器的监听，此时服务就可以接受到外部连接了。在开始监听前，要检查状态是否正确，sock->state==SS_UNCONNECTED确保仍是未连接的socket，sock->type==SOCK_STREAM确保是TCP协议，old_state确保此时状态是TCP_CLOSE或TCP_LISTEN，在其它状态下进行listen都是错误的。</p>

<pre><code>    if (sock-&gt;state != SS_UNCONNECTED || sock-&gt;type != SOCK_STREAM)  
        goto out;  
    old_state = sk-&gt;sk_state;  
    if (!((1 &lt;&lt; old_state) &amp; (TCPF_CLOSE | TCPF_LISTEN)))  
        goto out;  
</code></pre>

<p>  如果已是TCP_LISTEN态，则直接跳过，不用再执行listen了，而只是重新设置listen队列长度sk_max_ack_backlog，改变listen队列长也是多次执行listen的作用。如果还没有执行listen，则还要调用inet_csk_listen_start()开始监听。</p>

<p>  inet_csk_listen_start()变迁状态至TCP_LISTEN，分配监听队列，如果之前没有调用bind()绑定地址，则这里会分配一个随机地址。</p>

<pre><code>    if (old_state != TCP_LISTEN) {  
        err = inet_csk_listen_start(sk, backlog);  
        if (err)  
            goto out;  
    }  
    sk-&gt;sk_max_ack_backlog = backlog;  
</code></pre>

<h5>accept()</h5>

<p>accept() -> sys_accept4() -> inet_accept() -> inet_csk_accept()</p>

<p>  accept()实际要做的事件并不多，它的作用是返回一个已经建立连接的socket(即经过了三次握手)，这个过程是异步的，accept()并不亲自去处理三次握手过程，而只是监听icsk_accept_queue队列，当有socket经过了三次握手，它就会被加到icsk_accept_queue中，所以accept要做的就是等待队列中插入socket，然后被唤醒并返回这个socket。而三次握手的过程完全是协议栈本身去完成的。换句话说，协议栈相当于写者，将socket写入队列，accept()相当于读者，将socket从队列读出。这个过程从listen就已开始，所以即使不调用accept()，客户仍可以和服务器建立连接，但由于没有处理，队列很快会被占满。</p>

<pre><code>    if (reqsk_queue_empty(&amp;icsk-&gt;icsk_accept_queue)) {  
        long timeo = sock_rcvtimeo(sk, flags &amp; O_NONBLOCK);  
        ……  
        error = inet_csk_wait_for_connect(sk, timeo);  
        ……  
    }  

    newsk = reqsk_queue_get_child(&amp;icsk-&gt;icsk_accept_queue, sk);  
</code></pre>

<p>  协议栈向队列中加入socket的过程就是完成三次握手的过程，客户端通过向已知的listen fd发起连接请求，对于到来的每个连接，都会创建一个新的sock，当它经历了TCP_SYN_RCV -> TCP_ESTABLISHED后，就会被添加到icsk_accept_queue中，而监听的socket状态始终为TCP_LISTEN，保证连接的建立不会影响socket的接收。</p>

<h4>接收客户端发来的SYN，发送SYN+ACK</h4>

<h5>tcp_v4_do_rcv()</h5>

<p>  tcp_v4_do_rcv()是TCP模块接收的入口函数，客户端发起请求的对象是listen fd，所以sk->sk_state == TCP_LISTEN，调用tcp_v4_hnd_req()来检查是否处于半连接，只要三次握手没有完成，这样的连接就称为半连接，具体而言就是收到了SYN，但还没有收到ACK的连接，所以对于这个查找函数，如果是SYN报文，则会返回listen的socket(连接尚未创建)；如果是ACK报文，则会返回SYN报文处理中插入的半连接socket。其中存储这些半连接的数据结构是syn_table，它在listen()调用时被创建，大小由sys_ctl_max_syn_backlog和listen()传入的队列长度决定。</p>

<p>此时是收到SYN报文，tcp_v4_hnd_req()返回的仍是sk，调用tcp_rcv_state_process()来接收SYN报文，并发送SYN+ACK报文，同时向syn_table中插入一项表明此次连接的sk。</p>

<pre><code>    if (sk-&gt;sk_state == TCP_LISTEN) {  
        struct sock *nsk = tcp_v4_hnd_req(sk, skb);  
        if (!nsk)  
            goto discard;  
        if (nsk != sk) {  
            if (tcp_child_process(sk, nsk, skb)) {  
                rsk = nsk;  
                goto reset;  
            }  
            return 0;  
        }  
    }  
    TCP_CHECK_TIMER(sk);  
    if (tcp_rcv_state_process(sk, skb, tcp_hdr(skb), skb-&gt;len)) {  
        rsk = sk;  
        goto reset;  
    }  
</code></pre>

<p>  tcp_rcv_state_process()处理各个状态上socket的情况。下面是处于TCP_LISTEN的代码段，处于TCP_LISTEN的socket不会再向其它状态变迁，它负责监听，并在连接建立时创建新的socket。实际上，当收到第一个SYN报文时，会执行这段代码，conn_request() => tcp_v4_conn_request。</p>

<pre><code>    case TCP_LISTEN:  
    ……  
        if (th-&gt;syn) {  
            if (icsk-&gt;icsk_af_ops-&gt;conn_request(sk, skb) &lt; 0)  
                return 1;  
            kfree_skb(skb);  
            return 0;  
        }  
</code></pre>

<p>  tcp_v4_conn_request()中注意两个函数就可以了：tcp_v4_send_synack()向客户端发送了SYN+ACK报文，inet_csk_reqsk_queue_hash_add()将sk添加到了syn_table中，填充了该客户端相关的信息。这样，再次收到客户端的ACK报文时，就可以在syn_table中找到相应项了。</p>

<pre><code>    if (tcp_v4_send_synack(sk, dst, req, (struct request_values *)&amp;tmp_ext) || want_cookie)  
        goto drop_and_free;  
    inet_csk_reqsk_queue_hash_add(sk, req, TCP_TIMEOUT_INIT);  
</code></pre>

<h4>接收客户端发来的ACK</h4>

<h5>tcp_v4_do_rcv()</h5>

<p>  过程与收到SYN报文相同，不同点在于syn_table中已经插入了有关该连接的条目，tcp_v4_hnd_req()会返回一个新的sock: nsk，然后会调用tcp_child_process()来进行处理。在tcp_v4_hnd_req()中会创建新的sock，下面详细看下这个函数。</p>

<pre><code>    if (sk-&gt;sk_state == TCP_LISTEN) {  
        struct sock *nsk = tcp_v4_hnd_req(sk, skb);  
        if (!nsk)  
            goto discard;  
        if (nsk != sk) {  
            if (tcp_child_process(sk, nsk, skb)) {  
                rsk = nsk;  
                goto reset;  
            }  
            return 0;  
        }  
    }  
</code></pre>

<h5>tcp_v4_hnd_req()</h5>

<p>之前已经分析过，inet_csk_search_req()会在syn_table中找到req，此时进入tcp_check_req()</p>

<pre><code>    struct request_sock *req = inet_csk_search_req(sk, &amp;prev, th-&gt;source, iph-&gt;saddr, iph-&gt;daddr);  
    if (req)  
        return tcp_check_req(sk, skb, req, prev);  
</code></pre>

<h5>tcp_check_req()</h5>

<p>  syn_recv_sock() -> tcp_v4_syn_recv_sock()会创建一个新的sock并返回，创建的sock状态被直接设置为TCP_SYN_RECV，然后因为此时socket已经建立，将它添加到icsk_accept_queue中。</p>

<p>  状态TCP_SYN_RECV的设置可能比较奇怪，按照TCP的状态转移图，在服务端收到SYN报文后变迁为TCP_SYN_RECV，但看到在实现中收到ACK后才有了状态TCP_SYN_RECV，并且马上会变为TCP_ESTABLISHED，所以这个状态变得无足轻重。这样做的原因是listen和accept返回的socket是不同的，而只有真正连接建立时才会创建这个新的socket，在收到SYN报文时新的socket还没有建立，就无从谈状态变迁了。这里同样是一个平衡的存在，你也可以在收到SYN时创建一个新的socket，代价就是无用的socket大大增加了。</p>

<pre><code>    child = inet_csk(sk)-&gt;icsk_af_ops-&gt;syn_recv_sock(sk, skb, req, NULL);  
    if (child == NULL)  
        goto listen_overflow;  
    inet_csk_reqsk_queue_unlink(sk, req, prev);  
    inet_csk_reqsk_queue_removed(sk, req);  
    inet_csk_reqsk_queue_add(sk, req, child);  
</code></pre>

<h5>tcp_child_process()</h5>

<p>如果此时sock: child被用户进程锁住了，那么就先添加到backlog中__sk_add_backlog()，待解锁时再处理backlog上的sock；如果此时没有被锁住，则先调用tcp_rcv_state_process()进行处理，处理完后，如果child状态到达TCP_ESTABLISHED，则表明其已就绪，调用sk_data_ready()唤醒等待在isck_accept_queue上的函数accept()。</p>

<pre><code>    if (!sock_owned_by_user(child)) {  
        ret = tcp_rcv_state_process(child, skb, tcp_hdr(skb), skb-&gt;len);  
        if (state == TCP_SYN_RECV &amp;&amp; child-&gt;sk_state != state)  
            parent-&gt;sk_data_ready(parent, 0);  
    } else {  
        __sk_add_backlog(child, skb);  
    }  
</code></pre>

<p>  tcp_rcv_state_process()处理各个状态上socket的情况。下面是处于TCP_SYN_RECV的代码段，注意此时传入函数的sk已经是新创建的sock了(在tcp_v4_hnd_req()中)，并且状态是TCP_SYN_RECV，而不再是listen socket，在收到ACK后，sk状态变迁为TCP_ESTABLISHED，而在tcp_v4_hnd_req()中也已将sk插入到了icsk_accept_queue上，此时它就已经完全就绪了，回到tcp_child_process()便可执行sk_data_ready()。</p>

<pre><code>    case TCP_SYN_RECV:  
        if (acceptable) {  
            ……  
            tcp_set_state(sk, TCP_ESTABLISHED);  
            sk-&gt;sk_state_change(sk);  
            ……  
            tp-&gt;snd_una = TCP_SKB_CB(skb)-&gt;ack_seq;  
            tp-&gt;snd_wnd = ntohs(th-&gt;window) &lt;&lt; tp-&gt;rx_opt.snd_wscale;  
            tcp_init_wl(tp, TCP_SKB_CB(skb)-&gt;seq);   
            ……  
    }  
</code></pre>

<p>最后总结三次握手的过程</p>

<p><img src="/images/kernel/2015-06-01.png" alt="" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[字符设备驱动和等待队列样例]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/05/21/kernel-sched-waitqueue-sample/"/>
    <updated>2015-05-21T15:58:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/05/21/kernel-sched-waitqueue-sample</id>
    <content type="html"><![CDATA[<p>前两篇的样例</p>

<p><a href="/blog/2015/05/21/kernel-base-chardev/">字符设备驱动程序</a><br/>
<a href="/blog/2015/05/21/kernel-sched-waitqueue/">Linux内核中的等待队列</a></p>

<h4>waitqueue.c</h4>

<pre><code>    #include &lt;linux/module.h&gt;
    #include &lt;linux/init.h&gt;
    #include &lt;linux/fs.h&gt;
    #include &lt;asm/uaccess.h&gt;
    #include &lt;linux/wait.h&gt;
    #include &lt;linux/semaphore.h&gt;
    #include &lt;linux/kernel.h&gt;
    #include &lt;linux/proc_fs.h&gt;

    #include &lt;linux/socket.h&gt;
    #include &lt;linux/tcp.h&gt;
    #include &lt;linux/proc_fs.h&gt;
    #include &lt;net/net_namespace.h&gt;

    #include &lt;net/tcp.h&gt;


    static ssize_t globalvar_read(struct file *, char *, size_t, loff_t*);
    static ssize_t globalvar_write(struct file *, const char *, size_t, loff_t*);

    struct file_operations globalvar_fops =
    {
        .owner   = THIS_MODULE,
        .read = globalvar_read,
        .write = globalvar_write,
    };

    #define LEN 1024
    static char global_var[LEN];
    static int read_index = 0;
    static int write_index = 0;
    static spinlock_t var_lock;
    static wait_queue_head_t waitq;
    static int flag = 0;
    static int major;

    static const char procname[] = "testvar";

    static int __init globalvar_init(void)
    {
        init_waitqueue_head(&amp;waitq);
        spin_lock_init(&amp;var_lock);
    //  if (!proc_net_fops_create(&amp;init_net, procname, S_IRUSR, &amp;globalvar_fops)) {
        if (!(major = register_chrdev(0, "globalvar", &amp;globalvar_fops))) {
            printk("globalvar register failure\n");
            return -1;
        }
        printk("major = %d\n", major);
        return 0;
    }

    static void __exit globalvar_exit(void)
    {
    //  proc_net_remove(&amp;init_net, procname);
        unregister_chrdev(major, "globalvar");
    }

    static ssize_t globalvar_read(struct file *filp, char *buf, size_t len, loff_t *off)
    {
        int read_len;
        //等待数据可获得
        if (wait_event_interruptible(waitq, flag != 0))
            return -ERESTARTSYS;

        spin_lock(&amp;var_lock);
        read_len = write_index - read_index;
        if (copy_to_user(buf, global_var+read_index, read_len)) {
            spin_unlock(&amp;var_lock);
            return -EFAULT;
        }
        read_index = write_index;
        flag = 0;
        spin_unlock(&amp;var_lock);
        return read_len;
    }

    static ssize_t globalvar_write(struct file *filp, const char *buf, size_t len, loff_t *off)
    {
        spin_lock(&amp;var_lock);
        if (copy_from_user(global_var+write_index, buf, len)) {
            spin_unlock(&amp;var_lock);
            return -EFAULT;
        }
        write_index += len;
        spin_unlock(&amp;var_lock);

        flag = 1;
        //通知数据可获得
        wake_up_interruptible(&amp;waitq);
        return len;
    }

    module_init(globalvar_init);
    module_exit(globalvar_exit);
    MODULE_LICENSE("GPL");
</code></pre>

<h4>Makefile</h4>

<pre><code>    obj-m += waitqueue.o

    PWD = $(shell pwd)
    KERNEL := /lib/modules/`uname -r`/build

    all:
        make -C $(KERNEL) M=$(PWD) modules
</code></pre>

<h5>安装模块</h5>

<pre><code>    insmod ./waitqueue.ko
</code></pre>

<h5>查看对应的设备号</h5>

<pre><code>    $ cat /proc/devices | grep globalvar
    $ 249 globalvar
</code></pre>

<h5>建立文件</h5>

<pre><code>    mknod /dev/globalvar c 249 0
</code></pre>

<h5>终端1: cat文件</h5>

<pre><code>    cat /dev/globalvar
</code></pre>

<h5>终端2: echo数据到文件</h5>

<pre><code>    echo 123 &gt; /dev/globalvar
    echo 1234567 &gt; /dev/globalvar
    echo 123 &gt; /dev/globalvar
</code></pre>

<p>这时就能看见终端1读到了内容。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linux内核中的等待队列]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/05/21/kernel-sched-waitqueue/"/>
    <updated>2015-05-21T15:58:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/05/21/kernel-sched-waitqueue</id>
    <content type="html"><![CDATA[<p><a href="http://blog.sina.com.cn/s/blog_49d5604e010008bn.html">http://blog.sina.com.cn/s/blog_49d5604e010008bn.html</a></p>

<p>等待队列可以参考net/ipv4/tcp_probe.c的实现</p>

<p><a href="/blog/2015/05/21/kernel-sched-waitqueue-sample/">简单样例</a></p>

<h4>Linux内核中的等待队列</h4>

<p>  Linux内核的等待队列是以双循环链表为基础数据结构，与进程调度机制紧密结合，能够用于实现核心的异步事件通知机制。在Linux2.4.21中，等待队列在源代码树include/linux/wait.h中，这是一个通过list_head连接的典型双循环链表，</p>

<p>如下图所示。</p>

<p><img src="/images/kernel/2015-05-21.jpg" alt="" /></p>

<p>  在这个链表中，有两种数据结构：等待队列头（wait_queue_head_t）和等待队列项（wait_queue_t）。等待队列头和等待队列项中都包含一个list_head类型的域作为"连接件"。由于我们只需要对队列进行添加和删除操作，并不会修改其中的对象（等待队列项），因此，我们只需要提供一把保护整个基础设施和所有对象的锁，这把锁保存在等待队列头中，为wq_lock_t类型。在实现中，可以支持读写锁（rwlock）或自旋锁（spinlock）两种类型，通过一个宏定义来切换。如果使用读写锁，将wq_lock_t定义为rwlock_t类型；如果是自旋锁，将wq_lock_t定义为spinlock_t类型。无论哪种情况，分别相应设置wq_read_lock、wq_read_unlock、wq_read_lock_irqsave、wq_read_unlock_irqrestore、wq_write_lock_irq、wq_write_unlock、wq_write_lock_irqsave和wq_write_unlock_irqrestore等宏。</p>

<h5>等待队列头</h5>

<pre><code>    struct __wait_queue_head {
        wq_lock_t lock;
        struct list_head task_list;
    };
    typedef struct __wait_queue_head wait_queue_head_t;
</code></pre>

<p>  前面已经说过，等待队列的主体是进程，这反映在每个等待队列项中，是一个任务结构指针（struct task_struct * task）。flags为该进程的等待标志，当前只支持互斥。</p>

<h5>等待队列项</h5>

<pre><code>    struct __wait_queue {
        unsigned int flags;
    #define WQ_FLAG_EXCLUSIVE 0x01
        struct task_struct * task;
        struct list_head task_list;
    };
    typedef struct __wait_queue wait_queue_t;
</code></pre>

<h5>声明和初始化</h5>

<pre><code>    #define DECLARE_WAITQUEUE(name, tsk)            \
        wait_queue_t name = __WAITQUEUE_INITIALIZER(name, tsk)
    #define __WAITQUEUE_INITIALIZER(name, tsk) {    \
        task:  tsk,                                 \
        task_list: { NULL, NULL },                  \
        __WAITQUEUE_DEBUG_INIT(name)}
</code></pre>

<p>  通过DECLARE_WAITQUEUE宏将等待队列项初始化成对应的任务结构，并且用于连接的相关指针均设置为空。其中加入了调试相关代码。
<code>
    #define DECLARE_WAIT_QUEUE_HEAD(name)                    \
        wait_queue_head_t name = __WAIT_QUEUE_HEAD_INITIALIZER(name)
    #define __WAIT_QUEUE_HEAD_INITIALIZER(name) {            \
        lock:  WAITQUEUE_RW_LOCK_UNLOCKED,                   \
        task_list: { &amp;(name).task_list, &amp;(name).task_list }, \
        __WAITQUEUE_HEAD_DEBUG_INIT(name)}
</code></p>

<p>  通过DECLARE_WAIT_QUEUE_HEAD宏初始化一个等待队列头，使得其所在链表为空，并设置链表为"未上锁"状态。其中加入了调试相关代码。
<code>
    static inline void init_waitqueue_head(wait_queue_head_t *q)
</code></p>

<p>该函数初始化一个已经存在的等待队列头，它将整个队列设置为"未上锁"状态，并将链表指针prev和next指向它自身。
<code>
    {
        q-&gt;lock = WAITQUEUE_RW_LOCK_UNLOCKED;
        INIT_LIST_HEAD(&amp;q-&gt;task_list);
    }
    static inline void init_waitqueue_entry(wait_queue_t *q, struct task_struct *p)
</code></p>

<p>该函数初始化一个已经存在的等待队列项，它设置对应的任务结构，同时将标志位清0。
<code>
    {
        q-&gt;flags = 0;
        q-&gt;task = p;
    }
    static inline int waitqueue_active(wait_queue_head_t *q)
</code>
该函数检查等待队列是否为空。
<code>
    {
        return !list_empty(&amp;q-&gt;task_list);
    }
    static inline void __add_wait_queue(wait_queue_head_t *head, wait_queue_t *new)
</code></p>

<p>将指定的等待队列项new添加到等待队列头head所在的链表头部，该函数假设已经获得锁。
<code>
    {
        list_add(&amp;new-&gt;task_list, &amp;head-&gt;task_list);
    }
    static inline void __add_wait_queue_tail(wait_queue_head_t *head, wait_queue_t *new)
</code></p>

<p>将指定的等待队列项new添加到等待队列头head所在的链表尾部，该函数假设已经获得锁。
<code>
    {
        list_add_tail(&amp;new-&gt;task_list, &amp;head-&gt;task_list);
    }
    static inline void __remove_wait_queue(wait_queue_head_t *head, wait_queue_t *old)
</code>
将函数从等待队列头head所在的链表中删除指定等待队列项old，该函数假设已经获得锁，并且old在head所在链表中。
<code>
    {
        list_del(&amp;old-&gt;task_list);
    }
</code></p>

<h4>睡眠和唤醒操作</h4>

<p>对等待队列的操作包括睡眠和唤醒（相关函数保存在源代码树的/kernel/sched.c和include/linux/sched.h中）。思想是更改当前进程（CURRENT）的任务状态，并要求重新调度，因为这时这个进程的状态已经改变，不再在调度表的就绪队列中，因此无法再获得执行机会，进入"睡眠"状态，直至被"唤醒"，即其任务状态重新被修改回就绪态。</p>

<p>常用的睡眠操作有interruptible_sleep_on和sleep_on。两个函数类似，只不过前者将进程的状态从就绪态（TASK_RUNNING）设置为TASK_INTERRUPTIBLE，允许通过发送signal唤醒它（即可中断的睡眠状态）；而后者将进程的状态设置为TASK_UNINTERRUPTIBLE，在这种状态下，不接收任何singal。</p>

<p>以interruptible_sleep_on为例，其展开后的代码是：
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
<span class='line-number'>100</span>
<span class='line-number'>101</span>
<span class='line-number'>102</span>
<span class='line-number'>103</span>
<span class='line-number'>104</span>
<span class='line-number'>105</span>
<span class='line-number'>106</span>
<span class='line-number'>107</span>
<span class='line-number'>108</span>
<span class='line-number'>109</span>
<span class='line-number'>110</span>
<span class='line-number'>111</span>
<span class='line-number'>112</span>
<span class='line-number'>113</span>
<span class='line-number'>114</span>
<span class='line-number'>115</span>
<span class='line-number'>116</span>
<span class='line-number'>117</span>
<span class='line-number'>118</span>
<span class='line-number'>119</span>
<span class='line-number'>120</span>
<span class='line-number'>121</span>
<span class='line-number'>122</span>
<span class='line-number'>123</span>
<span class='line-number'>124</span>
<span class='line-number'>125</span>
<span class='line-number'>126</span>
<span class='line-number'>127</span>
<span class='line-number'>128</span>
<span class='line-number'>129</span>
<span class='line-number'>130</span>
<span class='line-number'>131</span>
<span class='line-number'>132</span>
<span class='line-number'>133</span>
<span class='line-number'>134</span>
<span class='line-number'>135</span>
<span class='line-number'>136</span>
<span class='line-number'>137</span>
<span class='line-number'>138</span>
<span class='line-number'>139</span>
<span class='line-number'>140</span>
<span class='line-number'>141</span>
<span class='line-number'>142</span>
<span class='line-number'>143</span>
<span class='line-number'>144</span>
<span class='line-number'>145</span>
<span class='line-number'>146</span>
<span class='line-number'>147</span>
<span class='line-number'>148</span>
<span class='line-number'>149</span>
<span class='line-number'>150</span>
<span class='line-number'>151</span>
<span class='line-number'>152</span>
<span class='line-number'>153</span>
<span class='line-number'>154</span>
<span class='line-number'>155</span>
<span class='line-number'>156</span>
<span class='line-number'>157</span>
<span class='line-number'>158</span>
<span class='line-number'>159</span>
<span class='line-number'>160</span>
<span class='line-number'>161</span>
<span class='line-number'>162</span>
<span class='line-number'>163</span>
<span class='line-number'>164</span>
<span class='line-number'>165</span>
<span class='line-number'>166</span>
<span class='line-number'>167</span>
<span class='line-number'>168</span>
<span class='line-number'>169</span>
<span class='line-number'>170</span>
<span class='line-number'>171</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>void interruptible_sleep_on(wait_queue_head_t &lt;em&gt;q)
</span><span class='line'>{
</span><span class='line'>    unsigned long flags;
</span><span class='line'>    wait_queue_t wait;
</span><span class='line'>    /&lt;/em&gt; 构造当前进程对应的等待队列项 */
</span><span class='line'>    init_waitqueue_entry(&amp;wait, current);&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    /* 将当前进程的状态从TASK_RUNNING改为TASK_INTERRUPTIBLE */
</span><span class='line'>current-&gt;state = TASK_INTERRUPTIBLE;
</span><span class='line'>
</span><span class='line'>/* 将等待队列项添加到指定链表中 */
</span><span class='line'>wq_write_lock_irqsave(&amp;q-&gt;lock,flags);
</span><span class='line'>__add_wait_queue(q, &amp;wait); 
</span><span class='line'>wq_write_unlock(&amp;q-&gt;lock);
</span><span class='line'>
</span><span class='line'>/* 进程重新调度，放弃执行权 */
</span><span class='line'>schedule();
</span><span class='line'>
</span><span class='line'>/* 本进程被唤醒，重新获得执行权，首要之事是将等待队列项从链表中删除 */
</span><span class='line'>wq_write_lock_irq(&amp;q-&gt;lock);
</span><span class='line'>__remove_wait_queue(q, &amp;wait);
</span><span class='line'>wq_write_unlock_irqrestore(&amp;q-&gt;lock,flags);
</span><span class='line'>/* 至此，等待过程结束，本进程可以正常执行下面的逻辑 */
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;
</span><span class='line'>对应的唤醒操作包括wake_up_interruptible和wake_up。wake_up函数不仅可以唤醒状态为TASK_UNINTERRUPTIBLE的进程，而且可以唤醒状态为TASK_INTERRUPTIBLE的进程。
</span><span class='line'>
</span><span class='line'>wake_up_interruptible只负责唤醒状态为TASK_INTERRUPTIBLE的进程。这两个宏的定义如下：
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;#define wake_up(x)   __wake_up((x),TASK_UNINTERRUPTIBLE | TASK_INTERRUPTIBLE, 1)
</span><span class='line'>#define wake_up_interruptible(x) __wake_up((x),TASK_INTERRUPTIBLE, 1)
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;
</span><span class='line'>__wake_up函数主要是获取队列操作的锁，具体工作是调用__wake_up_common完成的。
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;void __wake_up(wait_queue_head_t *q, unsigned int mode, int nr)
</span><span class='line'>{
</span><span class='line'>if (q) {
</span><span class='line'>    unsigned long flags;
</span><span class='line'>    wq_read_lock_irqsave(&amp;q-&gt;lock, flags);
</span><span class='line'>    __wake_up_common(q, mode, nr, 0);
</span><span class='line'>    wq_read_unlock_irqrestore(&amp;q-&gt;lock, flags);
</span><span class='line'>}
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;
</span><span class='line'>/* The core wakeup function.  Non-exclusive wakeups (nr_exclusive == 0) just wake everything up.  If it's an exclusive wakeup (nr_exclusive == small +ve number) then we wake all the non-exclusive tasks and one exclusive task.
</span><span class='line'>There are circumstances in which we can try to wake a task which has already started to run but is not in state TASK_RUNNING.  try_to_wake_up() returns zero in this (rare) case, and we handle it by contonuing to scan the queue. */
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;static inline void __wake_up_common (wait_queue_head_t *q, unsigned int mode, int nr_exclusive, const int sync)
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;
</span><span class='line'>参数q表示要操作的等待队列，mode表示要唤醒任务的状态，如TASK_UNINTERRUPTIBLE或TASK_INTERRUPTIBLE等。nr_exclusive是要唤醒的互斥进程数目，在这之前遇到的非互斥进程将被无条件唤醒。sync表示？？？
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;{
</span><span class='line'>struct list_head *tmp;
</span><span class='line'>struct task_struct *p;
</span><span class='line'>
</span><span class='line'>CHECK_MAGIC_WQHEAD(q);
</span><span class='line'>WQ_CHECK_LIST_HEAD(&amp;q-&gt;task_list);
</span><span class='line'>
</span><span class='line'>/* 遍历等待队列 */
</span><span class='line'>list_for_each(tmp,&amp;q-&gt;task_list) {
</span><span class='line'>    unsigned int state;
</span><span class='line'>    /* 获得当前等待队列项 */
</span><span class='line'>    wait_queue_t *curr = list_entry(tmp, wait_queue_t, task_list);
</span><span class='line'>
</span><span class='line'>    CHECK_MAGIC(curr-&gt;__magic);
</span><span class='line'>    /* 获得对应的进程 */
</span><span class='line'>    p = curr-&gt;task;
</span><span class='line'>    state = p-&gt;state;
</span><span class='line'>
</span><span class='line'>    /* 如果我们需要处理这种状态的进程 */
</span><span class='line'>    if (state &amp; mode) {
</span><span class='line'>        WQ_NOTE_WAKER(curr);
</span><span class='line'>        if (try_to_wake_up(p, sync) &amp;&amp; (curr-&gt;flags&amp;WQ_FLAG_EXCLUSIVE) &amp;&amp; !--nr_exclusive)
</span><span class='line'>            break;
</span><span class='line'>    }
</span><span class='line'>}
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;
</span><span class='line'>/* 唤醒一个进程，将它放到运行队列中，如果它还不在运行队列的话。"当前"进程总是在运行队列中的（except when the actual re-schedule is in progress)，and as such you're allowed to do the simpler "current-&gt;state = TASK_RUNNING" to mark yourself runnable without the overhead of this. */
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;static inline int try_to_wake_up(struct task_struct * p, int synchronous)
</span><span class='line'>{
</span><span class='line'>unsigned long flags;
</span><span class='line'>int success = 0;
</span><span class='line'>
</span><span class='line'>/* 由于我们需要操作运行队列，必须获得对应的锁 */
</span><span class='line'>spin_lock_irqsave(&amp;runqueue_lock, flags);
</span><span class='line'>/* 将进程状态设置为TASK_RUNNING */
</span><span class='line'>p-&gt;state = TASK_RUNNING;
</span><span class='line'>/* 如果进程已经在运行队列中，释放锁退出 */
</span><span class='line'>if (task_on_runqueue(p))
</span><span class='line'>    goto out;
</span><span class='line'>/* 否则将进程添加到运行队列中 */
</span><span class='line'>add_to_runqueue(p);
</span><span class='line'>
</span><span class='line'>/* 如果设置了同步标志 */
</span><span class='line'>if (!synchronous || !(p-&gt;cpus_allowed &amp; (1UL &lt;&lt; smp_processor_id())))
</span><span class='line'>    reschedule_idle(p);
</span><span class='line'>/* 唤醒成功，释放锁退出 */
</span><span class='line'>success = 1;
</span><span class='line'>out:
</span><span class='line'>spin_unlock_irqrestore(&amp;runqueue_lock, flags);
</span><span class='line'>return success;
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;
</span><span class='line'>#### 等待队列应用模式
</span><span class='line'>
</span><span class='line'>等待队列的的应用涉及两个进程，假设为A和B。A是资源的消费者，B是资源的生产者。A在消费的时候必须确保资源已经生产出来，为此定义一个资源等待队列。这个队列同时要被进程A和进程B使用，我们可以将它定义为一个全局变量。
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;DECLARE_WAIT_QUEUE_HEAD(rsc_queue); /* 全局变量 */
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;
</span><span class='line'>在进程A中，执行逻辑如下：
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;while (resource is unavaiable) {
</span><span class='line'>interruptible_sleep_on( &amp;wq );
</span><span class='line'>}
</span><span class='line'>consume_resource();
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;
</span><span class='line'>在进程B中，执行逻辑如下：
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;produce_resource();
</span><span class='line'>wake_up_interruptible( &amp;wq );
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;```&lt;/p&gt;
</span><span class='line'>]]&gt;&lt;/content&gt;
</span><span class='line'>  &lt;/entry&gt;
</span><span class='line'>  
</span><span class='line'>  &lt;entry&gt;
</span><span class='line'>&lt;title type="html"&gt;&lt;![CDATA[字符设备驱动程序]]&gt;&lt;/title&gt;
</span><span class='line'>&lt;link href="http://abcdxyzk.github.io/blog/2015/05/21/kernel-base-chardev/"/&gt;
</span><span class='line'>&lt;updated&gt;2015-05-21T15:58:00+08:00&lt;/updated&gt;
</span><span class='line'>&lt;id&gt;http://abcdxyzk.github.io/blog/2015/05/21/kernel-base-chardev&lt;/id&gt;
</span><span class='line'>&lt;content type="html"&gt;&lt;![CDATA[&lt;p&gt;&lt;a href="http://techlife.blog.51cto.com/212583/39225"&gt;http://techlife.blog.51cto.com/212583/39225&lt;/a&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;&lt;a href="/blog/2015/05/21/kernel-sched-waitqueue-sample/"&gt;简单样例&lt;/a&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;实现如下的功能:&lt;br/&gt;
</span><span class='line'>  -字符设备驱动程序的结构及驱动程序需要实现的系统调用&lt;br/&gt;
</span><span class='line'>  -可以使用cat命令或者自编的readtest命令读出"设备"里的内容&lt;br/&gt;
</span><span class='line'>  -以8139网卡为例，演示了I/O端口和I/O内存的使用&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;本文中的大部分内容在Linux Device Driver这本书中都可以找到，这本书是Linux驱动开发者的唯一圣经。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;hr /&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;先来看看整个驱动程序的入口，是char8139_init()这个函数，如果不指定MODULE_LICENSE(&ldquo;GPL&rdquo;), 在模块插入内核的时候会出错，因为将非"GPL"的模块插入内核就沾污了内核的"GPL"属性。</span></code></pre></td></tr></table></div></figure>
    module_init(char8139_init);
    module_exit(char8139_exit);</p>

<pre><code>MODULE_LICENSE("GPL");
MODULE_AUTHOR("ypixunil");
MODULE_DESCRIPTION("Wierd char device driver for Realtek 8139 NIC");
</code></pre>

<pre><code>
接着往下看char8139_init()
</code></pre>

<pre><code>static int __init char8139_init(void)
{
    int result;

    PDBG("hello. init.\n");

    /* register our char device */
    result = register_chrdev(char8139_major, "char8139", &amp;char8139_fops);
    if (result &lt; 0) {
        PDBG("Cannot allocate major device number!\n");
        return result;
    }
    /* register_chrdev() will assign a major device number and return if it called
     * with "major" parameter set to 0 */
    if(char8139_major == 0)
        char8139_major=result;

    /* allocate some kernel memory we need */
    buffer = (unsigned char*)(kmalloc(CHAR8139_BUFFER_SIZE, GFP_KERNEL));
    if (!buffer) {
        PDBG("Cannot allocate memory!\n");
        result = -ENOMEM;
        goto init_fail;
    }
    memset(buffer, 0, CHAR8139_BUFFER_SIZE);
    p_buf = buffer;

    return 0; /* everything's ok */

init_fail:
    char8139_exit();
    return result;
}
</code></pre>

<pre><code>
这个函数首先的工作就是使用register_chrdev()注册我们的设备的主设备号和系统调用。系统调用对于字符设备驱动程序来说就是file_operations接口。

我们先来看看char8139_major的定义，
</code></pre>

<pre><code>#define DEFAULT_MAJOR 145         /* data structure used by our driver */
int char8139_major=DEFAULT_MAJOR; /* major device number. if initial value is 0,
                                   * the kernel will dynamically assign a major device
                                   * number in register_chrdev() */
</code></pre>

<pre><code>
这里我们指定我们的设备的主设备号是145,你必须找到一个系统中没有用的主设备号，可以通过"cat /proc/devices"命令来查看系统中已经使用的主设备号。
</code></pre>

<pre><code>[michael@char8139]$ cat /proc/devices
Character devices:
1 mem
2 pty
3 ttyp
4 ttyS
5 cua
7 vcs
10 misc
14 sound
116 alsa
128 ptm
136 pts
162 raw
180 usb
195 nvidia
226 drm

Block devices:
2 fd
3 ide0
22 ide1
[michael@char8139]$
</code></pre>

<pre><code>
可见在我的系统中，145还没有被使用。

指定主设备号值得考虑。像上面这样指定一个主设备号显然缺乏灵活性，而且不能保证一个驱动程序在所有的机器上都能用。可以在调用register_chrdev()时将第一个参数，即主设备号指定为0,这样register_chrdev()会分配一个空闲的主设备号作为返回值。 但是这样也有问题，我们只有在将模块插入内核之后才能得到我们设备的主设备号(使用 "cat /proc/devices")，但是要操作设备需要在系统/dev目录下建立设备结点，而建立结点时要指定主设备号。当然，你可以写一个脚本来自动完成这些事情。

总之，作为一个演示，我们还是指定主设备号为145，这样我们可以在/dev/目录下建立几个设备节点。
</code></pre>

<pre><code>[root@char8139]$ mknod /dev/char8139_0 c 145 0
[root@char8139]$ mknod /dev/char8139_0 c 145 17
[root@char8139]$ mknod /dev/char8139_0 c 145 36
[root@char8139]$ mknod /dev/char8139_0 c 145 145
</code></pre>

<pre><code>
看一下我们建立的节点
</code></pre>

<pre><code>[michael@char8139]$ ll /dev/char8139*
crw-r--r-- 1 root root 145, 0 2004-12-26 20:33 /dev/char8139_0
crw-r--r-- 1 root root 145, 17 2004-12-26 20:34 /dev/char8139_1
crw-r--r-- 1 root root 145, 36 2004-12-26 20:34 /dev/char8139_2
crw-r--r-- 1 root root 145, 145 2004-12-26 20:34 /dev/char8139_3
[michael@char8139]$
</code></pre>

<pre><code>
我们建立了四个节点，使用了四个次设备号，后面我们会说明次设备号的作用。


再来看看我们的file_operations的定义。这里其实只实现了read()，open()，release()三个系统调用，ioctl()只是简单返回。更有write()等函数甚至根本没有声明，没有声明的函数系统可能会调用默认的操作。
</code></pre>

<pre><code>struct file_operations char8139_fops =
{
    owner: THIS_MODULE,
    read: char8139_read,
    ioctl: char8139_ioctl,
    open: char8139_open,
    release: char8139_release,
};
</code></pre>

<pre><code>
file_operations是每个字符设备驱动程序必须实现的系统调用，当用户对/dev中我们的设备对应结点进行操作时，linux就会调用我们驱动程序中提供的系统调用。比如用户敲入"cat /dev/char8139_0"命令，想想cat这个应用程序的实现，首先它肯定调用C语言库里的open()函数去打开/dev/char8139_0这个文件，到了系统这一层，系统会看到/dev/char8139_0不是普通磁盘文件，而是一个代表字符设备的节点，所以系统会根据/dev/char8139_0的主设备号来查找是不是已经有驱动程序使用这个相同的主设备号进行了注册，如果有，就调用驱动程序的open()实现。

为什么要这样干？因为要提供抽象，提供统一的接口，别忘了操作系统的作用之一就是这个。因为我们的设备提供的统一的接口，所以cat这个应用程序使用一般的文件操作就能从我们的设备中读出数据，
而且more, less这些应用程序都能从我们的设备中读出数据。

现在来看看我们的设备
</code></pre>

<pre><code>#define CHAR8139_BUFFER_SIZE 2000
unsigned char *buffer=NULL; /* driver data buffer */
unsigned char *p_buf;
unsigned int data_size=0;
</code></pre>

<pre><code>我们的设备很简单，一个2000字节的缓冲区， data_size指定缓冲区中有效数据的字节数。我们的设备只支持读不支持写。我们在char8139_init()中为缓冲区分配空间。

char8139_exit()里面的操作就是char8139_init()里面操作的反向操作。

现在我们来看看，假如用户调用了"cat /dev/char8139_3"这个命令会发生什么事情。

根据前面的介绍，我们驱动程序中的open()函数会被调用。
</code></pre>

<pre><code>int char8139_open(struct inode *node, struct file *flip)
{
    int type = MINOR(node-&gt;i_rdev)&gt;&gt;4;
    int num = MINOR(node-&gt;i_rdev) &amp; 0x0F;

    /* put some char in buffer to reflect the minor device number */
    *buffer=(unsigned char)('0');
    *(buffer+1)=(unsigned char)('x');
    *(buffer+2)=(unsigned char)('0'+type);
    *(buffer+3)=(unsigned char)('0'+num);
    *(buffer+4)=(unsigned char)('\n');
    data_size+=5;

    PDBG("Ok. Find treasure! 8139 I/O port base: %x\n", detect_8139_io_port());
    PDBG("OK. Find treasure! 8139 I/O memory base address: %lx\n",
    detect_8139_io_mem());

    MOD_INC_USE_COUNT;

    return 0;
}
</code></pre>

<pre><code>
这里演示了次设备号的作用，它让我们知道用户操作的是哪一个"次设备"，是/dev/char8139_0还是/dev/char8139_3，因为对不同的"次设备"，具体的操作方法可能是不一样的，这样就为一个驱动程序控制多个类似的设备提供了可能。

我们根据次设备号的不同，在buffer中填入不同的字符(次设备号的16进制表示)。

接着驱动程序中的read()函数会被调用，因为cat程序的实现就是读取文件中的内容。
</code></pre>

<pre><code>ssize_t char8139_read (struct file *filp, char *buf, size_t count, loff_t *f_pos)
{
    ssize_t ret=0;

    PDBG("copy to user. count=%d, f_pos=%ld\n", (int)count, (long)*f_pos);
    if (*f_pos&gt;= data_size)
        return ret;
    if (*f_pos + count &gt; data_size)
        count = data_size-*f_pos;
    if (copy_to_user(buf, p_buf, count))
    {
        PDBG("OOps, copy to user error.\n");
        return -EFAULT;
    }

    p_buf += count;
    *f_pos += count;
    ret = count;

    return ret;
}
</code></pre>

<p>```</p>

<p>要正确的实现一个read()调用，你得想一想一个应用程序是如何调用read()从文件中读取数据的。如果你想明白了就很简单，驱动程序所要做的就是把恰当的数据传递给应用程序，这是使用copy_to_user()函数完成的。</p>

<p>另外，我们必须得意识到，这里只是一个很简单的演示。还有很多复杂的问题有待考虑，比如两个应用程序可能同时打开我们设备，我们的设备应该怎样反应(这取决于具体的设备应有的行为)，还有互斥的问题。</p>

<p>然后我们看看I/O端口和I/O内存的操作。这里使用8139网卡作为一个硬件实例来演示I/O端口和I/O内存的操作。没有什么特别的，都是标准的步骤。在使用时需要注意，如果你的系统中已经有8139网卡的驱动程序，必须先关掉网络设备，卸载驱动，然后再使用本驱动程序。</p>

<p>使用程序包的步骤：(在我的Debian系统上如此，你的可能不同)<br/>
1. 解压<br/>
2. 编译(/usr/src/linux处必须要有内核源代码)<br/>
3. ifconfig eth0 down 关掉网络设备<br/>
rmmod 8139too 卸载原来的8139网卡驱动<br/>
insmod char8139.o 插入我们的模块<br/>
(insmod会出错， 如果你现在运行的linux版本不是你编译本驱动程序时使用的内核源代码的版本，insmod时会报告模块版本与内核版本不一致。这时，你得看看内核源代码中/include/linux/version.h文件，这个文件中的UTS_RELEASE定义了内核的版本号，你可以在驱动程序中预先定义这个宏为当前运行的内核的版本号，这样就能避免上述错误。)<br/>
4. mknode(见本文前述)<br/>
5. 试试我们的设备<br/>
./readtest<br/>
或者<br/>
cat /dev/char8139_0或<br/>
cat /dev/char8139_1或<br/>
cat /dev/char8139_2或<br/>
cat /dev/char8139_3<br/>
6. 恢复系统<br/>
rmmod char8139<br/>
modprobe 8139too<br/>
ifconfig eth0 up<br/>
如果你使用dhcp可能还需要运行dhclient</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ipv6初始化和处理流程分析]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/05/15/kernel-net-ipv6/"/>
    <updated>2015-05-15T15:57:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/05/15/kernel-net-ipv6</id>
    <content type="html"><![CDATA[<p><a href="/download/kernel/ipv6%E5%88%9D%E5%A7%8B%E5%8C%96%E5%92%8C%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90.pdf">ipv6初始化和处理流程分析.pdf</a></p>
]]></content>
  </entry>
  
</feed>
