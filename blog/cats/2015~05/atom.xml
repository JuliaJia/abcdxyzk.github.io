<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 2015~05 | kk Blog —— 通用基础]]></title>
  <link href="http://abcdxyzk.github.io/blog/cats/2015~05/atom.xml" rel="self"/>
  <link href="http://abcdxyzk.github.io/"/>
  <updated>2015-05-13T14:20:30+08:00</updated>
  <id>http://abcdxyzk.github.io/</id>
  <author>
    <name><![CDATA[kk]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[tcp三个接收队列]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/05/11/kernel-net-tcp_queue/"/>
    <updated>2015-05-11T15:46:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/05/11/kernel-net-tcp_queue</id>
    <content type="html"><![CDATA[<p><a href="http://www.cnblogs.com/alreadyskb/p/4386565.html">http://www.cnblogs.com/alreadyskb/p/4386565.html</a></p>

<h4>三个接收队列</h4>

<ul>
<li>tcp协议栈数据接收实现了三个接收缓存分别是prequeue、sk_write_queue、sk_backlog。</li>
</ul>


<p>之所以需要三个接收缓存的原因如下：<br/>
tcp协议栈接收到数据包时struct sock *sk 可能被进程下上文或者中断上下文占用：</p>

<p>  1、如果处于进程上下文sk_lock.owned=1，软中断因为sk_lock.owned=1，所以数据只能暂存在后备队列中（backlog），当进程上下文逻辑处理完成后会回调tcp_v4_do_rcv处理backlog队列作为补偿，具体看tcp_sendmsg 函数 release_sock的实现。</p>

<p>  2、如果当前处于中断上下文，sk_lock.owned=0，那么数据可能被放置到receive_queue或者prequeue，数据优先放置到prequeue中，如果prequeue满了则会放置到receive_queue中，理论上这里有一个队列就行了，但是TCP协议栈为什么要设计两个呢？其实是为了快点结束软中断数据处理流程，软中断处理函数中禁止了进程抢占和其他软中断发生，效率应该是很低下的，如果数据被放置到prequeue中，那么软中断流程很快就结束了，如果放置到receive_queue那么会有很复杂的逻辑需要处理。receive_queue队列的处理在软中断中，prequeue队列的处理则是在进程上下文中。总的来说就是为了提高TCP协议栈的效率。</p>

<h4>后备队列的处理逻辑</h4>

<h5>1、什么时候使用后备队列</h5>

<p>tcp协议栈对struct sock <em>sk有两把锁，第一把是sk_lock.slock，第二把则是sk_lock.owned。sk_lock.slock用于获取struct sock </em>sk对象的成员的修改权限；sk_lock.owned用于区分当前是进程上下文或是软中断上下文，为进程上下文时sk_lock.owned会被置1，中断上下文为0。</p>

<p>如果是要对sk修改，首先是必须拿锁sk_lock.slock，其后是判断当前是软中断或是进程上下文，如果是进程上下文，那么接收到的skb则只能先放置到后备队列中sk_backlog中。如果是软中断上下文则可以放置到prequeue和sk_write_queue中。</p>

<p>代码片段如下：
<code>
        bh_lock_sock_nested(sk);               // 获取第一把锁。
        ret = 0;
        if (!sock_owned_by_user(sk)) {         // 判断第二把锁，区分是处于进程上下文还是软中断上下文。
    #ifdef CONFIG_NET_DMA
            struct tcp_sock *tp = tcp_sk(sk);
            if (!tp-&gt;ucopy.dma_chan &amp;&amp; tp-&gt;ucopy.pinned_list)
                tp-&gt;ucopy.dma_chan = dma_find_channel(DMA_MEMCPY);
            if (tp-&gt;ucopy.dma_chan)
                ret = tcp_v4_do_rcv(sk, skb);
            else
    #endif
            {
                if (!tcp_prequeue(sk, skb))    // 如果处于中断上下文，则优先放置到prequeue中，如果prequeue满则放置到sk_write_queue中。
                    ret = tcp_v4_do_rcv(sk, skb);
            }
        } else if (unlikely(sk_add_backlog(sk, skb,  // 如果是处于进程上下文则直接放置到后备队列中(sk_backlog中)。
                            sk-&gt;sk_rcvbuf + sk-&gt;sk_sndbuf))) {
            bh_unlock_sock(sk);
            NET_INC_STATS_BH(net, LINUX_MIB_TCPBACKLOGDROP);
            goto discard_and_relse;
        }
        bh_unlock_sock(sk);
</code></p>

<h5>2、skb怎么add到sk_backlog中</h5>

<p>sk_add_backlog函数用于add sbk到sk_backlog中，所以下面我们分析次函数。
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
<span class='line-number'>100</span>
<span class='line-number'>101</span>
<span class='line-number'>102</span>
<span class='line-number'>103</span>
<span class='line-number'>104</span>
<span class='line-number'>105</span>
<span class='line-number'>106</span>
<span class='line-number'>107</span>
<span class='line-number'>108</span>
<span class='line-number'>109</span>
<span class='line-number'>110</span>
<span class='line-number'>111</span>
<span class='line-number'>112</span>
<span class='line-number'>113</span>
<span class='line-number'>114</span>
<span class='line-number'>115</span>
<span class='line-number'>116</span>
<span class='line-number'>117</span>
<span class='line-number'>118</span>
<span class='line-number'>119</span>
<span class='line-number'>120</span>
<span class='line-number'>121</span>
<span class='line-number'>122</span>
<span class='line-number'>123</span>
<span class='line-number'>124</span>
<span class='line-number'>125</span>
<span class='line-number'>126</span>
<span class='line-number'>127</span>
<span class='line-number'>128</span>
<span class='line-number'>129</span>
<span class='line-number'>130</span>
<span class='line-number'>131</span>
<span class='line-number'>132</span>
<span class='line-number'>133</span>
<span class='line-number'>134</span>
<span class='line-number'>135</span>
<span class='line-number'>136</span>
<span class='line-number'>137</span>
<span class='line-number'>138</span>
<span class='line-number'>139</span>
<span class='line-number'>140</span>
<span class='line-number'>141</span>
<span class='line-number'>142</span>
<span class='line-number'>143</span>
<span class='line-number'>144</span>
<span class='line-number'>145</span>
<span class='line-number'>146</span>
<span class='line-number'>147</span>
<span class='line-number'>148</span>
<span class='line-number'>149</span>
<span class='line-number'>150</span>
<span class='line-number'>151</span>
<span class='line-number'>152</span>
<span class='line-number'>153</span>
<span class='line-number'>154</span>
<span class='line-number'>155</span>
<span class='line-number'>156</span>
<span class='line-number'>157</span>
<span class='line-number'>158</span>
<span class='line-number'>159</span>
<span class='line-number'>160</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>/&lt;em&gt; The per-socket spinlock must be held here. &lt;/em&gt;/
</span><span class='line'>static inline __must_check int sk_add_backlog(struct sock &lt;em&gt;sk, struct sk_buff &lt;/em&gt;skb,
</span><span class='line'>                           unsigned int limit)
</span><span class='line'>{
</span><span class='line'>    if (sk_rcvqueues_full(sk, skb, limit))  // 判断接收缓存是否已经用完了，很明显sk_backlog的缓存大小也算在了总接收缓存中。
</span><span class='line'>        return -ENOBUFS;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    __sk_add_backlog(sk, skb);              // 将skb添加到sk_backlog队列中。
</span><span class='line'>sk_extended(sk)-&gt;sk_backlog.len += skb-&gt;truesize;  // 更新sk_backlog中已经挂载的数据量。
</span><span class='line'>return 0;
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;/* OOB backlog add */
</span><span class='line'>static inline void __sk_add_backlog(struct sock *sk, struct sk_buff *skb)
</span><span class='line'>{
</span><span class='line'>if (!sk-&gt;sk_backlog.tail) {   // 如果当前sk_backlog为NULL，此时head和tail都指向skb。
</span><span class='line'>    sk-&gt;sk_backlog.head = sk-&gt;sk_backlog.tail = skb;
</span><span class='line'>} else {                      // 分支表示sk_backlog中已经有数据了，那么skb直接挂在tail的尾部，之后tail指针后移到skb。
</span><span class='line'>    sk-&gt;sk_backlog.tail-&gt;next = skb;
</span><span class='line'>    sk-&gt;sk_backlog.tail = skb;
</span><span class='line'>}
</span><span class='line'>skb-&gt;next = NULL;             // 这种很重要，在sk_backlog处理时会用来判断skb是否处理完毕。
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;
</span><span class='line'>##### 3、sk_backlog中skb的处理
</span><span class='line'>
</span><span class='line'>很明显sk_backlog的处理必然中进程上下文进行，对于数据接收，进程上下文的接口是tcp_recvmmsg，所以sk_backlog肯定要在tcp_recvmmsg中处理。
</span><span class='line'>
</span><span class='line'>tcp_recvmmsg sk_backlog的代码处理片段如下：
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;tcp_cleanup_rbuf(sk, copied);
</span><span class='line'>TCP_CHECK_TIMER(sk);
</span><span class='line'>release_sock(sk);
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;
</span><span class='line'>release_sock(sk)涉及到sk_backlog处理。
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;void release_sock(struct sock *sk)
</span><span class='line'>{
</span><span class='line'>/*
</span><span class='line'>* The sk_lock has mutex_unlock() semantics:
</span><span class='line'>*/
</span><span class='line'>mutex_release(&amp;sk-&gt;sk_lock.dep_map, 1, _RET_IP_);
</span><span class='line'>
</span><span class='line'>spin_lock_bh(&amp;sk-&gt;sk_lock.slock);   // 获取第一把锁。
</span><span class='line'>if (sk-&gt;sk_backlog.tail)            // 如果后备队列不为NULL，则开始处理。
</span><span class='line'>    __release_sock(sk);
</span><span class='line'>
</span><span class='line'>if (proto_has_rhel_ext(sk-&gt;sk_prot, RHEL_PROTO_HAS_RELEASE_CB) &amp;&amp;
</span><span class='line'>        sk-&gt;sk_prot-&gt;release_cb)
</span><span class='line'>    sk-&gt;sk_prot-&gt;release_cb(sk);
</span><span class='line'>
</span><span class='line'>sk-&gt;sk_lock.owned = 0;              // 进成上下文skb处理完了，释放第二把锁。
</span><span class='line'>if (waitqueue_active(&amp;sk-&gt;sk_lock.wq))
</span><span class='line'>    wake_up(&amp;sk-&gt;sk_lock.wq);
</span><span class='line'>spin_unlock_bh(&amp;sk-&gt;sk_lock.slock); // 释放第一把锁。
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;
</span><span class='line'>`__release_sock(sk)`是后备队列的真正处理函数。
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;static void __release_sock(struct sock *sk)
</span><span class='line'>{
</span><span class='line'>struct sk_buff *skb = sk-&gt;sk_backlog.head;
</span><span class='line'>
</span><span class='line'>do {
</span><span class='line'>    sk-&gt;sk_backlog.head = sk-&gt;sk_backlog.tail = NULL;
</span><span class='line'>    bh_unlock_sock(sk);
</span><span class='line'>
</span><span class='line'>    do {
</span><span class='line'>        struct sk_buff *next = skb-&gt;next;
</span><span class='line'>
</span><span class='line'>        skb-&gt;next = NULL;
</span><span class='line'>        sk_backlog_rcv(sk, skb);    // skb的处理函数，其实调用的是tcp_v4_do_rcv函数。
</span><span class='line'>
</span><span class='line'>        /*
</span><span class='line'>         * We are in process context here with softirqs
</span><span class='line'>         * disabled, use cond_resched_softirq() to preempt.
</span><span class='line'>         * This is safe to do because we've taken the backlog
</span><span class='line'>         * queue private:
</span><span class='line'>         */
</span><span class='line'>        cond_resched_softirq();
</span><span class='line'>
</span><span class='line'>        skb = next;
</span><span class='line'>    } while (skb != NULL);          // 如果skb=NULL，那么说明之前的sk_backlog已经处理完了。
</span><span class='line'>
</span><span class='line'>    bh_lock_sock(sk);
</span><span class='line'>} while ((skb = sk-&gt;sk_backlog.head) != NULL); // 在处理上一个sk_backlog时，可能被软中断中断了，建立了新的sk_backlog，新建立的sk_backlog也将一并被处理。
</span><span class='line'>
</span><span class='line'>/*
</span><span class='line'>* Doing the zeroing here guarantee we can not loop forever
</span><span class='line'>* while a wild producer attempts to flood us.
</span><span class='line'>*/
</span><span class='line'>sk_extended(sk)-&gt;sk_backlog.len = 0;
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;&lt;code&gt;``
</span><span class='line'>  一开始重置sk-&gt;sk_backlog.head ，sk-&gt;sk_backlog.tail为NULL。sk_backlog是一个双链表，head指向了链表头部的skb，而tail则指向了链表尾部的skb。这里之所以置NULL head 和tail，是因为struct sk_buff *skb = sk-&gt;sk_backlog.head 提前取到了head指向的skb，之后就可以通过skb-&gt;next来获取下一个skb处理，结束的条件是skb-&gt;next=NULL，这个是在&lt;/code&gt;__sk_add_backlog`函数中置位的，也就说对于sk_backlog的处理head和tail指针已经没有用了。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  为什么要置NULLsk-&gt;sk_backlog.head ，sk-&gt;sk_backlog.tail呢？第一想法是它可能要被重新使用了。那么在什么情况下会被重新使用呢？试想一下当前是在进程上下文，并且sk-&gt;sk_lock.slock没有被锁住，那是不是可能被软中断打断呢？如果被软中断打断了是不是要接收数据呢，tcp协议栈为了效率考虑肯定是要接收数据的，前面分析道这种情况的数据必须放置到后备队列中(sk_backlog)，所以可以肯定置NULL sk-&gt;sk_backlog.head ，sk-&gt;sk_backlog.tail是为了在处理上一个sk_backlog时，能重用sk_backlog，建立一条新的sk_backlog，或许有人会问为什么不直接添加到原先的sk_backlog tail末尾呢？这个问题我也没有想太清楚，或许是同步不好做吧。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h5&gt;4、skb被处理到哪去了&lt;/h5&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  很明显接收的数据最终都将被传递到应用层，在传递到应用层前必须要保证三个接收队列中的数据有序，那么这三个队列是怎么保证数据字节流有序的被递交给应用层呢？三个队列都会调用tcp_v4_do_rcv函数，prequeue和sk_backlog是在tcp_recvmsg中调用tcp_v4_do_rcv函数，也就是进程上下文中调用tcp_v4_do_rcv函数，但会local_bh_disable禁止软中断。如果在tcp_rcv_established, tcp_data_queue中如果刚好数据可以直接copy到用户空间，又会短暂开始软中断local_bh_enable。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  但在tcp_checksum_complete_user、tcp_rcv_established、tcp_data_queue函数中开启软中断将来容易出问题，进入软中断:softirq()+=1; local_bh_enable:softirq()-=2; 所以现在只是软中断中softirq()统计不准，进程中还是准的。但如果以后在软中断中在local_bh_enable之前给softirq()+=1了，那么就会导致软中断被打断，导致软中断执行途中被切走而且永远切不回来。tcp_checksum_complete_user被切走导致收包不成功，tcp_rcv_established、tcp_data_queue函数中如果在tp-&gt;copied_seq+=chunk后被切走就会导致tp-&gt;copied_seq&gt;tp-&gt;rcv_nxt，那么下次收包后就有可能出现tp-&gt;copied_seq &gt; sk_write_queue.first.end_seq, 等异常。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  如果仔细分析tcp_v4_do_rcv函数能发现，这个函数能保证数据有序的排列在一起，所以无论是在处理sk_backlog还是prequeue，最终都会调用tcp_v4_do_rcv函数将数据有效地插入到sk_write_queue中，最后被应用层取走。&lt;/p&gt;
</span><span class='line'>]]&gt;&lt;/content&gt;
</span><span class='line'>  &lt;/entry&gt;
</span><span class='line'>  
</span><span class='line'>  &lt;entry&gt;
</span><span class='line'>&lt;title type="html"&gt;&lt;![CDATA[tcp_read_sock BUG]]&gt;&lt;/title&gt;
</span><span class='line'>&lt;link href="http://abcdxyzk.github.io/blog/2015/05/11/debug-mark-tcp_read_sock_bug/"/&gt;
</span><span class='line'>&lt;updated&gt;2015-05-11T10:17:00+08:00&lt;/updated&gt;
</span><span class='line'>&lt;id&gt;http://abcdxyzk.github.io/blog/2015/05/11/debug-mark-tcp_read_sock_bug&lt;/id&gt;
</span><span class='line'>&lt;content type="html"&gt;&lt;![CDATA[&lt;pre&gt;&lt;code&gt;commit baff42ab1494528907bf4d5870359e31711746ae
</span><span class='line'>Author: Steven J. Magnani &lt;steve@digidescorp.com&gt;
</span><span class='line'>Date:   Tue Mar 30 13:56:01 2010 -0700
</span><span class='line'>
</span><span class='line'>net: Fix oops from tcp_collapse() when using splice()
</span><span class='line'>
</span><span class='line'>tcp_read_sock() can have a eat skbs without immediately advancing copied_seq.
</span><span class='line'>This can cause a panic in tcp_collapse() if it is called as a result
</span><span class='line'>of the recv_actor dropping the socket lock.
</span><span class='line'>
</span><span class='line'>A userspace program that splices data from a socket to either another
</span><span class='line'>socket or to a file can trigger this bug.
</span><span class='line'>
</span><span class='line'>Signed-off-by: Steven J. Magnani &lt;steve@digidescorp.com&gt;
</span><span class='line'>Signed-off-by: David S. Miller &lt;davem@davemloft.net&gt;
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    diff --git a/net/ipv4/tcp.c b/net/ipv4/tcp.c
</span><span class='line'>index 6afb6d8..2c75f89 100644
</span><span class='line'>--- a/net/ipv4/tcp.c
</span><span class='line'>+++ b/net/ipv4/tcp.c
</span><span class='line'>@@ -1368,6 +1368,7 @@ int tcp_read_sock(struct sock *sk, read_descriptor_t *desc,
</span><span class='line'>        sk_eat_skb(sk, skb, 0);
</span><span class='line'>        if (!desc-&gt;count)
</span><span class='line'>            break;
</span><span class='line'>+       tp-&gt;copied_seq = seq;
</span><span class='line'>    }
</span><span class='line'>    tp-&gt;copied_seq = seq;
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;如果在tcp_read_sock中sk_eat_skb时copied_seq没有及时一起修改的话，就会出现copied_seq小于sk_write_queue队列第一个包的seq。&lt;br/&gt;
</span><span class='line'>tcp_read_sock的recv_actor指向的函数(比如tcp_splice_data_recv)是有可能释放sk锁的，如果这时进入收包软中断且内存紧张调用tcp_collapse，&lt;br/&gt;
</span><span class='line'>tcp_collapse中：&lt;br/&gt;</span></code></pre></td></tr></table></div></figure>
    start = copied_seq
    &hellip;
    int offset = start - TCP_SKB_CB(skb)->seq;</p>

<pre><code>BUG_ON(offset &lt; 0);
</code></pre>

<p>```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[tcp_match_skb_to_sack BUG]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/05/11/debug-mark-tcp_match_skb_to_sack_bug/"/>
    <updated>2015-05-11T10:09:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/05/11/debug-mark-tcp_match_skb_to_sack_bug</id>
    <content type="html"><![CDATA[<pre><code>commit 2cd0d743b05e87445c54ca124a9916f22f16742e
Author: Neal Cardwell &lt;ncardwell@google.com&gt;
Date:   Wed Jun 18 21:15:03 2014 -0400

    tcp: fix tcp_match_skb_to_sack() for unaligned SACK at end of an skb

    If there is an MSS change (or misbehaving receiver) that causes a SACK
    to arrive that covers the end of an skb but is less than one MSS, then
    tcp_match_skb_to_sack() was rounding up pkt_len to the full length of
    the skb ("Round if necessary..."), then chopping all bytes off the skb
    and creating a zero-byte skb in the write queue.

    This was visible now because the recently simplified TLP logic in
    bef1909ee3ed1c ("tcp: fixing TLP's FIN recovery") could find that 0-byte
    skb at the end of the write queue, and now that we do not check that
    skb's length we could send it as a TLP probe.

    Consider the following example scenario:

     mss: 1000
     skb: seq: 0 end_seq: 4000  len: 4000
     SACK: start_seq: 3999 end_seq: 4000

    The tcp_match_skb_to_sack() code will compute:

     in_sack = false
     pkt_len = start_seq - TCP_SKB_CB(skb)-&gt;seq = 3999 - 0 = 3999
     new_len = (pkt_len / mss) * mss = (3999/1000)*1000 = 3000
     new_len += mss = 4000

    Previously we would find the new_len &gt; skb-&gt;len check failing, so we
    would fall through and set pkt_len = new_len = 4000 and chop off
    pkt_len of 4000 from the 4000-byte skb, leaving a 0-byte segment
    afterward in the write queue.

    With this new commit, we notice that the new new_len &gt;= skb-&gt;len check
    succeeds, so that we return without trying to fragment.

    Fixes: adb92db857ee ("tcp: Make SACK code to split only at mss boundaries")
    Reported-by: Eric Dumazet &lt;edumazet@google.com&gt;
    Signed-off-by: Neal Cardwell &lt;ncardwell@google.com&gt;
    Cc: Eric Dumazet &lt;edumazet@google.com&gt;
    Cc: Yuchung Cheng &lt;ycheng@google.com&gt;
    Cc: Ilpo Jarvinen &lt;ilpo.jarvinen@helsinki.fi&gt;
    Acked-by: Eric Dumazet &lt;edumazet@google.com&gt;
    Signed-off-by: David S. Miller &lt;davem@davemloft.net&gt;
</code></pre>

<pre><code>    diff --git a/net/ipv4/tcp_input.c b/net/ipv4/tcp_input.c
    index 40661fc..b5c2375 100644
    --- a/net/ipv4/tcp_input.c
    +++ b/net/ipv4/tcp_input.c
    @@ -1162,7 +1162,7 @@ static int tcp_match_skb_to_sack(struct sock *sk, struct sk_buff *skb,
                unsigned int new_len = (pkt_len / mss) * mss;
                if (!in_sack &amp;&amp; new_len &lt; pkt_len) {
                    new_len += mss;
    -               if (new_len &gt; skb-&gt;len)
    +               if (new_len &gt;= skb-&gt;len)
                        return 0;
                }
                pkt_len = new_len;
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[gro收包]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/05/08/debug-mark-gro-attention/"/>
    <updated>2015-05-08T16:32:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/05/08/debug-mark-gro-attention</id>
    <content type="html"><![CDATA[<p><a href="/blog/2015/04/18/kernel-net-gro/">linux kernel 网络协议栈之GRO(Generic receive offload)</a></p>

<p>gro会合并多个gso_size不同的包, 会将gso_size设置成第一个包的gso_size.</p>

<p>如果此时把这个包发出去，那么就会导致不满足： skb->gso_size * (skb->segs-1) &lt; skb->len &lt;= skb->gso_size * skb->segs</p>

<p>那么后面的三个函数就有可能出错</p>

<h4>一、tcp_shift_skb_data</h4>

<pre><code>    mss = skb-&gt;gso_size
    len = len/mss * mss

    |---|-------|-------|
     mss    |
            V
    |---|---|
</code></pre>

<h4>二、tcp_mark_head_lost</h4>

<pre><code>    len = (packets - cnt) * mss

    |--------|--|--|
       mss   |
             V
    |--------|--------|
</code></pre>

<h4>三、tcp_match_skb_to_sack</h4>

<pre><code>    new_len = (pkt_len/mm)*mss
    in_sack = 1
    pkt_len = new_len

    |---|-------|-------|
     mss    |
            V
    |---|---|
</code></pre>

<h4>修改</h4>

<p>加入发包队列前
<code>
    skb_shinfo(skb)-&gt;gso_size = 0;
    skb_shinfo(skb)-&gt;gso_segs = 0;
    skb_shinfo(skb)-&gt;gso_type = 0;
</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[tcp_trim_head BUG]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/05/08/debug-mark-tcp_trim_head_bug/"/>
    <updated>2015-05-08T16:24:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/05/08/debug-mark-tcp_trim_head_bug</id>
    <content type="html"><![CDATA[<p><a href="http://kernel.opensuse.org/cgit/kernel/commit/?id=5b35e1e6e9ca651e6b291c96d1106043c9af314a">http://kernel.opensuse.org/cgit/kernel/commit/?id=5b35e1e6e9ca651e6b291c96d1106043c9af314a</a></p>

<p>author  Neal Cardwell <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#110;&#99;&#x61;&#x72;&#100;&#x77;&#101;&#108;&#x6c;&#x40;&#103;&#111;&#x6f;&#x67;&#x6c;&#x65;&#x2e;&#99;&#x6f;&#109;">&#x6e;&#99;&#x61;&#114;&#100;&#119;&#x65;&#x6c;&#108;&#x40;&#x67;&#111;&#111;&#103;&#x6c;&#101;&#46;&#x63;&#x6f;&#x6d;</a>    2012-01-28 17:29:46 (GMT)<br/>
committer   David S. Miller <a href="&#109;&#x61;&#x69;&#108;&#116;&#x6f;&#x3a;&#100;&#x61;&#118;&#101;&#109;&#64;&#100;&#97;&#118;&#101;&#x6d;&#x6c;&#111;&#x66;&#116;&#46;&#x6e;&#x65;&#x74;">&#x64;&#x61;&#118;&#101;&#109;&#x40;&#100;&#97;&#x76;&#101;&#x6d;&#108;&#111;&#102;&#116;&#x2e;&#110;&#101;&#x74;</a>   2012-01-30 17:42:58 (GMT)<br/>
commit  5b35e1e6e9ca651e6b291c96d1106043c9af314a (patch)<br/>
tree    d18caadee5e93dc45d0c5fa2c530537cfa14586c<br/>
parent  4acb41903b2f99f3dffd4c3df9acc84ca5942cb2 (diff)</p>

<h4>tcp: fix tcp_trim_head() to adjust segment count with skb MSS</h4>

<p>This commit fixes tcp_trim_head() to recalculate the number of segments in the skb with the skb&rsquo;s existing MSS, so trimming the head causes the skb segment count to be monotonically non-increasing - it should stay the same or go down, but not increase.</p>

<p>Previously tcp_trim_head() used the current MSS of the connection. But if there was a decrease in MSS between original transmission and ACK (e.g. due to PMTUD), this could cause tcp_trim_head() to counter-intuitively increase the segment count when trimming bytes off the head of an skb. This violated assumptions in tcp_tso_acked() that tcp_trim_head() only decreases the packet count, so that packets_acked in tcp_tso_acked() could underflow, leading tcp_clean_rtx_queue() to pass u32 pkts_acked values as large as 0xffffffff to ca_ops->pkts_acked().</p>

<p>As an aside, if tcp_trim_head() had really wanted the skb to reflect the current MSS, it should have called tcp_set_skb_tso_segs() unconditionally, since a decrease in MSS would mean that a single-packet skb should now be sliced into multiple segments.</p>

<p>Signed-off-by: Neal Cardwell <a href="&#109;&#97;&#x69;&#108;&#116;&#x6f;&#x3a;&#110;&#x63;&#97;&#x72;&#100;&#x77;&#101;&#108;&#108;&#x40;&#x67;&#111;&#x6f;&#103;&#108;&#101;&#46;&#x63;&#111;&#109;">&#110;&#99;&#97;&#x72;&#100;&#x77;&#101;&#108;&#x6c;&#x40;&#x67;&#x6f;&#111;&#x67;&#x6c;&#101;&#x2e;&#x63;&#111;&#x6d;</a> <br/>
Acked-by: Nandita Dukkipati <a href="&#109;&#97;&#x69;&#x6c;&#116;&#x6f;&#x3a;&#x6e;&#97;&#x6e;&#x64;&#105;&#x74;&#97;&#x64;&#64;&#x67;&#x6f;&#111;&#103;&#108;&#101;&#x2e;&#99;&#x6f;&#109;">&#110;&#97;&#110;&#x64;&#x69;&#116;&#x61;&#100;&#64;&#x67;&#111;&#x6f;&#103;&#108;&#x65;&#46;&#x63;&#111;&#109;</a> <br/>
Acked-by: Ilpo Järvinen <a href="&#109;&#97;&#x69;&#108;&#x74;&#x6f;&#58;&#x69;&#108;&#x70;&#111;&#x2e;&#x6a;&#x61;&#x72;&#118;&#105;&#x6e;&#x65;&#x6e;&#x40;&#x68;&#x65;&#x6c;&#x73;&#x69;&#110;&#x6b;&#105;&#46;&#x66;&#x69;">&#x69;&#108;&#x70;&#111;&#x2e;&#x6a;&#x61;&#114;&#118;&#x69;&#x6e;&#101;&#110;&#64;&#104;&#x65;&#x6c;&#115;&#105;&#x6e;&#x6b;&#x69;&#46;&#102;&#105;</a> <br/>
Signed-off-by: David S. Miller <a href="&#109;&#97;&#x69;&#108;&#x74;&#111;&#x3a;&#100;&#x61;&#x76;&#x65;&#109;&#64;&#x64;&#x61;&#118;&#101;&#x6d;&#108;&#111;&#x66;&#x74;&#46;&#x6e;&#x65;&#x74;">&#100;&#97;&#x76;&#x65;&#x6d;&#x40;&#x64;&#97;&#x76;&#101;&#109;&#x6c;&#111;&#x66;&#116;&#46;&#x6e;&#x65;&#x74;</a></p>

<p>1 files changed, 2 insertions, 4 deletions
<code>
    diff --git a/net/ipv4/tcp_output.c b/net/ipv4/tcp_output.c
    index 8c8de27..4ff3b6d 100644
    --- a/net/ipv4/tcp_output.c
    +++ b/net/ipv4/tcp_output.c
    @@ -1141,11 +1141,9 @@ int tcp_trim_head(struct sock *sk, struct sk_buff *skb, u32 len)
        sk_mem_uncharge(sk, len);
        sock_set_flag(sk, SOCK_QUEUE_SHRUNK);
    -   /* Any change of skb-&gt;len requires recalculation of tso
    -    * factor and mss.
    -    */
    +   /* Any change of skb-&gt;len requires recalculation of tso factor. */
        if (tcp_skb_pcount(skb) &gt; 1)
    -       tcp_set_skb_tso_segs(sk, skb, tcp_current_mss(sk));
    +       tcp_set_skb_tso_segs(sk, skb, tcp_skb_mss(skb));
        return 0;
     }
</code></p>

<hr />

<p>会出现tp->packets_out不正确, 导致sk_write_queue为空时却掉tcp_rearm_rto()，判断tp->packets_out不为0，启动重传定时器，然后重传时取出的是list_head的地址，不是skb的地址，导致后面异常。</p>
]]></content>
  </entry>
  
</feed>
